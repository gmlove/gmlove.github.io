1. cd4ml实践映射

1. dev experience: 
    - 以“代码化一切”为原则进行设计，开发人员还是会面向代码做开发，而非拖拉拽完成开发（TW认为这种方式无法完成复杂的开发）
    - 提供通用的代码模板，解决了大数据量下的性能问题，同时快速帮开发者生成生产可用的代码，提高开发效率
    - 可以和任何现有基于代码仓库的工具一起使用，因为都是操作同一个代码仓库
    - 开发人员可以非常容易（几个简单的配置）的将已有的命令行工具集成到工作台
    - 工作台基于web实现，可以基于云端搭建开发环境，开发者不用维护本地环境，节省了时间，同时也推动了各类工程实践的统一
2. data mesh如何统一？
    - data workbench可以很好的支持data mesh的落地： 
        + 符合data mesh的理念
        + 给data product团队赋能，提供sensible default practise让他们快速具备专业的数据工程化能力
        + 给data product团队赋能，提供工具让他们可以快速完成日常开发
        + 统一的工具带来了统一的实践，便于联合治理
3. 和现有的IDE\工具链如何整合
    - 可以和任何现有基于代码仓库的工具一起使用，因为都是操作同一个代码仓库
    - 已经整合过的生态服务及工具包括：
        + 和aws glue进行整合，用aws s3做数据存储，glue做计算引擎
        + 和azure synapse进行整合，用azure Gen2做数据存储，synapse做计算引擎
        + 和gcp bigquery进行整合，支持用bigquery做数据仓库
        + 支持以自建或云端的pg/clickhouse做轻量级数仓，支持百万/千万级数据量
    - 调度工具：
        + 默认支持airflow调度工具的数据流水线自动生成
        + 可通过代码模板的方式，支持其他数据流水线管理工具


5. 方案是啥？
6. metrics: 


gismo / gap
晓强：GSK

成本更低



- 如何做短平快的项目？接入、cloud工具链整合、BI\3周能不能完成
    接入：
    - 确认数据存储方案：spark/clickhouse/pg/bigquery等
    - 沟通确认数据源信息
    - 工作台配置接入数据源信息（连接信息、接入任务配置、接入表及策略配置）
    - 工作台生成数据接入代码
    - 工作台手动运行数据接入任务（全量/增量）
    - 部署流水线，让Airflow自动调度数据接入任务运行
    开发：
    - 确定指标计算口径
    - 可视化ETL开发
    - 工作台手动运行ETL任务
    - 部署流水线，让Airflow自动调度数据任务运行
    cloud工具链整合：
    - 目前可以整合aws glue/azure synapse/gcp bigquery/pg/clickhouse
    BI:
    - 无直接支持，BI一般可通过配置工作台的数据存储，提取里面的数据进行展示


- Codified governance?
- 数据质量、数据安全的故事？
- 数据发现、接入、ETL、安全、质量、治理
- 如何快速进行数据质量的跟踪？



考虑先做社区推广(谁来组织？)：
- 材料（英文）
- 哪些社区
- 谁去讲（John Spense / https://jigsaw.thoughtworks.net/consultants/12264 / Kent）

Actions:
- 联系做一次北美的内部分享（@chenfeng）

data mesh材料：
- https://www.datamesh-architecture.com/




## discover data products:

目前的支持方式如下：
- 指标管理应用，用于查询指标，基于json数据源进行展示（http://52.77.249.130:8080/#data_application/indicator_management/system=sales）
    基本设计：
    + data product以宽表的形式作为输出，一般一个产品一张宽表
    + data product的元数据以json的形式存储在代码库
    跨团队共享data product方式1：
    + 团队1生成指标管理应用元数据
    + 团队1将元数据对应的json文件上传到中心的元数据服务器
    + 部署一个指标管理应用，将所有团队的数据集中起来
    + 其他团队通过指标管理应用查找数据产品
    跨团队共享data product方式2（可能需要一个查找入口的地方，暂没有内置工具）：
    + 团队1将指标管理应用部署
    + 团队2直接访问团队1部署的指标管理应用
- 标签管理应用（http://52.77.249.130:8080/#data_application/label_management/system=sales）
    与指标管理应用使用体验一致，标签与指标的区别在于标签只有当前的数据值。比如客户标签，客户粘性指标，只支持当前值的查询（系统定期计算此数据）
- 数据报告应用（http://52.77.249.130:8080/#data_application/data_report/system=sales）
    统一查询数据表的统计信息，帮助理解字段级的数据，辅助数据开发
    使用方式：
    + 团队1根据输出的数据产品数据表生成数据报告
    + 团队1将数据报告上传到中心数据报告存储
    + 其他团队通过数据报告应用搜索数据

## request access to data product:

由于data product的输出是一张数据库表，所以，基于不同的存储方案，一般可以基于该存储方案提供商的能力来实现。目前工作台没有直接支持。

以hadoop数据中心（所有数据存储在同一个hadoop集群）为例：

1. 团队1构建data product，并存储在某一张hive表中
2. 团队2request access（可通过邮件等渠道，目前软件功能不支持）
3. 团队1在ranger上给团队2授权
4. 团队2访问数据

如果是云端的分布式基础设施，可以考虑的方案：

1. 团队1构建data product，并存储在某一azure storage container
2. 团队2request access，提供账号（可通过邮件等渠道，目前软件功能不支持）
3. 团队1在azure控制台，给团队2提供的账号授权
4. 团队2访问数据

## quickly add checks to verify quality

目前有数据质量模块支持此诉求。http://52.77.249.130:8080/#data_development/data_quality/system=sales

使用方式如下：
- 定义质量规则（或直接使用内置规则）
- 根据数据表配置检查项（如主键唯一、主键非空、电话号码正确率等）
- 根据配置生成数据任务ETL代码及流水线代码
- 手动运行ETL
- 部署流水线，自动调度运行ETL
- 用BI工具，对接输出的质量数据，根据需求配置报表展示


## enable creation of up to date doc

目前可以：
- 在数据建模配置中记录数据相关信息，转换为json格式存储在代码库，纳入版本控制：http://52.77.249.130:8080/#data_development/data_modeling/system=sales
- 根据数据表自动生成数据报告：http://52.77.249.130:8080/#data_development/data_export/system=sales

数据建模层数据表文档：
- 目前以json格式存储，可以使用此数据来生成一份文档，并共享给其他团队。目前还没有内置功能支持

数据报告：
- 运行ETL将此表的数据计算出来
- 使用工作台工具针对此表自动生成数据报告










