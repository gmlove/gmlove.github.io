<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bright LGM&#39;s Blog</title>
  
  <subtitle>Code speaks.</subtitle>
  <link href="http://brightliao.com/atom.xml" rel="self"/>
  
  <link href="http://brightliao.com/"/>
  <updated>2023-12-17T15:28:15.824Z</updated>
  <id>http://brightliao.com/</id>
  
  <author>
    <name>Bright LGM</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>平淡与不平淡</title>
    <link href="http://brightliao.com/2023/12/17/common-vs-not-common/"/>
    <id>http://brightliao.com/2023/12/17/common-vs-not-common/</id>
    <published>2023-12-17T12:00:00.000Z</published>
    <updated>2023-12-17T15:28:15.824Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><p>早上送完人回到家，快两岁的小孩子已经起床穿好了衣裳，家里人正在准备早餐。</p><p>“要吃饼饼”，小孩子指着一袋子蛋卷说。看看给他准备的牛奶马上就要好了，就对他说，“我们先喝牛牛好不好，喝完牛牛再吃饼饼”。小孩子一改以往得不到就吵闹的样子，嘴上说着“喝完牛牛再吃饼饼”，往摇奶器那边走过去。</p><span id="more"></span><p>安安静静地喝完了牛奶，热好的烧麦和煮好的鸡蛋已经上桌。我叫上小孩子，“要不要吃包包？” 小孩子走过来，“包包好吃。” 我一边吃烧麦，一边喂给他。小孩子一改以往吃东西乱吐的习惯，开始大口吃起来。</p><p>吃完了大半个烧麦，我剥开一个鸡蛋，问他吃不吃。小孩子笑一笑看着我，“蛋波波好吃”。先喂他吃一口蛋白，再挑出一瓣蛋黄给到他。小孩子一改以往吃东西落满地食物的习惯，今天照单全收了，地板上甚至很难找到掉下来的残渣。</p><p>吃过饭就和姥姥出门了，回来已经11点。姥姥买回来一袋水果，说，“椪柑应该还好吃。” 我剥开一个准备尝一尝，小孩子也凑过来想要吃。我剥开一瓣给到他，他还是像以往一样想要双手拿一大把。我对他说，“水果一次只能吃一瓣，吃完才能吃下一瓣”。他重复我的话，然后把多余的给我，开始吃起来。吃出了一个桔子核，小孩子一改往日随地就吐的习惯，说，“桔子核要丢垃圾桶”，然后跑到垃圾桶旁边把桔子核丢了进去。</p><p>早上的时光还剩一点，小孩子玩起了识字挂图，开始点按里面的人物。“科学家，画家，服务员，老师…”，识字挂图开始发出声音。我走过去问他科学家在哪里，“在这里啊”，他说着并把手指向科学家。我又问了画家在哪里，他竟然也指对了！我满脸的惊诧。</p><p>玩了大约20分钟的识字挂图，他又去找其他的玩。这时姥姥出门丢垃圾回来，小孩子就跟着姥姥进了卧室。我在沙发上休息。过了一会儿，姥姥也到沙发上休息。突然我一晃神，怎么这么久没有听到小孩子的声音了。赶紧走过去想看看小孩子在干啥。一过去就发现，原来小孩子把房门钥匙拔下来了，正在想办法把钥匙插回去。他身高还不够，尝试了好几次都插不进去。有几次眼看要插进去了，最后还是滑到了其他地方。小孩子一点声音也没有发出，努力地尝试要把钥匙插回去，努力地尝试要把钥匙插回去。</p><p>到了午饭点，小孩子还没有插好钥匙。“快过来吃莽莽，吃完莽莽再来插钥匙好不好？” 小孩子犹豫了一会儿，就过来开始吃午饭。午饭依然很顺利地就吃了一小碗。</p><p>吃完午饭，是小孩子午睡的时间点了。我坐在椅子上，闭目养神，跟他说，“睡觉觉好舒服，睡好觉觉精神好好。你想不想睡觉啊？”小孩子起初还有点抗拒，等了一会，看我想要休息的样子，他也跑去找到姥姥，说要睡觉。然后要姥姥帮助脱下鞋子和袜子。脱下鞋袜之后，他就开始自己哼着歌，“嗯…嗯…嗯…嗯…”，这稚嫩的声音混合着姥姥的“哦…哦…哦…”，组成了一首绝佳的催眠曲。时间也就过去了几分钟，他就进入了梦乡。</p><p>不知道他的梦乡甜不甜，反正我今天已经感动到不写完上面这些文字就睡不着觉了。</p><p>转念一想，带小孩其实跟项目交付很像。有些项目交付，在表面上看，也许就只是如同今天小孩的表现一样，平平淡淡，普普通通。但是在这背后，十里地开外，也许就是波涛汹涌的洪水。如果不是身在团队中间，怎能知道这平淡和普通其实是由于团队建立起了一座阻挡洪水的高高的堤坝？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;早上送完人回到家，快两岁的小孩子已经起床穿好了衣裳，家里人正在准备早餐。&lt;/p&gt;
&lt;p&gt;“要吃饼饼”，小孩子指着一袋子蛋卷说。看看给他准备的牛奶马上就要好了，就对他说，“我们先喝牛牛好不好，喝完牛牛再吃饼饼”。小孩子一改以往得不到就吵闹的样子，嘴上说着“喝完牛牛再吃饼饼”，往摇奶器那边走过去。&lt;/p&gt;</summary>
    
    
    
    <category term="敏捷" scheme="http://brightliao.com/categories/%E6%95%8F%E6%8D%B7/"/>
    
    <category term="心态" scheme="http://brightliao.com/categories/%E6%95%8F%E6%8D%B7/%E5%BF%83%E6%80%81/"/>
    
    
    <category term="agile" scheme="http://brightliao.com/tags/agile/"/>
    
    <category term="心态" scheme="http://brightliao.com/tags/%E5%BF%83%E6%80%81/"/>
    
    <category term="专业服务" scheme="http://brightliao.com/tags/%E4%B8%93%E4%B8%9A%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/12/11/daily-thoughts/"/>
    <id>http://brightliao.com/2023/12/11/daily-thoughts/</id>
    <published>2023-12-11T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.493Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-12-17-传承"><a class="markdownIt-Anchor" href="#2023-12-17-传承"></a> 2023-12-17: 传承</h2><p>“搞破坏，搞破坏不乖”，在跟他说过这些之后，这几句成了他这几天的口头禅。</p><p>“瞎搞”，偶尔也能从他嘴里冒出来。</p><p>很久以前，每次在他准备打开平板电脑的时候，都会跟他说，“这个搞不得”。现在，当他发现平板电脑在旁边，想去玩一玩的时候，只要我在旁边，他都会重复之前说过的话：“这个搞不得”。</p><p>“拉粑粑要跟妈妈说”，虽然他现在还是每次拉完粑粑都不说，但是每当我们问起他为什么不说的时候，他总是重复着上面这句之前跟他说过的话。</p><p>传承，是个很奇妙的过程。</p><h2 id="2023-12-16-他其实学会了"><a class="markdownIt-Anchor" href="#2023-12-16-他其实学会了"></a> 2023-12-16: 他其实学会了</h2><p>和家里人一起在公园游玩，小孩子推着他自己的婴儿车往前走，这是他一直很喜欢的事情。</p><p>突然，地上有一个坑，挡住了婴儿车的轮子，推不动了。他望着姥姥说，“姥姥帮忙，姥姥帮忙！” 姥姥一脸惊诧，“哎呀呀，姥姥帮忙？真能干啊，什么时候学会叫姥姥帮忙了？什么时候学会的啊？这没人 …</p><h2 id="2023-12-14-又好气又好笑"><a class="markdownIt-Anchor" href="#2023-12-14-又好气又好笑"></a> 2023-12-14: 又好气又好笑</h2><p>家里小孩很喜欢吃蛋卷，但是有个坏毛病是喜欢每次拿一大卷，在吃的时候又总是免不了残渣掉满地。</p><p>一天中午，我特意把蛋卷掰成了一小块，递给他，小伙子用充满稚气的话说，“好大坨！” 小孩妈妈在旁边捂着嘴，禁不住地笑出声来，“哈哈哈，哈哈哈，好大坨！”</p><p>从此，小伙子记住了，“好大坨”这个词很好笑。后来很多次，他玩着玩着就突然冒出来一句：“好大坨”。</p><h2 id="2023-12-12-难得的安静"><a class="markdownIt-Anchor" href="#2023-12-12-难得的安静"></a> 2023-12-12: 难得的安静</h2><p>早上送完人回到家，小孩子已经起床穿好了衣裳，家里人正在准备早餐。</p><p>“要吃饼饼”，小孩子指着一袋子蛋卷说。看看给他准备的牛奶马上就要好了，就对他说，“我们先喝牛牛好不好，喝完牛牛再吃饼饼”。小孩子一改以往得不到就吵闹的样子，嘴上说着“喝完牛牛再吃饼饼”，往摇奶器那边走过 …</p><h2 id="2023-12-11-他也要做决定"><a class="markdownIt-Anchor" href="#2023-12-11-他也要做决定"></a> 2023-12-11: 他也要做决定</h2><p>今天天气不错，一阵微风吹散了笼罩城市多日的雾霾，太阳也从云层中探出头，把光明撒向大地。</p><p>去晒个太阳吧，这是个好主意。午饭之后，待孩子睡了一个午觉起来，家里人就开始收拾东西，尿不湿、水、各类纸巾、零食等等。我还特意剥了一个特别难剥的柚子。</p><p>好了，一切收拾妥当，准备出发。</p><p>小孩子快两岁了，跟着我们迈着小步子朝电梯走去。到停车场了，准备上车。爸妈把门拉开，示意小孩上车。“不上车，不上车，不坐车车，不坐车车”，小孩突然开始吵闹起来，一靠近车子就把身子往外面扭。咦？这是怎么回事，以前坐车都好好的，怎么今天突然不愿意坐车呢？</p><p>爸妈开始哄孩子，“乖，快上车我们去晒太阳，外面天气好好哦”，劝了好半天，小孩就是无动于衷。</p><p>等了好一会儿，小孩还是不愿意上车。实在没办法，大家只好收拾东西折回家去。</p><p>到了两岁的年纪，小孩子也想要自己做决定了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-12-17-传承&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; </summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/12/10/daily-thoughts/"/>
    <id>http://brightliao.com/2023/12/10/daily-thoughts/</id>
    <published>2023-12-10T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.493Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-12-10-城里人去住别墅"><a class="markdownIt-Anchor" href="#2023-12-10-城里人去住别墅"></a> 2023-12-10: 城里人去住别墅</h2><p>卧室外面是一个十来平的小院子，一张长方形的石桌子摆在一边，桌子旁立着四张方形石凳。桌子和凳子不少地方都露出了石头原本的颜色和样貌。</p><p>深秋时节，金黄色的银杏叶飘落，零星的洒落在院子里。拾起一片，满片树叶都是黄色，像是已经熟透了，有的地方甚至有点泛白。叶子上面的脉络，清晰可见。</p><p>邻居院子突然传来几声清脆的交谈声，有两个人，说的是无聊的琐事，但清脆的声音在安静的环境中显得特别清晰且响亮。可惜和他们隔着一堵墙，我踮起脚也只能看到他们的头顶，否则真想和他们打声招呼，问问看是否吃过了早餐。</p><p>这里也能听到鸟叫，偶尔还有几只小鸟悄没声地飞到院子里，在草丛里面翻找着食物。</p><p>不禁开始怀念起小时候在农村家里的样子，也是安静的早晨，新鲜的空气，唧唧乱叫的小鸟，和清脆的人声。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-12-10-城里人去住别墅&quot;&gt;&lt;a class=&quot;markdownIt-Anc</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/09/25/daily-thoughts/"/>
    <id>http://brightliao.com/2023/09/25/daily-thoughts/</id>
    <published>2023-09-25T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.493Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-09-30-中秋节的月亮"><a class="markdownIt-Anchor" href="#2023-09-30-中秋节的月亮"></a> 2023-09-30: 中秋节的月亮</h2><p>10小时自驾，在一轮满月的陪伴下到家，终于见到一个月未见的小孩。平常的他10点睡觉，但今天，快到11点了，他还没有睡。</p><p>开门就发现，他满脸堆笑的站在门口，“哦！哦！哦！”高兴地拍手。</p><p>妈妈上前去，想要抱起，小孩有点不知所措，竟然推开妈妈的手，想要保持一点距离。大概在快速成长的小孩眼里，一个月已经是很长很长的时间了。</p><p>最终他还是投入了妈妈的怀抱。妈妈仔细端详起来，“长高了一点，越来越瘦了”。</p><p>小孩还是那么活泼，感觉有无穷的精力，在屋子里跑来跑去。现在正是语言爆发期，大人说什么，简单的，他也能跟着说。“好多大车车”，“骨头”，“鸡肉”，“丢到垃圾桶”，这些都是他新学到的几句话。</p><p>今天晚上，小孩兴奋了很久，12点才有睡意。</p><h2 id="2023-09-29-你看到的是什么"><a class="markdownIt-Anchor" href="#2023-09-29-你看到的是什么"></a> 2023-09-29: 你看到的是什么</h2><p>客户下班时间有事找到你，你会觉得这是一个新机会，还是烦人的加班工作？</p><p>碰到一个很复杂的技术问题，你会觉得这是一个提升的机会，还是一个巨大的风险？</p><p>梭罗说，我并不是说约翰或者乔纳森能够彻底明白这个道理；但正因为他们不明白，所以早晨只是时间的流逝，而不是真正的破晓。导致我们闭上双眼的阳光，对我们来说就是黑暗。只有在我们醒着时，天才是真正的破晓。日出未必意味着光明。太阳也无非是一颗晨星而已。</p><p>世界是什么样，那取决于你看世界是什么眼光。</p><h2 id="2023-09-27-正义的原则"><a class="markdownIt-Anchor" href="#2023-09-27-正义的原则"></a> 2023-09-27: 正义的原则</h2><p>作为20世纪最有名的政治理论家，罗尔斯，针对当下流行的功利主义正义观提出了批评，并提出了正义二原则来促进更大程度的自由和平等。</p><p>罗尔斯认为存在两类基本善（基本善是指经由社会合作产生并用于可分配的所有东西，比如财富、收入、人的自由、权利、机会、自尊等）：一类“基本善”，如言论自由、人的自尊等，可以被平等分配；另一类“基本善”，如出身、天赋、收入、财富、机会、权力等，无法被平等分配。</p><p>如何解决这两类基本善的公平分配问题？罗尔斯提出了两个原则，即正义二原则：</p><p>第一原则是最大的平等自由原则：每个人对与其他人所拥有的最广泛的基本自由体系相容的类似自由体系都应有一种平等的权利。</p><p>第二原则包括差异原则和公平的机会平等原则：社会的和经济的不平等应这样安排，使它们</p><ul><li><p>适合于最不利者的最大利益，并与正义的储蓄原则相一致（差异原则）；</p></li><li><p>在公平的机会平等的条件下，使所有的职务和地位向所有的人开放（公平的机会平等原则）。</p></li></ul><p>其中：第一原则优先于第二原则，即第一类基本善应该被公平分配；第二原则中，公平的机会平等原则优先于差异原则，即在出现第二类基本善的分配问题时，对所有人公平优于向弱者倾斜。</p><h2 id="2023-09-26-流水"><a class="markdownIt-Anchor" href="#2023-09-26-流水"></a> 2023-09-26: 流水</h2><p>最近做了一个明智的决定，那就是设置了短视频的每日使用时长。达到设置的时长之后，就需要输入密码才能继续使用，而我特意设置了一个随机的记不住的密码。</p><p>每次短视频提醒我的时候，我就知道：今天时间已到，需要安排其他事情了。于是，果断关闭它。</p><p>突然发现周末的时间变多了，可以做一些其他的事情。散步，做一餐饭，运动一下，打扫一下家里的卫生，把早该清洗的家具清洗一下，上网查一查东西要买哪一个品牌。生活一下子变得更充实了。</p><p>短视频就像流水，你坐在船上，水载着你自动往低处流去。如果你不主动停下，水就载着你越走越远。</p><h2 id="2023-09-25-用户体验"><a class="markdownIt-Anchor" href="#2023-09-25-用户体验"></a> 2023-09-25: 用户体验</h2><p>开火1：按压燃气灶开关，扭动，电池打火装置啪啪啪响起来；此时不能松手，需一直按着开关，否则将因为刚开始时燃气浓度太低而熄火；直到火势稳定，放手。</p><p>开火2：按压燃气灶开关，扭动，松手，火随之打着；为避免熄火，燃气灶自动保持啪啪啪打火，直到火势稳定。</p><p>煮蛋1：打着火，放入水和鸡蛋，记下开始时间，并计算煮好的时间；去忙其他事；时间到了，进厨房关火，捞出鸡蛋；再忙也不能忘记，否则，鸡蛋煮过了，或者，锅烧坏了。</p><p>煮蛋2：打着火，放入水和鸡蛋，设置13分钟定时关火；安心做其他事；其他事做完，回厨房，捞出煮到恰好的鸡蛋。</p><p>油烟机操作1：根据火势大小及油烟强度，判断吸力档位，然后，按动烟机上面的按键；炒菜完，不能忘关烟机，否则，它一直工作并发出呼呼呼的噪音；不能关太早，否则，空气中油烟未完全消除，余味绕梁。</p><p>油烟机操作2：无。（自动与燃气灶联动，开机、调节大小随火势自动变化；炒菜完毕自动延迟关闭）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-09-30-中秋节的月亮&quot;&gt;&lt;a class=&quot;markdownIt-Anch</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/09/18/daily-thoughts/"/>
    <id>http://brightliao.com/2023/09/18/daily-thoughts/</id>
    <published>2023-09-18T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.492Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-09-24-感染力"><a class="markdownIt-Anchor" href="#2023-09-24-感染力"></a> 2023-09-24: 感染力</h2><p>她双手分别拿起一块鹅卵石，快速碰在一起，“铿”，声音不大，但在偌大的掉一根针都能听到的大厅里，这清脆的声音可以清晰地传入耳中。</p><p>紧跟着这一声“铿”，后排的三位伴奏者也同时敲下手里面的鹅卵石，三声“铿”分秒不差地重叠在一起，合并为更沉闷的一声“铿”传入耳中。</p><p>随着节奏往前推进，她一边敲击鹅卵石，一边轻声走向后面摆好的鼓阵。“铿铿”声瞬间切换为“咚咚咚咚”密集的鼓点，时而大时而小，时而密集时而稀疏，有时候是敲在鼓边的更清脆的“咚”，有时候是敲在鼓心的大声的沉闷的“咚”。这声音快起来的时候，她挥舞的鼓锤变成了看不清的残影；这声音慢起来的时候，她舞动的手臂像是在跳优雅而浪漫的古典舞。</p><p>谁说鼓只能拿来敲？只见她双手在鼓面上面划着圆圈摩挲，顿时响起一阵阵沙沙声，伴奏的三人也随之摩挲起来，一阵阵“沙沙沙沙”声此起彼伏。一会儿呼呼的风声响起，一会儿滔滔的水声响起，一会儿又“啪啪”的海水拍击礁石声响起，这声音哪里是来自她们手里的乐器？这声音分明是来自大自然！</p><p>一曲终了，她移步另一处鼓阵，这鼓锤再次与她手臂融为一体，一阵密集的鼓声之间，竟然出现了一声清脆的木棒碰到金属的声音。哦，这是旁边的金属架。开始是一声，后面越来越多，鼓声则越来越少。再后来，竟然全部是鼓锤敲击金属架的声音。随着她缓慢移步，旁边的鼓架、放乐器的桌面、桌子的桌角、地面全都变成了她的乐器。“铿铿”、“锵锵”、“砰砰”、“哐哐”、“啪啪”这不同质地的敲击声和谐地有节奏地发出，组成了一曲错落有致的壮丽乐章。</p><h2 id="2023-09-23-语言的能指和所指"><a class="markdownIt-Anchor" href="#2023-09-23-语言的能指和所指"></a> 2023-09-23: 语言的能指和所指</h2><p>著名的语言学家索绪尔在分析了语言的结构之后，发现我们所说的每一个语词都存在能指和所指两个概念。</p><p>能指是指一个语词的符号，而所指是指该符号所表达的概念和意义。能指是人们想要表达其所指时赋予的一个符号。</p><p>所指优先于能指而出现。比如，苹果作为一种球形的红色水果，在没有人类语言之前也是一直都存在的，人们为了表达这种水果，于是设计了一个符号“苹果”。</p><p>区分了能指和所指之后，就能理解，当我们想要表达某一个意思（所指）时，我们通过说出来一系列符号（能指）来实现。但是，听众却只能通过这些能指来理解我们的所指。于是，误解便经常产生。</p><p>如何消除误解？结合语音和表情，我们就能更多的获得对方的所指，这即是我们所推崇的当面沟通。</p><p>在软件开发领域，我们用变量名、函数名、类名、包名等编程语言元素来表达我们的所指。由于这些名字同样是自然语言，只是一种能指符号，其背后的所指常常因人而异。这就不难理解为什么我们读他人的代码会比较困难。</p><p>如何缓解？这可能需要我们每一个人在编写代码时都需要尽可能用清晰易懂的、没有歧义的语词。</p><h2 id="2023-09-22-理解就是视域融合"><a class="markdownIt-Anchor" href="#2023-09-22-理解就是视域融合"></a> 2023-09-22: 理解就是视域融合</h2><p>人是历史的人，每个人都身处不同的历史阶段下。人同时也是社会的人，每个人都在他的社会圈子中的活动。所以，人的认知范围受限于这个历史阶段和社会圈子，这个有限的范围就是人的视域。</p><p>一个人与另一个人沟通，需要彼此相互理解，理解的过程就是视域融合的过程。如果两者的视域完全没有交集，则无法相互理解。如果两者视域接近，则很容易相互理解。</p><p>视域的融合就如同可乐和雪碧的融合。当你作为可乐与另一个作为雪碧的人交流时，他的雪碧就会融入你的可乐，从而改变你的颜色和味道。同时，你的可乐也会融入他的雪碧，他的颜色与味道也因你而改变。</p><p>理解是两个人的双向奔赴，相互找到可融合的点，求同存异。理解也是无法相融的视域发生碰撞的点，碰撞激起的火花带来创新。</p><h2 id="2023-09-21-交往理性"><a class="markdownIt-Anchor" href="#2023-09-21-交往理性"></a> 2023-09-21: 交往理性</h2><p>在物质生活高度发达的今天，普世的教育是对科技的崇尚，认为脱胎于科技的工具就是人类之光。但是过于崇尚科技和工具，就容易忽略一个问题：科技的最终目的是服务于人并提升人类整体的幸福感。</p><p>在资本主义社会里，企业执着于创造利润，人不仅没有得到应有的服务，反而不得不面对越来越长的工作时间，越来越大的工作强度以及越来越低下的生活品质。如此一来，我们发展科技的作用是什么？</p><p>德国当代哲学家哈贝马斯提出的交往理性正是针对当今社会的工具理性中的问题而提出。工具理性的社会里，人由于使用工具产生了莫名的优越感，人与人之间的情感变得冰冷。交往理性呼吁人们关注和回归人与人之间的交往过程，提醒人们保持谦逊，用相互可以理解的方式沟通，求同存异，从而营造一个和谐共处的环境。</p><p>敏捷宣言中有一句话是个体和互动高于流程和工具。可以发现，敏捷宣言与交往理性所推崇的价值观是一致的。在软件开发活动中，我们只有把人的位置放在工具之上才能帮助我们构建更高质量、更人性化的软件工具。</p><h2 id="2023-09-20-知觉"><a class="markdownIt-Anchor" href="#2023-09-20-知觉"></a> 2023-09-20: 知觉</h2><p>我们很熟悉感觉，即用身体器官去感受世界而觉察到一些东西。它包括视觉、听觉、触觉、嗅觉等等。</p><p>知觉是什么？在梅洛庞蒂看来，从身体到心灵之间的桥梁就是知觉，知觉是基于心灵的认知的感觉，它把身体和心灵给紧密联系起来。</p><p>试想，如果我们在感觉之前，没有任何的感受经验，对将要感受的外物也没有任何的认知，那我们会如何对待接下来的感受？大概就如同婴儿，只剩下一些模糊不清的本能反应。</p><p>如果我们没有感受，而只有心灵，如同笛卡尔所说“我思故我在”，那我们可能根本无法成长起来，因为婴儿最开始的成长就是不断感受世界。即便成长起来了，没有身体的心灵也将如果植物人一样没有任何依附。</p><p>所以，心灵和身体是和谐的，不可分割的，知觉就是建立它们之间联系的桥梁。我们依靠心灵的思考，结合从感觉中获取的信息，而与世界进行交互。</p><h2 id="2023-09-19-匠艺与匠人"><a class="markdownIt-Anchor" href="#2023-09-19-匠艺与匠人"></a> 2023-09-19: 匠艺与匠人</h2><p>我们常常听说软件匠艺，字面意思是指把软件做成艺术品，这应该是每一个做软件开发的人的追求。</p><p>我对此的理解是，要做软件匠艺先做软件匠人。</p><p>什么是匠人，最初的匠人是指精益求精的手工艺人。在软件开发领域同样需要精益求精的精神。</p><p>如果你舍得多花五分钟把代码格式调整好，那你就在精益求精的路上往前走了一步。</p><p>如果你舍得在写代码之前仔细思考如何用一个更好的模型来抽象当前的问题，那你就在精益求精的路上又往前走了一步。</p><p>如果你不惜多花费一点时间从软件的用户、代码（api）的用户角度思考如何设计一个易用且好理解的接口，那你再次往前走了一步。</p><p>如果你对做好的功能建立了良好的易维护的测试，那你离匠人就更近了，你的软件就离匠艺更近了。</p><p>好的软件的背后是好的软件开发人员，是对好的极致追求。</p><h2 id="2023-09-18-逛超市"><a class="markdownIt-Anchor" href="#2023-09-18-逛超市"></a> 2023-09-18: 逛超市</h2><p>周末，午饭后，躺在沙发上休息了一阵子。我叫上她一起出去走走，消消食。去超市吧，顺便买点东西回来。</p><p>下楼，穿过地铁通道，往超市方向走去。她走得很快，我不得不叫住她，“你走太快了，能慢一点吗？” 她也突然意识到这一点，才放慢脚步。但是，走了几步，竟然又开始恢复到之前的速度。</p><p>到超市了。如何逛？我说，“我们要走一走每一个货架，看看以前都遗漏了什么”。于是我拉着她沿着货架左拐右拐，像是寻宝一般。超市的东西还真是多种多样，想得到的想不到的东西都有，包括各类睡衣、婴儿车、鞋子、毯子、玩具等等。食品区东西更是丰富，诱人的果脯，新鲜的水果和牛奶，各类糕点，各类干粮，卤的炸的，不管是颜色还是香气都肆无忌惮地诱惑着你。逛完之后，不禁感叹，人类的物质竟然丰富到这样的程度了吗？</p><p>我还在寻宝，但是她对此似乎没什么兴趣，望着一处搭建好到帐篷发呆。我叫她，这才反应过来，继续一同往前走去。</p><p>你越慢，时间越慢，你越快，时间越快。然而，生命的时间是有限的，究竟是要快一点还是慢一点？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-09-24-感染力&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot;</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/09/11/daily-thoughts/"/>
    <id>http://brightliao.com/2023/09/11/daily-thoughts/</id>
    <published>2023-09-11T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.491Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-09-16-没有成行的钓鱼"><a class="markdownIt-Anchor" href="#2023-09-16-没有成行的钓鱼"></a> 2023-09-16: 没有成行的钓鱼</h2><p>有一个漂亮的湖，湖水里经常有会飞的野鸭子出没，你明明看到在某一处有一只野鸭子，一转眼它已不见了身影。你开始睁大眼睛四处找寻，过来好一会儿，终于在几十米开外的地方突然发现有一个鸭子头从水里钻出来。</p><p>湖边是绿油油的草坪，这里几乎没有人，你可以肆意的进去踩一踩松软的草坪，甚至坐一坐，或者打个滚，丝毫不会影响小草的生长。</p><p>湖边的树木也是经过精心设计的艺术，一大株一小株相间排列，品种各样。小鸟禁不住诱惑，高兴地在树丛间飞来飞去，叽叽喳喳似乎在向同伴述说自己的意外发现。</p><p>有一次，我和家里人在湖边散步，看到有几个年长的人在钓鱼。他们不紧不慢，悠闲的坐在湖边，水里面的线很长时间也没有动一下。但是他们不在乎，好像只是在等着那些愿意上钩的鱼儿。这让我想起了儿时钓鱼的场景，那会儿钓鱼可是我们几个小朋友最大的爱好。</p><p>我说，我也要找个机会来钓鱼。</p><p>第二次，看到类似这样的场景，我说，一定要找个机会来钓鱼。</p><p>第三次，我也这么说过。</p><p>现在，时间已经过去了一年有余，钓鱼终也没有成行。也许再也不愿意花几个小时安安静静坐在湖边了。</p><h2 id="2023-09-15-一个管理命令行工具的工具"><a class="markdownIt-Anchor" href="#2023-09-15-一个管理命令行工具的工具"></a> 2023-09-15: 一个管理命令行工具的工具</h2><p>开发人员在开发阶段常常需要执行很多自动化的shell命令，以帮助完成测试和调试工作。</p><p>虽然执行命令已经很快了，但还是免不了需要手动输入命令名称以及命令参数。特别是在命令和参数非常多的时候，记忆负担变得更重了。</p><p>我们实现的开发工作台（<a href="http://data-workbench.com">data-workbench.com</a>）引入了这样一些方法来解决这个问题：</p><ul><li><p>提供一种方式将这些常用的命令分组并添加描述</p></li><li><p>提供多种访问和执行这些命令的入口，如网页版搜索、ide插件版快捷键等功能</p></li><li><p>在上述这些入口中，提供自动参数校验，参数历史记录等便于使用的功能</p></li></ul><p>有了这些功能，不仅记忆负担大大降低，而且方便了团队里面大家共享这些命令行工具。团队效率得到极大提升。</p><h2 id="2023-09-14-一个开发者工具的出现"><a class="markdownIt-Anchor" href="#2023-09-14-一个开发者工具的出现"></a> 2023-09-14: 一个开发者工具的出现</h2><p>为了能及时获得开发反馈，我们开发了一个工具，将easysql编写的etl转换为impala可执行的sql代码。它给我们带来了极大的便利，定位olap引擎的impala可以非常及时的告诉我们代码是否有问题。</p><p>但是，改进似乎是没有尽头的。工具虽然好用，但是是以命令行的形式调用的。于是，为了能获得这样的开发反馈，总是需要手动的执行命令，拷贝代码，然后粘贴到impala的sql执行工具中执行一下。每改一行代码都需要重复这个工作。总是感觉很繁琐。</p><p>然后，我们又在酝酿下一次工具的迭代升级。思路非常简单，把之前需要手动完成的操作进一步自动化，使得开发者可以一键获取反馈，或者更进一步，自动在后台给我们提供开发反馈。</p><p>道家说，道生一，一生二，二生三，三生万物。回过头来看这样的工具演进路线，一个之前完全不存在的工具，似乎正在变得越来越丰富和完善。这可能是软件的发展之道。</p><h2 id="2023-09-13-etl代码静态检查"><a class="markdownIt-Anchor" href="#2023-09-13-etl代码静态检查"></a> 2023-09-13: etl代码静态检查</h2><p>作为数据开发的专用dsl–sql，它与其他编程语言存在着一个显著差异：其正确性严重依赖于外部环境。这带来了一些问题：</p><ul><li><p>无法在编码时静态检查字段有没有写错。一个字段在当前没有，不代表后续建表的时候不会重新加上；一个当前已存在的字段也可能在运行时被删除了。</p></li><li><p>无法检查潜在的类型转换问题。比如，当你在按照数字类型处理数据时，如果实际上该类型为字符串，就可能由于类型隐式转换导致大量空置。</p></li></ul><p>这些基本的错误常常在代码执行时才暴露出来，反馈循环太长，开发人员因此效率低下。</p><p>因此，数据开发迫切需要一个能实时连接元数据库进行静态代码检查的集成开发环境。目前有很多数据库专用开发工具可以给到这样的开发体验，比如Oracle提供的sql developer。但支持spark或hive这类大数据引擎的目前还比较少。</p><p>一个简单的实现思路是：</p><ol><li><p>将sql代码转换为基于cte的一条查询语句</p></li><li><p>在最后添加一个用于输出结果的占位语句，如select 1</p></li><li><p>在查询引擎上面执行这个查询，并分析报错结果，然后映射回对应的代码行</p></li></ol><h2 id="2023-09-12-想念"><a class="markdownIt-Anchor" href="#2023-09-12-想念"></a> 2023-09-12: 想念</h2><p>一位很久没有打电话问候的奶奶，突然入梦，慈祥的脸上还是洋溢着那般笑容。</p><p>她不愿意到城市里面和我们一起生活，一个人在农村老家待得自在。</p><p>上次回去，家里显得空空的，大大的堂屋里面只有一张陈旧的小桌子和两张长条凳。但是，她一个人在家，也要收拾得干干净净，桌凳上、地上、墙壁上没有一点灰尘。</p><p>上次回家带上一岁儿子一起，一开始，小孙子还认生，不愿意跟太奶奶亲近。太奶奶很想和小孙子抱一抱，张开双臂，嘴里温和的念叨着要抱一抱，还比着手势。小孩子纠结了好一会儿，最后笑着投入太奶奶的怀抱。</p><p>在老家，有人气的家里就会受燕子的青睐，上次回去，墙壁上又筑好了两个燕子巢。</p><h2 id="2023-09-11-影响"><a class="markdownIt-Anchor" href="#2023-09-11-影响"></a> 2023-09-11: 影响</h2><p>跑步，总是能碰到跑在我前面的。遇到实力差不多的，总是忍不住想要跟上去，从而不自觉地加快脚步。</p><p>图书馆，看到满座都是捧起书津津有味吸取知识精华的人，总是忍不住也伸手拿起一本，沉下心来，享受阅读的乐趣。</p><p>景区，附近一个停车的地方，一位当地的老乡说，这是他们自己家修的，不收费。附近的几处停车的地方也没有收费。</p><p>景区，你随手捡起地上的塑料垃圾，准备收集起来处理掉，避免污染环境。旁边也有人效仿你开始捡起其他的垃圾。</p><p>泰戈尔说：把自己活成一道光，因为你不知道，谁会借着你的光，走出了黑暗；请保持心中的善良，因为你不知道，谁会借着你的善良，走出了绝望。</p><p>这就是影响的力量！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-09-16-没有成行的钓鱼&quot;&gt;&lt;a class=&quot;markdownIt-Anc</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/09/04/daily-thoughts/"/>
    <id>http://brightliao.com/2023/09/04/daily-thoughts/</id>
    <published>2023-09-04T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.491Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-09-10-这味道停不下来"><a class="markdownIt-Anchor" href="#2023-09-10-这味道停不下来"></a> 2023-09-10: 这味道，停不下来</h2><p>一锅香气四溢的辣子红油汤上桌，每人一个浅腹平底碗，装满调料。</p><p>服务员过来打汤，将一半敞口，一半过滤网的汤匙浸入辣子红油汤锅，旋转勺子底部撇开部分红油，舀起一大勺汤，过滤，倒入调料碗。</p><p>夹起一只美蛙，放入调料碗中，来回旋转几次，彻底浸入调料汁。拆下一块肉，送入口中。一辣一麻一香，以肉的位置为中心，瞬间四溢到周围的味蕾。活动牙齿，咀嚼一下，鲜嫩的蛙肉一下子散开。散开的每一小块都味道十足，带着劲道的汤汁无情地席卷整个口腔。</p><p>辣，刺激着口腔发痛；麻，让每一个口腔细胞都开始跳舞；香，带着令人幸福爽快的小分子穿越口腔和鼻腔直达脑门。</p><p>十足的辣让人想赶紧把食物咽下去，十足的麻和香又让人想把食物一直留在口中。于是，在辣味还未来得及传递到大脑时，赶紧咀嚼几次，得到美味的享受之后，辣味也快速到来，赶紧吞咽下肚。然而，大脑哪里能抵抗这种美味，立即指挥手开始拆下一块肉送入口中。</p><p>哪管他会不会长肉！哪管他是不是健康！这味道，停不下来！</p><h2 id="2023-09-09-宗教与诗"><a class="markdownIt-Anchor" href="#2023-09-09-宗教与诗"></a> 2023-09-09: 宗教与诗</h2><p>在很多西方人看来，每个人都应该信仰一种宗教。但是中国却很特别，一个具有数千年历史的文明古国竟然没有一个广泛信仰的宗教。</p><p>中国人如何处世？引导中国人一直向前的可能是诗。从最早的六经，到唐朝的绝句，到宋朝的词，到元朝的曲，再到现代的白话诗。</p><p>诗中的故事和情感，感动着一代又一代的人。诗中的哲学和世界观，引导着一代又一代的人。在彷徨时，诗给人以一束穿透迷雾的光；在失意时，诗给人以一种从容向前的动力；在成功时，诗警醒人前路依然不平坦。</p><p>宗教，虽然充满理性和逻辑，但总少不了神秘主义色彩。相比起来，诗是作者的想象，是脱离现实而高于现实的，这是读者在读诗之前就知道的。同时，诗中有大量的留白，需要读者去填补和想象；诗中有很多道理，需要读者结合自己的经历去体悟。</p><p>这可能就是为什么中国人的哲学既是出世的也是入世的，既追求内圣也追求外王。</p><h2 id="2023-09-08-负能量"><a class="markdownIt-Anchor" href="#2023-09-08-负能量"></a> 2023-09-08: 负能量</h2><p>小a上项目半天，说，我感觉很懵，别的团队上新人会有技术的业务的各类onboarding，这边都没有。</p><p>公司人员缩减，小a说，这两天上班心都悬着，别什么时候被约谈了。</p><p>小a遇到了一种新的语法规则，说，这个怎么这么奇怪，完全反人类，我是怎么都理解不了。</p><p>小a碰到一个不熟悉的工具，说，这个怎么这么难用，感觉很难理解，看不懂。</p><h2 id="2023-09-07-道德的本源"><a class="markdownIt-Anchor" href="#2023-09-07-道德的本源"></a> 2023-09-07: 道德的本源</h2><p>道德是一个非常抽象的概念。根据当今时代的理解，道德是社会意识形态之一，是人们共同生活及其行为的准则和规范，通过社会的或一定阶级舆论对社会生活起约束作用。（百度百科）</p><p>为什么道德是规则和意识形态？要追寻其最初的本义，需要回归老子的道德经。</p><p>经上说，“道生一，一生二，二生三，三生万物”。从这里来看，道是万物之源，并且，万物在生成过程之中，也都有“道”在其中。但是初始的“道”不同于万物之“道”，因此，“道可道，非常道”。</p><p>什么是德？在万物之中的“道”就是“德”, “德”的含义是“能力”或“品德”，它可以解释为万物本有的品质。“万物莫不尊道而贵德”, “道”是万物的由来，“德”则是万物本性的依据。</p><p>当今我们所指的道德更近似道家的“德”，即人的本性，本性具象化就是一些规律，延伸出来就是规则。</p><h2 id="2023-09-06-他乡遇故知"><a class="markdownIt-Anchor" href="#2023-09-06-他乡遇故知"></a> 2023-09-06: 他乡遇故知</h2><p>想象一下，你在一个陌生的城市，正在为生计做着事，突然一抬头，发现一个多年未见的朋友，恰好，他也看见你。</p><p>“xxx”！“xxx”！你们兴奋而又惊讶地相互叫出名字。</p><p>相约去大餐一顿。饭间，以前一起经历的往事，搞笑的尴尬的，变成现在的笑谈…</p><p>人类的情感就是这么奇妙，一次偶然的遇见，一段尘封的往事，就足以令人潸然泪下。</p><h2 id="2023-09-05-稳妥的技术路线"><a class="markdownIt-Anchor" href="#2023-09-05-稳妥的技术路线"></a> 2023-09-05: 稳妥的技术路线</h2><p>今天被一个hive分区问题给坑了。</p><p>问题表现非常奇怪，使用create table as select *…创建的表居然与原表数据量不一致！</p><p>调查了很久，发现了一些端倪：</p><ol><li><p>原表为分区表，采用insert overwrite以动态分区的形式创建</p></li><li><p>查看底层文件系统发现，原表有一些分区列值为空的分区，以及分区列值为 <strong>hive__default</strong> 的分区</p></li><li><p>直接查询原表按照分区统计数量，不会出现上述不合法的分区</p></li><li><p>对新表（未分区表）按照原表的分区字段统计数量，出现了值为空及 <strong>hive__default</strong> 的数据</p></li></ol><p>通过这些现象，可以了解到，可能由于之前某一次运行etl产生了一些脏数据。由于insert overwrite在动态分区场景下不会覆盖没有出现过数据的分区，所以之前的脏数据也一直被保留了下来。</p><p>在技术选择上，我一直推荐采用保守而稳妥的策略。事实上由于软件已经过于复杂了，如果再引入没必要的复杂度，那就很容易导致问题。比如，避免上述问题的一种稳妥的策略是，先truncate或drop table清除所有数据，然后再写入新数据。</p><p>一些其他常见的稳妥技术选择包括：</p><ol><li><p>采用最简单和通用的语法，避免采用过于风格化的编程语言语法（如Scala过于复杂的类型推导）</p></li><li><p>采用最成熟的api，比如写在库或工具的上手文档中的那些api</p></li><li><p>保持实现的幂等性（类似纯函数），尽量隔离副作用</p></li><li><p>…</p></li></ol><h2 id="2023-09-04-成就自己与成就他人"><a class="markdownIt-Anchor" href="#2023-09-04-成就自己与成就他人"></a> 2023-09-04: 成就自己与成就他人</h2><p>儒家之仁有两层意义。</p><ul><li><p>一是忠，即：己欲立而立人，己欲达而达人（《论语 雍也》）。</p></li><li><p>二是恕，即：己所不欲，勿施于人。</p></li></ul><p>第一点的忠，并不是愚忠（别人叫你干什么就干什么）。而是指，自己想要成功，就需要真心实意帮助别人成功。</p><p>当今社会很多人为了自己的成功不择手段，损害他人利益，这显然不是一种可持久的方式。对己难以保持平和，对人难以受到信任。</p><p>事实恰好在于，如果帮助他人成功了，自己往往也会成功。这就是：成就他人即成就自己。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-09-10-这味道停不下来&quot;&gt;&lt;a class=&quot;markdownIt-Anc</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/08/28/daily-thoughts/"/>
    <id>http://brightliao.com/2023/08/28/daily-thoughts/</id>
    <published>2023-08-28T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.491Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-09-03-生命不能承受之重"><a class="markdownIt-Anchor" href="#2023-09-03-生命不能承受之重"></a> 2023-09-03: 生命不能承受之重</h2><p>汽车突然刹停，“嘭”，一个响亮的声音传入车厢里的几人耳中。</p><p>“哇！”接着传来小孩的哭声。“遭了，这下遭了！”后排的人嘴里说着话，赶紧抱起摔倒的小孩。</p><p>大家开始检查小孩的伤势，鼻子旁边有擦伤，在流血，另一只鼻子也隐隐流出血来。</p><p>事发之前，小孩站在后排中间座椅上，刹车时，被惯性带着往前倾倒。由于正对着空调出风口，直直的撞了上去。</p><p>这是多幸运？只是一点擦伤！如果没有这么幸运，那可能就是生命不能承受之重！</p><h2 id="2023-09-02-代码行数的错觉"><a class="markdownIt-Anchor" href="#2023-09-02-代码行数的错觉"></a> 2023-09-02: 代码行数的错觉</h2><p>最近进行的是一个数据开发重构项目，最重要任务之一是改善先前代码的质量。</p><p>经过几周时间的分析和实践，我们发现核心问题是代码重复。</p><p>团队随处能看到一大段一大段的重复代码，一个800行的sql代码消除重复之后可能只有不到200行。</p><p>消除重复带来的益处非常明显：</p><ol><li><p>原来的数据计算逻辑清晰地呈现了出来，代码更容易理解，调查问题更快了</p></li><li><p>不少隐藏的bug被发现并修复</p></li><li><p>取数逻辑的一致性得到更好的保证</p></li></ol><p>代码行数一直是程序员工作量的一个参考指标，不少企业甚至为每个员工设定一个代码行数的目标。因此，很多开发人员为了达成这个所谓的目标胡乱复制粘贴代码，最终导致了低下的项目质量。</p><p>这其实是关于代码行数的错觉。</p><p>事实上，在代码量指标上，不仅不应该求多，反而应该求少。能完成同样功能的代码，当然是越少越容易维护。</p><h2 id="2023-09-01-数据流水线自动生成"><a class="markdownIt-Anchor" href="#2023-09-01-数据流水线自动生成"></a> 2023-09-01: 数据流水线自动生成</h2><p>在很多数据项目中，数据流水线的配置都是靠团队手工完成。这带来了很大的工作量（特别是在开发阶段），而且容易出错。</p><p>事实上，数据流水线完全可根据etl血缘关系自动生成出来。</p><p>我们在团队中这样做：</p><ol><li><p>采用airflow这样的可通过代码定义流水线的调度工具</p></li><li><p>自动解析etl文件中的依赖表，并找到对应的etl文件</p></li><li><p>根据以上依赖关系自动生成etl依赖图</p></li><li><p>将此依赖图转化为流水线代码</p></li></ol><p>自动化数据流水线的生成和管理给我们带来了极大的便利，节省了团队大量的时间。团队变得更高效和敏捷了。</p><h2 id="2023-08-31-核污染水的数据与逻辑"><a class="markdownIt-Anchor" href="#2023-08-31-核污染水的数据与逻辑"></a> 2023-08-31: 核污染水的数据与逻辑</h2><p>日本核污染水排海事件正在成为国际关注的焦点。关于这件事有很多说法，比如日本自私论，美国阴谋论，中国过分反应论等等。</p><p>日本举出了很多数据说明核污染水无害。然而中国的逻辑很简单，如果无害就没必要排海，如果有害就更不应该排海。</p><p>在这件事上，我们的态度应该是保守的，因为它关乎全人类的健康。同时，在这件事上，过分相信数据那是极危险的，因为数据极容易作假，特别是这样复杂的专业领域，漏报一项数据就可能让性质完全不一样。</p><p>这件事上更应该相信的是简单的逻辑，中国的逻辑就是简单而显然的。</p><p>在关乎健康的重大问题上，中国是偏保守的，这让我们很放心，感觉更安全。为什么要拿安全去冒险？钱是可以赚的，安全问题发生了，就无法回头了。</p><h2 id="2023-08-30-无处藏身的bug"><a class="markdownIt-Anchor" href="#2023-08-30-无处藏身的bug"></a> 2023-08-30: 无处藏身的bug</h2><p>业务应用开发中大家常常遇到bug，有些bug隐藏很深，甚至连专业的qa都没法发现。比如某些并发场景下出现的bug，某些极少出现的边界场景、需求未定义的场景下的bug等等。</p><p>但是在数据开发中，这些bug往往无处遁形。究其原因，有：</p><ol><li><p>数据开发通常要并行处理大量数据，并发问题极易在开发阶段就暴露出来</p></li><li><p>数据开发通常要处理生产系统积累的全部数据，其背后覆盖了几乎所有业务场景</p></li><li><p>数据开发以专用的sql语言为主要开发语言，如果代码本身有问题，从结果上很容易发现（比如结果集数据量不对，统计上不符合业务直觉等等）</p></li></ol><p>因此，数据开发人员在编写代码时应当极为谨慎，因为bug很容易暴露出来，而出现bug后也得由自己处理，总也跑不掉。</p><h2 id="2023-08-29-自动化一切"><a class="markdownIt-Anchor" href="#2023-08-29-自动化一切"></a> 2023-08-29: 自动化一切</h2><p>如何让团队变得更敏捷，也许应该从自动化一切开始。</p><p>自动化大部分工作之后，团队的各类开发规范就不至于只停留在纸面上，而是通过工具自动化得到保证。团队也不会因为一些低级错误而耗费大量的时间。有了自动化加持，团队可以集中精力解决重要问题，从而实现效率更高、质量更好。</p><p>就像先进机器是工业化时代的生产力一样，自动化工具就是敏捷团队的生产力。</p><p>如何推进团队工作自动化？要点在于在团队内部建立这样的意识，当每个团队成员在完成开发任务的时候都想着是不是可以把一些工作自动化的时候，团队的自动化水平就会越来越高。</p><p>自动化一切要求团队成员具备很强的技术基础，不过，这正好是技术人员努力前进的方向。</p><h2 id="2023-08-28-诱惑"><a class="markdownIt-Anchor" href="#2023-08-28-诱惑"></a> 2023-08-28: 诱惑</h2><p>小孩到了一岁半的年纪，吃饭慢慢变成一件令人头疼的事情。</p><p>勺子自己拿不太稳，同时专注力也不够，吃两口之后就动来动去，上蹿下跳。</p><p>家里人没办法只能喂饭，喂一口，又跑了，得跟着追。有时候喂快了没吃完，或者他只是单纯抗拒食物，到嘴里面了，也直接吐出来，吐一地。</p><p>最近发现一个特别有效 …</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-09-03-生命不能承受之重&quot;&gt;&lt;a class=&quot;markdownIt-An</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/08/21/daily-thoughts/"/>
    <id>http://brightliao.com/2023/08/21/daily-thoughts/</id>
    <published>2023-08-21T12:00:00.000Z</published>
    <updated>2023-08-31T15:01:00.054Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-08-27-积木与故事"><a class="markdownIt-Anchor" href="#2023-08-27-积木与故事"></a> 2023-08-27: 积木与故事</h2><p>家里小孩有一个积木桌，搭配有很多积木小方块。小孩年纪还十分小，这些小方块也都是随意的搭配，没有成套。</p><p>这正好给人充分发挥创造力的空间。</p><p>那我们就来设计一个理想中的院子吧。将高度相同的小方块连续排列，围成一个方形，就是院子的围墙。围墙上面留一个小口子，口子两边凸出一块，就变成门。院子里面再围一圈高一点的小方块，就是院子里面的房间。房间紧挨着再来一圈相同高度的方块，就构成了另一个房间。房间里面平铺几块低一些的小方块，这是床。类似的还可以做出厨房和卫生间以及院子里的茶座。</p><p>在院子搭建完毕之后，拿着成型的院子给家里人讲一个故事：这是妈妈，妈妈早上起床了，到卫生间洗漱，顺便上了一个厕所，然后到厨房找了早餐吃了，然后泡茶并在茶座上和家里人愉快地聊天。。。</p><p>《人类简史》中说，人类和黑猩猩之间真正不同的地方就在于那些虚构的故事，它像胶水一样把千千万万的个人、家庭和群体结合在一起。这种胶水，让我们成了万物的主宰。</p><p>比如小小的积木就可以让我们虚构一个故事，如果这个虚构的故事能被大家所接受，那它就可以把大家紧密地结合在一起。</p><h2 id="2023-08-26-从拥堵中寻找顺畅"><a class="markdownIt-Anchor" href="#2023-08-26-从拥堵中寻找顺畅"></a> 2023-08-26: 从拥堵中寻找顺畅</h2><p>上班路上有一个左转路口，之前一直非常拥堵，一般需要超过10分钟才能过去。最近这个路口却突然变得通畅起来，常常只用一个红绿灯就过去了。</p><p>观察发现是由于红绿灯设置更合理了。</p><p>这个路口是进出城市环线的路口，工作日的时候进环线和出环线的车都非常多。之前出环路后的直线路段第一个红绿灯的直行绿灯时间设置比较长，左转时间则非常短。为什么左转时间短呢？是为了支持更多的对向直行上环路的车。这就导致一个问题，下环路后直行左转和上环路的车都堵得厉害。</p><p>在车流并没有变少的情况下，现在的红绿灯设置为什么可以改善执行左转的拥堵呢？</p><p>很简单，只需要增加左转时间即可。因为原来对向直行本来就堵，在一定范围内，无论增加或减少时间都不会有任何改善。既然如此，最优解就是找到对向直行的最短绿灯的时间即可。这时，上环路的拥堵依然存在，下环路的拥堵却显著减少了。</p><p>很多事情虽然看起来已经最优，但在全局视角下，往往还有提升的空间。如果你找到了这样的方法，那就是从拥堵中寻找到了顺畅。</p><h2 id="2023-08-25-滴滴打车意外打到了一辆model-3"><a class="markdownIt-Anchor" href="#2023-08-25-滴滴打车意外打到了一辆model-3"></a> 2023-08-25: 滴滴打车意外打到了一辆model 3</h2><p>滴滴打车意外打到了一辆model 3，我靠近车门，准备拉开门上车，发现门把手嵌入到车门里面，不知道如何打开。我敲一敲车窗，司机师傅告诉我说要按一下门把手一边，另一边就会翘起，然后就可以开门了。上车之后，座位怎么坐都不太舒服，仔细感受之后发现坐垫太短且地台太高，腿一大截没有被支撑到。而且后面座位也显得有点紧张，活动空间有限。车辆行驶过程中，周边环境及车辆会跟随显示，非常清晰和准确，司机可以清楚的看到周围的情况。</p><p>更多能打到的车是比亚迪汉，靠近车门，门把手自动弹出来，我拉开门上车，一气呵成。进入车内，座位坐起来相当舒服，不管是地台高度还是坐垫长度都非常适当，活动空间也非常宽敞。座位皮质显得很高档，不管是触感还是观感都有一种高级和精致的感觉。当然这辆车虽然有大屏幕，但看起来只像是摆设，司机师傅只是用来显示空调信息。</p><h2 id="2023-08-24-spark同时读写某一张表"><a class="markdownIt-Anchor" href="#2023-08-24-spark同时读写某一张表"></a> 2023-08-24: spark同时读写某一张表</h2><p>spark无疑是一个优秀的计算引擎，但却在支持同时读写某一张表时略显不足。</p><p>一个简单的示例为：insert overwrite a select * from a</p><p>事实上，如果我们碰到了这样的场景，spark直接报错：cannot overwrite a path that is also being read from.</p><p>如何解决？一个很简单的方案是，先写入一个临时目录，然后再修改一下目录名。</p><p>用sql表达，即：</p><ol><li><p>insert overwrite a_tmp select * from a</p></li><li><p>drop table a</p></li><li><p>alter table a_tmp rename to a</p></li></ol><h2 id="2023-08-23-分解复杂数据问题"><a class="markdownIt-Anchor" href="#2023-08-23-分解复杂数据问题"></a> 2023-08-23: 分解复杂数据问题</h2><p>数据开发常常碰到数千行的sql代码，如何看待和优化这些代码呢？</p><p>可以将问题按照业务和技术复杂性进行分解。</p><ol><li><p>识别复杂代码中涉及的大表连接场景，这通常很慢而且难以优化。</p></li><li><p>针对大表连接场景，识别是否可以将更多数据筛选条件下推到连接之前；识别是否可以在连接条件中利用分区键；识别是否可以优化为小表连接大表等。这些场景下有很多成熟的优化方案。</p></li><li><p>将大表连接代码隔离出来，单独处理。即拆分为简单代码大数据量代码段及复杂代码小数据量代码段两部分。第一部分主要进行技术优化，第二部分主要进行业务逻辑优化。此即为关注点分离的模式。</p></li></ol><h2 id="2023-08-22-困局"><a class="markdownIt-Anchor" href="#2023-08-22-困局"></a> 2023-08-22: 困局</h2><p>地上一滩水，小蚂蚁在水边爬，恶作剧的人用手划过这滩水，圈出一个水圈，将小蚂蚁困在圈子里面。</p><p>小蚂蚁在水圈里面来回爬，想要找到一个出路，它一直尝试一直尝试，把能尝试的路径都试过了，始终都没有找到出路。</p><p>随着这滩水越积越多，水圈慢慢变小，慢慢变小。。。</p><h2 id="2023-08-21-推荐反被推荐误"><a class="markdownIt-Anchor" href="#2023-08-21-推荐反被推荐误"></a> 2023-08-21: 推荐反被推荐误</h2><p>下火车了，赶紧用滴滴订一个车，滴滴推荐了几个上车点，随意选一个，然后开始往上车点赶。到了上车点，是一个停车场，人真多，嘈杂的打电话声音夹杂着汽车的哄哄声，在夏天的潮湿而又闷热的空气中乱作一团。汽车排着长队一个车位一个车位往前挪，刺耳的喇叭声不绝于耳。等了20分钟终于上车。</p><p>这是很多人在火车站打滴滴车的日常。</p><p>这次到站了，由于我实在无法忍受在热浪中等待20分钟之久，就选择了一个自己定位的上车点。没想到一切竟出奇的顺利，快速的叫到了车，师傅快速到了上车点，我也快速的上了车。由于上车点在马路旁，人流量不大，车也没有排长队。体验一下子提升n个档次。</p><p>为什么滴滴推荐的上车点体验如此差，而自己选择的却还不错？</p><p>本来是一个不错的地点，但正是由于将少量资源推荐给了大多数人，这个不错的地点就变成了很差的地点。推荐反被推荐误！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-08-27-积木与故事&quot;&gt;&lt;a class=&quot;markdownIt-Ancho</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/08/14/daily-thoughts/"/>
    <id>http://brightliao.com/2023/08/14/daily-thoughts/</id>
    <published>2023-08-14T12:00:00.000Z</published>
    <updated>2023-08-31T15:38:41.996Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-08-20-在紧张中创造"><a class="markdownIt-Anchor" href="#2023-08-20-在紧张中创造"></a> 2023-08-20: 在紧张中创造</h2><p>反思一下个人的内容产出频率，我注意到一个很有意思的现象：往往工作最紧张、压力最大的那一个阶段，产出却是最多的。</p><p>为何？紧张的时期，大脑一直处于思考的状态，所以新的想法就很多；当渐渐进入到一个平稳的时期，创造力似乎就渐渐离我远去。</p><p>为什么春秋战国时期有百家争鸣，各类新思潮层出不穷？大概是因为那个时代是一个局势紧张的时代，各诸侯国之间战事不断，不仅要求发展还求如何得人心。</p><p>古希腊半岛位于各个文明圈的中央，也正是由于和周围的城邦不断的进行交流和碰撞才产生了西方哲学的萌芽。</p><p>所以，站在人类的历史长河看，有竞争的乱世也并不是一件坏事，因为它常常是破旧立新产生突破的时代。</p><h2 id="2023-08-19-记忆里的人"><a class="markdownIt-Anchor" href="#2023-08-19-记忆里的人"></a> 2023-08-19: 记忆里的人</h2><p>在记忆里她一直是一位很慈祥的母亲。</p><p>她很会做饭，曾经在一个山清水秀的煤矿厂里给厂子里面的人做饭；数次春节到她们家，也总是能有一大桌子菜等着大饱口福。</p><p>她也很慷慨，每次春节到她们家总少不了一个大红包；平时去也是各类零食水果从不间断。</p><p>她勤劳且持家，每次去她们家总是能看到干净整洁的房间；只要她在，总是在忙里忙外收拾这准备那。</p><p>很多年没常见面了，以后回去也见不到了。</p><h2 id="2023-08-18-半梦半醒"><a class="markdownIt-Anchor" href="#2023-08-18-半梦半醒"></a> 2023-08-18: 半梦半醒</h2><p>我左手抱着小孩，有一种沉甸甸的感觉。他今天很安静，没有吵闹。客厅的墙、过道、地板看起来都非常的清晰。我没有戴眼镜。家里人都正常在家休息，有的在沙发上半躺着，有的在房间其他角落走动。</p><p>我能清晰的知道这不是现实。但眼前的事物是那么的清晰和真实，即便在我的身体自然移动时，这些事物也可以正常地跟随切换。</p><p>我想挑战一下这自动在脑海里出现的场景，看看它的能力极限在哪里。</p><p>走进房间的过道，进入一间卧室。没错，是我们家的卧室！床和柜子的摆放都是没问题的。甚至连门上面的贴画上面的字都清晰映入眼帘。</p><p>穿出过道，进入了一个黑黢黢的拐角处，朝房间里面望去。这好像是好几年前的住过的房间。床和家具是深色木质的，被子铺在床上。看起来是被人直接在床上面躺过而没有将被子展平，有很多褶皱。衣柜的一边柜门贴着一张四方形的福字贴，里面有老婆很久以前写的八个字，字非常清晰。和我有关，读后感觉很感动。这个场景渐渐勾起了我更多的回忆。。。</p><h2 id="2023-08-17-程序员终极提效工具"><a class="markdownIt-Anchor" href="#2023-08-17-程序员终极提效工具"></a> 2023-08-17: 程序员终极提效工具</h2><p>最近的数据开发项目中，我们引入了一个etl代码编译解析工具，它可以自动分析代码为语法树，并从中提取代码相关信息。</p><p>然后，我发现有很多日常开发需要手工完成的事情都可以用它来自动完成，比如：</p><ul><li><p>自动生成建表语句</p></li><li><p>自动生成数据血缘分析图</p></li><li><p>自动提取输入表、输出表，辅助生成输入输出处理代码</p></li><li><p>自动代码格式化</p></li><li><p>自动重构</p></li><li><p>自动提示待重构的代码</p></li></ul><p>-…</p><p>有了语言级的信息支持，有一种思路一下子被打开的感觉。随着一个一个工具的实现，工作效率也是蹭蹭蹭往上涨。</p><p>程序员的终极提效工具是什么？除却GPT这种自动写代码的黑科技之外，那可能就是编程语言级的自动分析优化和代码生成了。</p><p>回想很多语言提出的元编程思想（比如rust），其实与这里的解法一致，不过它们大多由官方提供了支持。</p><h2 id="2023-08-16-当学习成为负担"><a class="markdownIt-Anchor" href="#2023-08-16-当学习成为负担"></a> 2023-08-16: 当学习成为负担</h2><p>最近团队在专业业务领域知识积累不太够，于是我们组织大家一起学习。</p><p>由于学习通常在工作时间之外，这势必会占用一些工作之外的其他时间。</p><p>这就导致了一个矛盾，团队成员可能不得不放弃一些个人安排，投入更多时间到学习中。</p><p>看起来学习成为了团队的一种负担。</p><p>但是，学习真的是负担吗？</p><p>回顾一下之前的学习过程就可以知道，学习常常能给人一种充实感。学到新知识了，我们会感觉获得更多能力了，变得更强大了。所以，学习常常是获得快乐的一种方式，而不是负担。</p><p>为什么会有人觉得是负担？可能是当代学以致用的观念胜过了学以致知。当我们将自己定位为一个学习的人，成长的人，坚持终身学习，把学习作为人生的一种状态，它也就不是负担了。</p><p>所以，是不是负担，关键在于个人的心态。</p><h2 id="2023-08-15-二级混沌系统"><a class="markdownIt-Anchor" href="#2023-08-15-二级混沌系统"></a> 2023-08-15: 二级混沌系统</h2><p>准确的预测可以帮我们更好的计划。比如，预测明天的天气，可以帮助我们计划旅游目的地；预测将来的行业趋势，可以帮助我们更好的选择职业。</p><p>随着科技的进步，越来越多的系统的原理被揭示出来，变得可以被准确预测。</p><p>然而有一类系统不可被准确预测，那就是二级混沌系统。</p><p>比如股市的预测，如果有一个方法宣称它可以准确预测股票的走势，那大家就可以完全根据它的预测结果进行股票交易。其结果反而将推翻之前的预测。</p><p>在这类系统中，预测会受到本身预测结果的影响，因而不可能准确预测。这就是二级混沌系统。</p><p>类似的系统还有很多，比如物理学中的测不准原理，政治预测，地图上的拥堵预测等等。</p><p>当下流行一句话，我预判了你的预判，这是指人思维也具有这样的特性。</p><p>在这样的系统中，一个科学的做法是什么？可能是要有多种完全不同的预测方法，然后不同的人选择不同的方法来做预测。这样一来，就有可能大家共赢。</p><h2 id="2023-08-14-动态的制度"><a class="markdownIt-Anchor" href="#2023-08-14-动态的制度"></a> 2023-08-14: 动态的制度</h2><p>在一个组织中，一项制度的确定一定是为了解决某一类问题，其初衷通常是好的。其操作过程也常常是因为得到大多数人的支持而形成。但制度常常解决某些问题却引出来另一些问题，这就是上面的政策引出来的下面的对策。</p><p>所以，制度的出台应该着眼于解决现实问题。但制度却不应该是死的制度，应该根据情况随时调整。对待调整的制度的态度应不亚于最初制定制度的态度，否则制度往往流于形式或导致更严重的问题。</p><p>钱穆在《中国历代政治得失》中说，应该用制度迁就现实，而不是现实迁就制度。直接生搬硬套拿来的制度不一定是好的制度，因为它没有着眼于具体的现实问题。而从汉唐一成不变的制度最终导致没落，又可说明制度需常改常新。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-08-20-在紧张中创造&quot;&gt;&lt;a class=&quot;markdownIt-Anch</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/08/08/daily-thoughts/"/>
    <id>http://brightliao.com/2023/08/08/daily-thoughts/</id>
    <published>2023-08-08T12:00:00.000Z</published>
    <updated>2023-08-15T16:18:04.621Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-08-13-会计准则与领域词典"><a class="markdownIt-Anchor" href="#2023-08-13-会计准则与领域词典"></a> 2023-08-13: 会计准则与领域词典</h2><p>财务是一个很有意思的部门，它的职责是记录和管理企业内部的各类经济事务。由于企业内部几乎所有活动都要和钱相关，因此也都需要和财务相关。故而，企业几乎所有业务活动所涉及的（高层次）知识都将流入财务系统，财务系统事实上成了企业内部的知识中心。</p><p>这对于软件开发有什么启示？</p><p>领域驱动设计建议我们逐步构建一个领域词典，以便于让大家对于系统的理解达成一致。如何构建这个词典？</p><p>企业财务管理需要符合国家会计准则，而国家会计准则则对于社会上的各类企业业务做了较为明确的高层次划分及规范制定。比如，会计科目中关于原材料采购业务定义了这几类相关科目：银行存款/应付账款/应交税费用于钱款流转事务，在途物资/原材料用于物品运输与仓储事务，生产成本/制造费用/管理费用则用于和原材料相关的各类生产消耗事务。从这里所涉及到的一些术语可以了解到会计准则中的知识几乎覆盖了企业所有核心业务过程。</p><p>基于以上分析，领域词典是不是可以基于标准的会计准则中的知识来定义？</p><p>会计准则作为一个国家颁布的企业经济行为标准，经过了社会广泛调研，可以覆盖绝大多数的关键场景，并且相关的解释和研究非常丰富，实在是不可多得的材料。</p><p>虽如此，会计准则之于领域词典也有局限。因为会计准则中的知识是仅停留在高层次，其应用范围也将限于高层次（比如架构、系统对接接口等），细节或较低层次的内容则还是应该根据具体系统相关的业务确定。</p><h2 id="2023-08-12-on中的条件和where中的条件"><a class="markdownIt-Anchor" href="#2023-08-12-on中的条件和where中的条件"></a> 2023-08-12: on中的条件和where中的条件</h2><p>sql语言的表达能力非常强，但在某些场景下看起来功能相同的代码却有着非常细微的差别，稍不留神可能就被坑了几个小时的调试时间。</p><p>比如，某一个条件写在表的连接条件中和写在连接后的where条件中有什么区别？</p><p>示例如下：</p><ul><li><p>写在on：select * from a left join b on <a href="http://a.id=b.id">a.id=b.id</a> and a.cond=1</p></li><li><p>写在where：select * from a left join b on <a href="http://a.id=b.id">a.id=b.id</a> where a.cond=1</p></li></ul><p>思考一下，第一个的结果集里面会出现a.cond!=1的数据吗？</p><h2 id="2023-08-11-用编程语言的性能提升你的效率"><a class="markdownIt-Anchor" href="#2023-08-11-用编程语言的性能提升你的效率"></a> 2023-08-11: 用编程语言的性能提升你的效率</h2><p>最近项目中需要搜集一个公开产品的文档，以便快速为团队补充相关知识。通过实现一个简单的爬虫，解析网页，获取信息就可以支持。</p><p>第一个版本，我采用了最近用得最多的python实现。代码很快写完，但是一运行就发现了一个较大的问题，网页解析太慢了！一个20MB的网页需要解析十分钟，而这样的网页总共有好几百个。没想到，本来以为可以快速完成的任务，结果输在了程序性能上！在Python这条路上折腾好长时间，无果。</p><p>由于nodejs天生就对网页解析和元素查找很擅长，所以，考虑改为nodejs实现。于是，花了一小时左右，程序搞定。上真实网页一测，性能简直不要太好，20MB的网页解析只用了不到5秒。于是这个任务在采用nodejs重新实现之后，顺利地在两小时内完成了。</p><p>我们在开发程序的时候，一般都先入为主采用自己最擅长的开发语言，希望通过熟练度来提升效率。但是每一种开发语言都有其擅长和不擅长的领域。了解这些，就可能可以通过编程语言的性能带来颠覆式的效率提升。</p><h2 id="2023-08-09-基于代码复用的数据工程"><a class="markdownIt-Anchor" href="#2023-08-09-基于代码复用的数据工程"></a> 2023-08-09: 基于代码复用的数据工程</h2><p>业界普遍采用以sql为主的数据开发语言进行数据开发，依靠数仓分层来避免重复计算，这一复用方式可以称之为数据复用。但数据复用的灵活性较差，可能会形成一个非常复杂的数据任务流水线，并且，当我们想独立运行某一个etl的时候，我们不得不执行所有依赖的任务。</p><p>基于代码复用的数据工程是指采用复用代码的方式进行数据工程中的复用。它采用以计算资源换取维护成本的方式帮我们节省开支。在大量数据量没那么大的场景下更适用。</p><p>目前很多框架可以支持这一使用方式，比如dbt，easysql都有相应的支持。</p><h2 id="2023-08-08-终点却是过程"><a class="markdownIt-Anchor" href="#2023-08-08-终点却是过程"></a> 2023-08-08: 终点却是过程</h2><p>人们常常奔着一个目标往前冲，甚至能不分昼夜，不顾身体。</p><p>但是等目标按照预期实现了之后，却觉得心里一下子空虚起来，不知道该干什么了。</p><p>有多少软件功能在实现之后就成了它的坟墓？无人问津的程度甚至不如当初开发阶段的测试使用频率。</p><p>也许有时候终点恰恰是过程，是奋力向上的有着充实感受的过程。而目的的意义在于为我们提供一个参与这个过程的机会。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-08-13-会计准则与领域词典&quot;&gt;&lt;a class=&quot;markdownIt-A</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/31/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/31/daily-thoughts/</id>
    <published>2023-07-31T12:00:00.000Z</published>
    <updated>2023-08-15T16:18:04.620Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-08-06-弄清需求背后的原因"><a class="markdownIt-Anchor" href="#2023-08-06-弄清需求背后的原因"></a> 2023-08-06: 弄清需求背后的原因</h2><p>家里孩子到了1岁半的年纪，开始喜欢抓人和咬人。经常没来由的给人脸上身上来一爪或者来一口，弄得家里人满身伤痕。家里人为此苦恼不已。</p><p>一开始我也不理解他的行为，采取打回去或者抓回去的办法，并且表现出很凶的样子试图进行教育。然而，教育的时候他总是表现出听不懂的样子，重复数次也没有效果。</p><p>在网上搜索一番之后，开始弄清楚了背后的原因：</p><ul><li><p>身体发育到特定阶段，用嘴和手来感知这个世界</p></li><li><p>以此寻求关注</p></li><li><p>以此发泄情绪</p></li><li><p>以此表达喜欢</p></li></ul><p>在理解这些之后，终于释然，现在会更注意用手护住自己，并且根据尝试平和地把他的想表达的东西说出来，引导他进行正常表达。</p><p>了解一开始不理解的事物的原因可以帮助我们更好的应对。</p><p>在软件开发过程中也是一样。很多开发人员总是只遵循需求描述进行开发，而极少思考背后的原因，即业务价值。所以，他们常常觉得很多需求实现起来很别扭，最终形成了我们在代码里面看到的很奇怪的注释或者条件判断。这样的软件常常满是bug，维护起来非常痛苦。这就像直接针对小孩抓人这件事用表现得很凶的样子教育孩子一样，自己很痛苦，效果也很差。长期下来，软件就渐渐腐坏掉。</p><p>如果我们可以尝试去弄清楚需求背后的原因（业务价值），就可以了解到可能有更符合设计的实现，也可以了解到将来可能的演进方向，那就更可能达到简单的架构和优雅的实现。</p><h2 id="2023-08-05-人类的本质差异"><a class="markdownIt-Anchor" href="#2023-08-05-人类的本质差异"></a> 2023-08-05: 人类的本质差异</h2><p>地球上有很多动物，为什么人类可以站在食物链最顶端？</p><p>是因为人类会语言？很多动物其实也会，猫狗的不同叫声代表不同意思，鸟类可以模仿人讲话，其音域甚至更广。</p><p>是因为人可以直立行走，操作工具？很多猴子、猩猩，甚至乌鸦也可以。</p><p>《人类简史》中提到人与其他动物的本质差异可能在于人类可以创造一些原本不存在的概念，并通过交流让大家广泛相信，从而聚集起大规模的群体，比如国家与军队。这个群体由于数量上可以大大超过其他动物形成的群体，而具有极强的优势。正是这个优势让肌肉并不发达，体格并不高大的人类站在了食物链最顶端。</p><h2 id="2023-08-04-sql代码的重构"><a class="markdownIt-Anchor" href="#2023-08-04-sql代码的重构"></a> 2023-08-04: sql代码的重构</h2><p>随着大数据越来越广泛的应用，基于sql的etl代码也越来越长。当代码达到数百行规模时，就急需要引入整洁代码（clean code）这类高质量编码实践，否则很容易变得不可维护。</p><p>重构是实现高质量代码的重要手段，如何针对etl进行代码重构呢？最重要的是需要有一款强大的sql代码重构工具。它应该具备这样一些功能：</p><ul><li><p>能将中间子查询抽取为cte格式的临时视图</p></li><li><p>能修改子查询或cte视图的别名</p></li><li><p>能修改子查询或cte视图中的字段名</p></li><li><p>能自动进行代码格式化</p></li></ul><p>目前似乎还没有特别匹配这些需求的工具，期待它的出现…</p><h2 id="2023-08-03-理性限制的建立与破除"><a class="markdownIt-Anchor" href="#2023-08-03-理性限制的建立与破除"></a> 2023-08-03: 理性限制的建立与破除</h2><p>康德认为这世界有很多二律背反问题，比如，正题：世界上的一切都是由单纯的部分复合而成；反题：世界上的一切都是复合的，没有单纯的东西。这两个论断都可以用理性来证明，却又是相互矛盾的结论。所以，理性是有限制的，不应该用于超验的自在之物上。</p><p>到了谢林和黑格尔，理性的限制被认为是理性的怯弱。矛盾不是不可调和的，而是所有事物发展过程的中必然出现的阶段。但是，随着认识的深入，矛盾和对立终将达到同一。这世界本质是发展和运动的，总是将一轮一轮的经历从同一到差别到对立，再到矛盾最后回到同一的过程，这也是黑格尔的肯定、否定和否定之否定。否定之否定是最初的肯定的升华。</p><p>到这里，终于能理解为什么马克思主义哲学的第一句话是：世界是物质的，物质是运动的。</p><h2 id="2023-08-02-早上六点钟"><a class="markdownIt-Anchor" href="#2023-08-02-早上六点钟"></a> 2023-08-02: 早上六点钟</h2><p>早上六点钟的世界是什么样子？机械化的工作时间可能已经将我们和这个时段划清了界限，但最近的生活节奏让我有机会出去看一看。</p><p>马路上的汽车要少一些，但还是络绎不绝，在红绿灯前面排着队。</p><p>公路清理工作已经开始，洒水车、扫地车、环卫工人已经就位，赶在天还不热的时候为大家创造一个更干净的环境。</p><p>住所旁边有一个运动公园，公园里已经有很多健身的人，有的开着节奏舒缓的音乐打着太极，有人沿着跑道散步，更多的是身着运动装跑步的。</p><p>我是去公园跑步的，跟着同跑的人流向前。大家节奏不一，脚步声噼里啪啦的，偶尔超过几个速度慢一点的，偶尔被几个速度快的超过。</p><p>唯物主义哲学认为世界是物质的，物质决定精神。但不管物质如何变化，精神却是自由的，如果没有早起的精神意识，那也看不到早上六点钟的世界。</p><h2 id="2023-08-01-没有测试代码的自动化测试"><a class="markdownIt-Anchor" href="#2023-08-01-没有测试代码的自动化测试"></a> 2023-08-01: 没有测试代码的自动化测试</h2><p>数据测试一直是一个难题，需要建表，构造数据，运行etl，对比结果等繁琐的步骤。手工构造测试场景不仅耗时巨大，而且难以维护。</p><p>有没有一种轻量级的自动化测试方式？</p><p>可以这样来做：</p><ul><li><p>根据etl代码生成数据血缘图</p></li><li><p>从图里面提取表和字段</p></li><li><p>自动根据输入表和字段创建空表</p></li><li><p>针对空表运行etl</p></li><li><p>以输出空表为标准，对比结果</p></li></ul><p>这种测试无法验证逻辑的正确性，但可以验证语法、udf、相互引用关系等基础正确性。</p><p>胜在无需编写任何一行测试代码即可运行程序，非常轻量，适合往大数据平台提交任务之前先在本地验证。</p><p>这种方法通过极低成本在早期提供反馈来提高开发效率，值得尝试。</p><h2 id="2023-07-31-突破时空的染色"><a class="markdownIt-Anchor" href="#2023-07-31-突破时空的染色"></a> 2023-07-31: 突破时空的染色</h2><p>康德在描述我们认识事物的过程时，认为我们认识任何自然事物都会带着主观的色彩。最基础的主观色彩就是时间和空间。时间、空间是自我与生俱来的先天直观形式，只要我们对事物进行感知，我们就必须把它放在空间和时间中。</p><p>当我们带着主观的色彩去认识事物的时候，事物就不是完全自由实在的了。至于完全自在的事物，那是不可知的。</p><p>结合弗洛伊德的意识和潜意识理论，这种先天直观形式其实是一种潜意识，它确实会给我们认识事物带来障碍，但通过训练我们也是可以认识并突破这种障碍的。所以，完全自在的事物依然是有可能可知的。</p><p>那么，我们等等看黑格尔是怎么说的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-08-06-弄清需求背后的原因&quot;&gt;&lt;a class=&quot;markdownIt-A</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/24/daily-thought/"/>
    <id>http://brightliao.com/2023/07/24/daily-thought/</id>
    <published>2023-07-24T12:00:00.000Z</published>
    <updated>2023-12-17T15:09:26.490Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="1217-传承"><a class="markdownIt-Anchor" href="#1217-传承"></a> 12.17: 传承</h2><p>“搞破坏，搞破坏不乖”，在跟他说过这些之后，这几句成了他这几天的口头禅。</p><p>“瞎搞”，偶尔也能从他嘴里冒出来。</p><p>很久以前，每次在他准备打开平板电脑的时候，都会跟他说，“这个搞不得”。现在，当他发现平板电脑在旁边，想去玩一玩的时候，只要我在旁边，他都会重复之前说过的话：“这个搞不得”。</p><p>“拉粑粑要跟妈妈说”，虽然他现在还是每次拉完粑粑都不说，但是每当我们问起他为什么不说的时候，他总是重复着上面这句之前跟他说过的话。</p><p>传承，是个很奇妙的过程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;1217-传承&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/24/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/24/daily-thoughts/</id>
    <published>2023-07-24T12:00:00.000Z</published>
    <updated>2023-08-15T16:18:04.620Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-07-30-技术卡上的方案"><a class="markdownIt-Anchor" href="#2023-07-30-技术卡上的方案"></a> 2023-07-30: 技术卡上的方案</h2><p>数据开发项目很多故事卡偏技术，比如数据接人，它并不直接产出用户价值（数据计算结果），而是用于支持其他直接产生价值的卡。</p><p>这类故事卡，一般称为技术卡。要如何描述其内容？</p><p>一般敏捷开发故事卡中会用“作为…我想要…以便…”这样的描述来阐明故事卡的背景、场景、目的及价值。对于实现方式的描述通常是模糊的，以便可以在团队中发生更多讨论，从而促进更好的技术建模。</p><p>但在技术卡中，是否要进行更清晰的实现方式（方案）描述？</p><p>技术上有很多要求来自团队技术规范，相比故事卡这类纵向需求，技术规范可以理解为横向需求，即每个故事卡都需要遵循的需求。比如整体架构、数据分层、关键命名等等。</p><p>已有的规范应该是每一个开发人员都需要熟悉的，没有的规范则应该通过每张故事卡去建立或完善。</p><p>因此，技术卡的描述应该涉及一部分实现方式，主要关于规范的部分，也可分为两类：</p><ul><li><p>来自通用技术规范的需求，可链接到技术规范文档对应章节</p></li><li><p>通用技术规范未提及部分，视团队成员能力情况确定是否由经验更丰富的成员事先调研梳理并描述</p></li></ul><p>第二部分中，前期调研程度应该适当，并快速完成，否则调研完就开发完，那开发就没有意义了。</p><h2 id="2023-07-29-看起来复杂的问题往往原因很简单"><a class="markdownIt-Anchor" href="#2023-07-29-看起来复杂的问题往往原因很简单"></a> 2023-07-29: 看起来复杂的问题往往原因很简单</h2><p>这两天团队碰到一个看起来非常棘手的问题。问题表现是：在一个分布式环境中，某些数据操作只有一部分成功，另一部分则总是失败。</p><p>在花费了大量精力观察和调试问题之后，团队总结出规律：某两个节点的数据操作总是失败，其他节点正常；针对这两个节点，如果手动发起操作可以成功，自动发起则总是超时。</p><p>虽然发现了规律，但还是令人很疑惑。</p><p>最后团队集思广益，终于找到原因，简单到不能再简单：这两个节点没有配置主机名解析，一直连接不上。</p><p>启发：</p><ul><li><p>需要在技术上理解分布式组件工作原理</p></li><li><p>分布式组件设计上应该在出错时输出更有效的日志</p></li><li><p>锲而不舍的精神，相信复杂问题背后的原因往往很简单</p></li></ul><h2 id="2023-07-28-重新思考标记问题"><a class="markdownIt-Anchor" href="#2023-07-28-重新思考标记问题"></a> 2023-07-28: 重新思考标记问题</h2><p>在继续读《西方哲学史讲演录》的同时，我意识到每个哲学家的观点具有很强的推理性质。如果只看到一个结论，而没有推理过程，那结论就不可理解。</p><p>比如，英国经验论的基本原则是：凡在理智之中的无不先在感觉之中。这有什么问题？应该如何看待？逻辑上讲，至少存在一个反例，对于第一个睁开眼看世界的人来说，世界的存在可以在理智之中，但却没有先在感觉之中。因为他无法凭借经验（还没有）证明这个世界在他睁眼以前就已经客观存在了。</p><p>回到笔记上，如果只标记结论，而没有标记推理，那标记就是无用的，而要标记推理，那就将标记整本书，也是无用的。</p><p>这大概就是为什么我之前有不想做标记的想法。</p><h2 id="2023-07-27-规则"><a class="markdownIt-Anchor" href="#2023-07-27-规则"></a> 2023-07-27: 规则</h2><p>这世界有很多奇怪的规则，它存在着，即便奇怪也是合理。</p><p>规则限定了游戏参与者的范围，你要是没法适应，那就没有资格入局。</p><p>自命清高者，多数饿死于沙滩上。在这个内卷的时代，拼的不只是能力。或者，在无法改变规则时，适应它也是一种能力。</p><h2 id="2023-07-26-少即是多"><a class="markdownIt-Anchor" href="#2023-07-26-少即是多"></a> 2023-07-26: 少即是多</h2><p>最近在一次团队活动中，有人分享关于写作的问题，提到一个如何让文字更简洁的办法，很有启发。</p><p>如何做？那就是在写完之后，问自己一个问题，如果删一个字给你100块，你会删哪个？当你通过这个办法把内容删到无法再删的时候，就足够精简了。</p><p>从读者的视角看，内容简洁已经成为当今这个知识爆炸的快节奏时代的必需品。但是从写作者的视角来看，很多时候都是越写越多。很多作者都恨不能把自己知道的都写进去，生怕遗漏某个细节。</p><p>然而内容的用户是读者，从用户体验的角度讲，作者在写作时应该要保持克制，时刻提醒自己，少即是多。</p><h2 id="2023-07-25-工作的节奏"><a class="markdownIt-Anchor" href="#2023-07-25-工作的节奏"></a> 2023-07-25: 工作的节奏</h2><p>在专业软件服务公司工作很容易形成一种节奏感。</p><p>我们都是以项目制为单位进行团队组织的。项目一般会定期更替，这是较长期的节奏。项目内部我们以敏捷迭代的方式展开，这是短期的节奏。</p><p>节奏就像持续不断的音乐节拍，有起有伏，井井有条，让人乐在其中。</p><p>有的时候，节奏却也像是时钟的滴答，背后的齿轮机械而固定地转动，一成不变，永无止境，让人感觉枯燥无聊。</p><p>人生也许就是需要有略显枯燥的大的节奏，以便可以在一个框架中前进，而不至于偏离方向。同时，大的节奏并不意味着无聊，因为在大的节奏期间我们可以尽情发挥。这就像音乐，高低不同的音符伴随着规律的节奏形成了打动人心的篇章。</p><h2 id="2023-07-24-有时读书不想做标记"><a class="markdownIt-Anchor" href="#2023-07-24-有时读书不想做标记"></a> 2023-07-24: 有时读书不想做标记</h2><p>最近在读赵林老师的《西方哲学史讲演录》，这是一本哲学通识类的书籍。内容清晰而精彩，作为一个没什么哲学背景的普通人，也能轻松读下来并抓住哲学思想发展过程中的重点。</p><p>全书内容非常精彩，随处都洋溢着老师对于当时那个时代哲学思想的深刻理解和剖析。读书过程中我在很多精彩处都通过标记做了一下笔记，希望能通过这些标记记住这些精彩的部分。但过一段时间之后再回过来看这些标记，发现之前认为精彩的部分竟然不那么精彩了，甚至看不懂了。只看一个片段时，已是失去了当时看到对应章节的理解和心情。</p><p>然而，当我重新开始全文读下标记部分对应的那一章节之后，我发现之前的感觉又都回来了。哲学思想发展的脉络，其历史背景，优势和局限性又清晰的呈现在了眼前。</p><p>也许有时候没有必要做标记，因为到处都是精彩的内容。如果认为通过标记可以把书读薄，那很可能将错过那些精彩的内容。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-07-30-技术卡上的方案&quot;&gt;&lt;a class=&quot;markdownIt-Anc</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/17/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/17/daily-thoughts/</id>
    <published>2023-07-17T12:00:00.000Z</published>
    <updated>2023-07-25T12:51:22.878Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-07-23-天真与成长"><a class="markdownIt-Anchor" href="#2023-07-23-天真与成长"></a> 2023-07-23: 天真与成长</h2><p>以前我们家小孩每次出电梯到时候都要给一起乘电梯的人表达再见，扬起手挥一挥，然后嘴巴里面不停的说着拜拜拜拜。碰到热心的邻居，也会笑着回复他跟他拜拜。他因此收获了无数人的喜欢。我们都说他是小社牛。</p><p>最近出差回来慢慢发现他已经越来越少表现出这样的行为了。</p><p>为什么会这样？可能是因为成长。这几个月是他智力高速发展的几个月，慢慢地他开始与我们能有更多的互动了。他学会了走路，能认识身边的人，还能说几句简单的话，还会自己吃葡萄并把葡萄皮吐出来丢到垃圾桶。他似乎已经开始以一个自主的主体参与世界的活动了。</p><p>随着快速的成长，原来天真的小社牛真的要慢慢变成理性的社会人？当我们成长之后，还有多少儿时的天真可以保留下来？</p><h2 id="2023-07-22-看代码没有信心的时候就去搞懂业务"><a class="markdownIt-Anchor" href="#2023-07-22-看代码没有信心的时候就去搞懂业务"></a> 2023-07-22: 看代码没有信心的时候，就去搞懂业务</h2><p>遗留代码的维护是件令人头疼的事。我们常常要面对多年以前的杂乱无章的代码，它们可能非常长，各类名称可能毫无意义，可能几乎没有文档。</p><p>如果直接从代码入手，妄图从代码中发现真理，那可能要落入一个爬不出去的陷阱。满是细节的代码就像是一片迷雾森林，你在其中绕来绕去也找不到一条清晰的出路。</p><p>换个思路出发会更容易，那就是从业务出发。事先可以完全抛离代码，只是从业务角度去理解当时的需求和目的。业务角度的需求和目的就像是一个个灯塔，可以为我们指引方向，而有了方向，在就不至于迷失在满是细节的迷雾森林。</p><h2 id="2023-07-21-依赖干系人管理"><a class="markdownIt-Anchor" href="#2023-07-21-依赖干系人管理"></a> 2023-07-21: 依赖干系人管理</h2><p>项目里面的干系人可以按照利益相关性和权利大小分为四个象限，对于利益相关度高权利低的人，通常采取的策略是随时告知。</p><p>但如果这些人是你的依赖方，就需要谨慎，因为有可能他们不依赖你，你依赖他们，你的成败需要他们支持，而他们的成败不需要你的支持。</p><p>此时，由于他们可能不关心你的项目，所以告知的意义未必大。你找他们协助，他们有可能善意的提供帮助，但绝没有直接的义务。</p><p>如何处理？一是以各种方式表达感谢搞好关系，二就是上升到利益相关度高权利高的干系人协助处理。</p><h2 id="2023-07-20-食不知味的感觉"><a class="markdownIt-Anchor" href="#2023-07-20-食不知味的感觉"></a> 2023-07-20: 食不知味的感觉</h2><p>酒店的早餐算是十分丰盛了，超过二十种食材随意挑选。然而，最近越来越发现面对这么多丰盛的食物却不知道选哪种。常常随意拿几个，狼吞虎咽下肚，事后竟然连这些美味的味道都回忆不起来了。</p><p>是什么原因？大概是因为早餐太急，想一想，好像很久很久没有一个早上能什么事都不想，什么安排也不做，就仅仅是吃早餐。完全放松下来，认认真真做这件事，细嚼慢咽品味每一粒米饭和每一片蔬菜的味道。</p><h2 id="2023-07-18-从业务入手讲方案"><a class="markdownIt-Anchor" href="#2023-07-18-从业务入手讲方案"></a> 2023-07-18: 从业务入手讲方案</h2><p>当需要为一个复杂系统提供方案时，需要从业务角度理解问题，不要陷入具体细节。抽象的上层业务模型通常是简单而一致的，这就是主线，基于这个主线就可以给出技术上合理的方案。</p><p>财务系统可能涉及几千个科目，每个科目的取数逻辑就是细节，非常多。但是财务统计本身是很简单的，就是简单的求和、取余额等。设计系统时，需要抓住这个主线，避免陷入细节。</p><h2 id="2023-07-17-应对数据开发复杂性有没有更好的方案"><a class="markdownIt-Anchor" href="#2023-07-17-应对数据开发复杂性有没有更好的方案"></a> 2023-07-17: 应对数据开发复杂性有没有更好的方案？</h2><p>DataMesh数据网格的思路是让业务系统开发团队来提供数据产品供下游消费。这些数据产品往往是聚合了很多数据表得到的一个可以隐藏底层复杂性的数据宽表。</p><p>由于这些中间数据产品来自于该系统的开发团队，而他们天然具备对该系统业务和数据的深刻认知，所以维护起来天然是得心应手的。</p><p>对于数据开发团队，他们基于这些中间数据产品去构建集成的数据分析报表。在报表开发过程中可以将知识限定于构建集成报表的范围，不至于扩大到各个复杂的底层系统中。</p><p>经过这样的职责划分，不至于让数据开发团队知识过载，各团队可以各司其职将最终的数据产品构建出来。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-07-23-天真与成长&quot;&gt;&lt;a class=&quot;markdownIt-Ancho</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/12/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/12/daily-thoughts/</id>
    <published>2023-07-12T12:00:00.000Z</published>
    <updated>2023-07-25T12:51:22.877Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-07-15-新时代的家族关系如何维系"><a class="markdownIt-Anchor" href="#2023-07-15-新时代的家族关系如何维系"></a> 2023-07-15: 新时代的家族关系如何维系？</h2><p>城市化让更多人搬进了封闭的钢筋混凝土小区，以前亲密的家族成员也分散到了全国各地。</p><p>在以前家族成员聚集到一起的时候，总能经常有些活动，比如谁谁过生日，大家就一起聚个餐，谁谁要办个什么事，大家就一起棒棒忙。亲密的亲戚关系于是得以维系。</p><p>现在离得远了，很多亲戚经常几年也没联系。如何继续这样的关系？</p><p>我们有一个家族微信群，每当有人过生日，大家就在里面发红包，十块八块大家有个心意就是了，过生日的人也会回一个随机红包，让大家一起热闹热闹。借此机会，大家也聊聊近况。由于大家过生日都这样做，一个十多个人的群里面，竟然每个月都至少能有一次这样的活动。</p><p>新时代下家族关系的维持可能需要我们去寻找这样的一些仪式。</p><h2 id="2023-07-14-基于chatgpt进行etl代码重构"><a class="markdownIt-Anchor" href="#2023-07-14-基于chatgpt进行etl代码重构"></a> 2023-07-14: 基于ChatGPT进行ETL代码重构</h2><p>复杂的数据分析场景背后常常使用复杂的ETL进行支撑。然而，由于工程化的缺失，基于SQL的ETL代码很容易变得难以理解。在经历团队人员流动之后，这一问题变得更为显著。如何优化已有的复杂ETL？ChatGPT也许可以帮到我们。以下场景中ChatGPT可以发挥作用。</p><p>代码格式化。ChatGPT可以很好的完成代码格式化，相比于学习和配置特定的代码格式化工具，ChatGPT可以用自然语言给出格式化的规范，并在无规范的地方使用行业通用的做法。</p><p>代码注释。ChatGPT可以帮我们理解代码并添加注释。当碰到难以理解的逻辑时，ChatGPT可以结合元数据，告诉我们代码的行为及背后的原因。</p><p>基于CTE的代码优化。ChatGPT还可以将子查询转化为CTE格式，我们可以基于此功能把多级嵌套的子查询分离出来，并给它一个名字辅助理解。</p><p>基于谓词下推的优化。将筛选条件尽可能放在查数据的源头可以有效提升性能，在提示ChatGPT去进行代码性能优化时，它可以帮助实现这一优化。</p><h2 id="2023-07-13-架构设计"><a class="markdownIt-Anchor" href="#2023-07-13-架构设计"></a> 2023-07-13: 架构设计</h2><p>最近读了一本书《重塑心灵》，讲的是身心语法程序学（NLP）的知识。NLP里有十二条前提假设，其中之一是：在任何一个系统里，最灵活的部分便是最能影响大局的部分。</p><p>最初读到这里的时候有一些诧异。联想到软件架构设计，在我们通常的认知里，架构就是那个最能影响大局的部分，那么架构到底是最稳定还是最灵活的部分？</p><p>如果架构是稳定的，它限定了一个框架，那么灵活性就在于填入框架中的细节，那么细节便会成为最能影响大局的部分。所以，细节比架构更重要。</p><p>如果架构是最灵活的，那么它就可以被经常调整，那么架构设计就应该在每一天的日常开发过程中进行。</p><p>总结起来，我们要么获得了一个不重要的架构，要么把架构变成日常开发中的每一张故事卡。</p><h2 id="2023-07-12-当面沟通的力量"><a class="markdownIt-Anchor" href="#2023-07-12-当面沟通的力量"></a> 2023-07-12: 当面沟通的力量</h2><p>在我还青春年少的时候，大家经常调侃失追女孩的情景说：屌丝有三废，在吗，忙不，早点睡；女神有三宝，干嘛，呵呵，去洗澡。</p><p>仔细分析一下为什么屌丝追女孩很失败，我发现这其中有一个重要的原因，屌丝是用短消息和女神交流的。屌丝没法从女神的“干嘛”中捕获到女神今天的心情，也没法知道女神当前的状态。</p><p>试想，即便还是用这样的开场白，但是屌丝直接一个电话或者视频打过去，会是什么场景？只要女神真的不是直接拒绝你，你就可以从她的话语、表情或者手势中感知到非常多的信息。她烦躁就听听她的吐槽，她无聊时就跟她讲讲网上查到的笑话，她在娱乐时就远程加入，交流于是如丝般顺滑的进行了下去。</p><p>为什么当面交流比起短消息更有效？佛洛依德的潜意识理论认为意识和潜意识就像冰山，意识是水上面的很小一部分，潜意识是水下面的大部分。当面交流时，潜意识帮助我们捕捉了大部分的信息，因而可以帮助我们更有效的交流。</p><p>回想起昨晚因为客户的一句拒绝性的短消息回复而失眠，今天见面交流时又能谈的很投机。谁说不是呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-07-15-新时代的家族关系如何维系&quot;&gt;&lt;a class=&quot;markdownI</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/03/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/03/daily-thoughts/</id>
    <published>2023-07-03T12:00:00.000Z</published>
    <updated>2023-07-25T12:51:22.877Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-07-09"><a class="markdownIt-Anchor" href="#2023-07-09"></a> 2023-07-09:</h2><p>今天陪小孩玩耍时，偶然发现了之前抓周时购买的三字经书。想想我们小时候竟然没有要求阅读和背诵这样的国学启蒙经典，于是翻开读起来。时间比较充裕，竟然读完了整个三字经。</p><p>看完之后，心里升起一种深深的相见恨晚的感觉。整部经书不仅仅是我们所熟知的三字一句朗朗上口，更是在内容上包罗万象，从前到后包括了学习方法、行为道德准则、生活中的名物常识、应该读的书目及次序、历史朝代更替，以及最后对读者学习的勉励。三字经就像是古人为后来求学者铺设的一条通往知识殿堂的道路，简明而具体，应该说是一本不可多得的启蒙经典。</p><h2 id="2023-07-08"><a class="markdownIt-Anchor" href="#2023-07-08"></a> 2023-07-08:</h2><p>财务报表中的各项指标计算涉及很复杂的计算过程，很多科目的计算规则并不是简单的统计，比如“固定资产”“投资性房地产”“无形资产”等项目的统计需要应用该项目期末余额扣除相应的减值准备科目及累计折旧（摊销）等的净额。另一个挑战是财务统计科目非常多（《企业会计准则》设置了85个常用一级科目，以及部分金融保险等领域专用会计科目；而《小企业会计准则》设置了58个一级科目），并且这些科目按照父子关系形成了多棵树形的结构。</p><p>如果每一个科目都去实现一套特定的统计方法，那一定是非常浩大的工程量。</p><p>如何设计一个合理的数据计算系统来应对这一问题呢？以下是一些初步的想法，看起来可以较好的解决这个问题：</p><p>将各科目（最细粒度）按照月度周期分别统计期初/期末值，存入一个中间表A</p><p>设计一个表格，以便业务人员可以针对每一个父级填入统计计算公式（如：固定资产统计 ＝ 固定资产.期末余额 － 固定资产减值准备.净额 － 固定资产累计折旧.净额）</p><p>设计一个程序，读取表格中的计算公式，生成一个数据计算的ETL</p><p>这样一来，业务人员只需要按需将所有计算公式填入表格，开发人员使用一个ETL生成程序就能应对所有的计算了。这一设计可以认为为财务指标计算定义了一套DSL（计算公式），看起来可以让整个系统变得一致而简单。</p><h2 id="2023-07-06"><a class="markdownIt-Anchor" href="#2023-07-06"></a> 2023-07-06:</h2><p>前不久我给团队里面一位刚毕业不久的一个同学分配了一个颇具探索性的任务。这个任务对于已经工作十年以上的我来说不算太复杂，但是对于他还是挺有挑战性。于是我尝试给他分享了这个任务要通过几个步骤如何如何完成，并期望他可以顺利地按照我提到的步骤去完成。然而等了一周之后，他没有给我任何反馈。我很奇怪，于是去问他进度。在沟通之后，我不禁大跌眼镜，没想到过了这么长时间他竟还处于完全不知道怎么做的状态！</p><p>今天恰好有另一位同学想找我做一些指导，无意中想起了这件事。</p><p>反思之前的事，我意识到很可能是因为我没有仔细分析被分配任务的同学能力在哪里，是不是胜任，而是更多站在自己的角度安排了难度太大的任务，而且过程中也没有及时的跟踪和指导。设身处地想一想，当我还在他们那个年纪的时候，刚刚毕业，对于职场几乎还是一片空白，不仅专业知识有很多缺漏，而且沟通也没有技巧和信心，如何能完成这样高难度的任务？</p><h2 id="2023-07-06-2"><a class="markdownIt-Anchor" href="#2023-07-06-2"></a> 2023-07-06:</h2><p>前两天刚去看了电影《消失的她》，由于电影结束已经是很晚了，于是刚播放片尾曲的时候我们就快速起身离开了电影院。后来发现竟然还有彩蛋，彩蛋里面是男主做了一个梦，醒来之后幡然醒悟，然后一家人重新过上了幸福生活。两者结局天壤之别，剧情中的男主也走向了两个极端。真实世界的人其实充满着不确定性！回想每天我们做过的决定，有多少次是因为我们头脑一热，灵机一动？这竟然与我最近正在研究的ChatGPT很像。ChatGPT能力很强，大部分时候都能给到正确的答案，但是由于随机性的存在，我们却很难确定它可以在每一个场景都能百分之百给出正确的答案。也许正是那百分之零点零一的概率让人走向了不同的极端。</p><h2 id="2023-07-05"><a class="markdownIt-Anchor" href="#2023-07-05"></a> 2023-07-05:</h2><p>老王特别擅长长跑，每次长跑，他都感觉浑身是劲，越跑越感觉得心应手，信心十足。但是最近一段时间队里面的比赛都是短跑，而队里面确实缺少了跑短跑的，没办法，老王硬着头皮顶了上去。由于这不是老王擅长和喜欢的，虽然顶了上去，老王也老是提不起兴趣，因此短跑也没什么成绩。长期下来，老王感觉越来越没信心，眼看着逐渐逝去的青春，慢慢地开始怀疑自己适不适合再继续做一个运动员。</p><h2 id="2023-07-03"><a class="markdownIt-Anchor" href="#2023-07-03"></a> 2023-07-03:</h2><p>OpenAI的创始人Sam Altman曾在描述大家如何看待ChatGPT时提到，很多人把ChatGPT当作一个数据库使用。刚听到这种描述的时候，我不禁一怔，在大家都在惊叹于ChatGPT强大的语言理解和推理能力的时候，它却只是一种传统得不能再传统的数据库？然而仔细一想，这似乎也没问题，当前大家使用ChatGPT的方式主要是提问，而ChatGPT只是负责给出答案，这不就跟数据库执行查询返回结果是一样的么？这样来看，ChatGPT最多可以称为一个存储知识的数据库，而查询语言是自然语言而已！那么我们期望的能力到底是什么？可能是真正能帮助我们处理日常事务，可以洗碗擦地上楼取东西的智能机器！而这离我们似乎还比较远。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-07-09&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; hre</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>每日一思</title>
    <link href="http://brightliao.com/2023/07/02/daily-thoughts/"/>
    <id>http://brightliao.com/2023/07/02/daily-thoughts/</id>
    <published>2023-07-02T12:00:00.000Z</published>
    <updated>2023-07-25T12:51:22.876Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><h2 id="2023-07-02"><a class="markdownIt-Anchor" href="#2023-07-02"></a> 2023-07-02:</h2><p>去过成都环球中心海洋乐园的人一定会惊叹于这片人造海滩的宏伟，400多米的海岸线竟然硬生生的在一个商场里面给造了出来。海洋乐园前几年是封闭运营，必须买票才能进场到海滩边，我去过几次，发现都是门可罗雀的景象，大概是它的高冷向世人阻挡了它的宏伟。今天带上小孩一起来环球中心闲逛，发现海洋乐园竟然开放了，不用买票就可以走到海滩边。作为一个在环球中心闲逛的游客，看到一层一层卷起的巨浪，听着浪花冲向海滩的哗哗声及沙滩上人们的欢呼声，竟也想买张票下水体验一番了。封闭还是开放？到底要如何选择？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;/attaches/assets/katex/katex.min.css&quot;&gt;&lt;h2 id=&quot;2023-07-02&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; hre</summary>
      
    
    
    
    <category term="每日一思" scheme="http://brightliao.com/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E6%80%9D/"/>
    
    
  </entry>
  
  <entry>
    <title>突破ChatGPT的知识限制</title>
    <link href="http://brightliao.com/2023/06/02/chatgpt-long-context/"/>
    <id>http://brightliao.com/2023/06/02/chatgpt-long-context/</id>
    <published>2023-06-02T12:00:00.000Z</published>
    <updated>2023-06-28T03:45:46.311Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><p>作为一个一直对AI技术很感兴趣的软件开发工程师，早在深度学习开始火起来的15、16年，我也开始了相关技术的学习。当时还组织了公司内部同样有兴趣的同学一起研究，最终的成果汇集成几次社区中的分享以及几篇学习文章（见<a href="https://brightliao.com/tags/ai/">这里</a>）。</p><p>从去年OpenAI发布ChatGPT以来，AI的能力再次惊艳了世人。在这样的一个时间节点，重新去学习相关技术显得很有必要。</p><p>ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。</p><p>ChatGPT本身就具备很丰富的知识，所以ChatGPT自身实际上就是一个很好的学习渠道，我也将借助ChatGPT来学习ChatGPT。</p><p>这是此系列的第六篇，突破ChatGPT的知识限制。</p><p><img data-src="/attaches/2023/2023-06-02-chatgpt-long-context/content.png" alt="文章内容" /></p><span id="more"></span><h2 id="chatgpt的局限性"><a class="markdownIt-Anchor" href="#chatgpt的局限性"></a> ChatGPT的局限性</h2><p><a href="https://brightliao.com/2023/05/25/2023-05-25-chatgpt-rlhf/">上一篇</a>文章我们深入分析了ChatGPT如何结合奖励模型和强化学习算法进行自动优化。 OpenAI开放ChatGPT模型给大家使用，随着大家使用越多，产生的对话就越多，ChatGPT就能自动优化得更智能。</p><p>虽然ChatGPT能力很强，但是其局限性也很明显，对于没有纳入训练数据的知识它是不知道的，从而也没法给出我们期望的回复。但由于我们的日常工作往往要基于大量的ChatGPT不知道的背景知识进行判断和决策，所以很多场景我们也没法简单的通过向ChatGPT提问来得到想要的答案。</p><p>如何突破ChatGPT的知识限制，让ChatGPT具备更强大的能力，以便可以更好的辅助我们完成工作？这就是本文想要讨论的问题。</p><h2 id="扩充chatgpt的知识的方法"><a class="markdownIt-Anchor" href="#扩充chatgpt的知识的方法"></a> 扩充ChatGPT的知识的方法</h2><h3 id="基础模型优化"><a class="markdownIt-Anchor" href="#基础模型优化"></a> 基础模型优化</h3><p>如何让ChatGPT认知到这些复杂的背景知识？最简单的想法是把这些知识整理成文本，制作成数据集，然后让ChatGPT进行模型优化。</p><p>如何优化呢？前面ChatGPT模型训练内容我们分析了ChatGPT类模型的训练过程，分为三个阶段：基础语言模型训练、监督微调及指令微调。其中监督微调及指令微调阶段的数据量小，主要是解决模型的指令跟随问题。如果想在ChatGPT之上加入背景知识，看起来应该在基础语言模型训练阶段实施。</p><p>进行基础语言模型调优理论上是可行的，但是实践时会遇到很多问题。比如：</p><ul><li>如果新数据中存在错误、噪音或不准确的信息，将可能对模型的性能产生负面影响</li><li>使用这些新数据进行训练时，必须要非常小心的调整学习率，否则可能让ChatGPT遗忘之前学习到的通用知识，变得更笨</li><li>基础模型优化可能会影响第二和第三阶段的优化结果，使得ChatGPT没法很好的进行问答</li><li>模型本身太大，进行模型调优需要的计算资源更大，以目前的算力成本来看，很难负担得起</li></ul><p>目前来看，这一方法的可操作性不强。当然，在将来某一天，模型优化到一定程度，或者算力成本降低到一定程度时，也许这一方案也变得可行了。</p><h3 id="长上下文支持"><a class="markdownIt-Anchor" href="#长上下文支持"></a> 长上下文支持</h3><p>另一种扩充ChatGPT知识的方案是想办法增加模型的上下文支持能力。模型支持的上下文越长，表示我们可以输入给模型的内容越多，从而可以让ChatGPT基于更多的背景知识回答问题。</p><p>目前ChatGPT可以支持的上下文长度为4k或16k，而GPT-4支持的上下文长度为8k或32k（参考<a href="https://openai.com/pricing">这里</a>）。</p><p>从前面文章对于ChatGPT原理的分析来看，要想将上下文变长并不是一件容易的事。主要的挑战有：</p><ul><li><strong>显存消耗增长</strong>：由于计算自注意力结果时需要计算并保存所有上下文计算出来的中间结果，增加上下文长度会导致模型需要更多的显存来存储这些中间计算结果。</li><li><strong>计算复杂度增加</strong>：随着上下文长度的增加，每一个训练样本的计算复杂度都会增加，计算时间和计算资源的需求也会增加。</li><li><strong>长期依赖问题</strong>：更长的上下文导致信息在序列中的传播路径更长，这可能导致模型难以捕捉到远距离的依赖关系。</li></ul><p>然而，最重要的可能是并没有如此高质量的数据集供模型训练。特别是针对监督微调及指令微调阶段的数据集。</p><p>有很多分析和研究表明（参考<a href="https://juejin.cn/post/7249173717751087164">这里</a>），如果模型在4k上下文的数据集上面训练，是很难直接让它支持超过4k长度上下文的。具体表现就是模型性能严重下降。这就是所谓的<strong>模型的上下文长度外推能力</strong>。</p><p>目前有一些方法来增强模型的外推能力，如文章<a href="https://juejin.cn/post/7249173717751087164">《语言大模型100K上下文窗口的秘诀》</a>中提到的有：</p><ul><li><strong>ALiBi位置编码</strong>: 分两个阶段进行训练，首先在2K个词元的上下文长度上训练基本模型，然后在更长的上下文（例如65K）上进行微调。对于网络模型，则移除位置正弦嵌入，采用线性偏置注意力代替（通过附加一个与当前词元距离具有比例关系的惩罚来修正查询出来的注意力分数）。</li><li><strong>稀疏注意力机制</strong>：基于并非所有长上下文内容都是相互关联的这一假设，在计算注意力分数时仅考虑部分词元，具体实现有<a href="https://paperswithcode.com/method/sliding-window-attention">滑动窗口技术</a>、<a href="https://arxiv.org/abs/2007.14062">BigBird</a>等</li><li><strong>FlashAttention</strong>：在GPU的注意力层实现时，采用IO更高效的优化手段，以便可以支持快速训练与预测</li><li><strong>多查询注意力</strong>：优化模型中间结果缓存，在推理过程中能够显著加快增量注意力分数的计算</li></ul><p>最近的很多模型更新显示业界大家都在尝试实现更长的上下文支持。如：</p><ul><li>Anthropic在5.11日发布新的Cloude模型，表示支持100K上下文</li><li>OpenAI也在近期更新了GPT-3.5及GPT-4模型的上下文长度支持，分别从4K增加到16K和从8K增加到32K</li><li>国内清华系近期开源的ChatGLM2-6B模型的上下文长度从2K扩展到了32K</li></ul><p>显然，这已经是大模型开发厂商们正在努力的方向。</p><h3 id="基于文档搜索的知识扩充"><a class="markdownIt-Anchor" href="#基于文档搜索的知识扩充"></a> 基于文档搜索的知识扩充</h3><p>长上下文固然可以提高模型的背景知识，但在企业应用中，我们往往拥有海量的背景知识，全部依靠长上下文还是显得力不从心。</p><p>为了支持海量背景知识，可以对比参考我们人类解决问题的方法。一种常见的步骤是：</p><ol><li>查找相关文档，进行大量调研</li><li>汇总各类信息形成基本的解决方案</li><li>执行</li></ol><p>可以发现，即便人类所具有的背景知识也是有限的，人类也可以很好的解决问题。这是因为人可以主动的去查找相关信息，然后汇总形成方案。</p><p>这一思路就是基于搜索的思路，它是目前可操作性更强的方案。</p><p>在ChatGPT类的模型中应用搜索机制就变成：</p><ol><li>将所有知识文档搜集起来，然后拆分为小的文档，比如一篇文档2000字</li><li>对每一篇2000字的小文档进行向量化（采用大语言模型将其编码为向量，并使得这个向量与可能的问题具备较高的相关度），然后存储起来</li><li>在用户提出一个问题之后，将问题采用相同的方式编码为向量</li><li>将问题对应的向量与所有知识小文档向量计算相关度，选出相关度最大的几个文档</li><li>将这些选中的小文档提取出来，构造一个提示语传给大语言模型，让大语言模型根据这些上下文回答问题</li></ol><p><img data-src="/attaches/2023/2023-06-02-chatgpt-long-context/langchain-chatglm.png" alt="ChatGPT文档查询，图片来自：https://github.com/imClumsyPanda/langchain-ChatGLM" /></p><p>目前有很多开源库支持以这样的方式来扩充ChatGPT类模型的知识。比如<a href="https://gpt-index.readthedocs.io/en/latest/">LlamaIndex</a>、<a href="https://docs.langchain.com/docs/use-cases/qa-docs">LangChain</a>等。</p><p>它们的主要特点是：</p><ol><li>支持很多直接可用的经过验证文档向量化模型（如OpenAI的<a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">Ada模型</a>、基于LLAMA的各类开源模型等）</li><li>提供多种方式实现上述第4步中的相关文档查找，比如基于向量存储库、基于树索引等</li><li>提供了一些常见的小文档使用模式及对应的上述第5步中的提示语，比如直接组合小文档的方式，基于每个小文档一步一步优化答案的方式，并行基于每个小文档生成回复然后一次性组合的方式等，详见<a href="https://python.langchain.com/docs/modules/chains/document/">这里</a></li></ol><p>一个简单的用LangChain实现的用例只需要几行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"></span><br><span class="line">loader = TextLoader(<span class="string">&quot;path/to/your/long-text-doc&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line">openai_key = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_key=openai_key)</span><br><span class="line">docsearch = Chroma.from_documents(texts, embeddings)</span><br><span class="line"></span><br><span class="line">docsearch.similarity_search_with_relevance_scores(<span class="string">&#x27;你的问题&#x27;</span>)</span><br><span class="line"></span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_key), chain_type=<span class="string">&quot;refine&quot;</span>, retriever=docsearch.as_retriever())</span><br><span class="line"></span><br><span class="line">qa.run(<span class="string">&quot;你的问题&quot;</span>)</span><br></pre></td></tr></table></figure><p>虽然看起来基于搜索的方案是目前给大语言模型注入知识的最佳方案，但实际效果却未也必能达到预期。</p><p>从实现原理来看，有很多因素可以导致效果下降，比如大文档切分时将关键的文本句子拆分为了不合理的小文档，文档搜索漏掉了某些关键的文档，搜索出来的相关但非关键的文档排名更靠前，组合得到的提示语过于简单等。</p><p>事实上，人类在决策时大量还是基于记忆而非搜索的知识的，这部分可能只能通过第一种方式（基础模型优化）才能更好的解决。</p><h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2><p>本文分享了ChatGPT类大语言模型在当下应用时的局限性，并分析了改善的方法。虽然目前大语言模型还远未达到完美，但我们可以看到很多聪明的大脑正在为解决这个问题贡献极具创意的方案，相信不久的将来这些问题都会迎刃而解！</p><p>将来的大语言模型的应用会是怎样的？大胆的猜测一下，大概是每个人一个模型，每个团队一个模型，每个公司一个模型吧，这些大语言模型具备不同程度的背景知识，可以更精准更贴心的帮助我们更智慧的解决问题。</p><p>自ChatGPT发布以来，很多人认为这是一个人类走向通用人工智能的突破，也有一些人认为它其实没什么本质的改进。有很多人对自己的职业发展产生了很深的焦虑感，也有很多人感觉触碰到了科幻世界中的未来，还有很多人觉得又是一个可以好好捞一把的机会。</p><p>也许每个人都有必要去了解一下机器学习技术的原理，这样才能形成对它的理性的认知。</p><p>ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。</p><p>这是此系列的第六篇，突破ChatGPT的知识限制。</p><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><ul><li>语言大模型100K上下文窗口的秘诀： <a href="https://juejin.cn/post/7249173717751087164">https://juejin.cn/post/7249173717751087164</a></li><li>Transformer升级之路：7、长度外推性与局部注意力：<a href="https://kexue.fm/archives/9431">https://kexue.fm/archives/9431</a></li><li>Transformer升级之路：8、长度外推性与位置鲁棒性: <a href="https://kexue.fm/archives/9444">https://kexue.fm/archives/9444</a></li><li>Transformer升级之路：9、一种全局长度外推的新思路: <a href="https://kexue.fm/archives/9603">https://kexue.fm/archives/9603</a></li><li>The Secret Sauce behind 100K context window in LLMs: <a href="https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c">https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c</a></li><li>What are Embeddings: <a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">https://platform.openai.com/docs/guides/embeddings/what-are-embeddings</a></li><li>FlashAttention: <a href="https://shreyansh26.github.io/post/2023-03-26_flash-attention/">https://shreyansh26.github.io/post/2023-03-26_flash-attention/</a></li><li>Fast Transformer Decoding: One Write-Head is All You Need: <a href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a></li><li>LangChain-Question answering over documents: <a href="https://python.langchain.com/docs/use_cases/question_answering.html">https://python.langchain.com/docs/use_cases/question_answering.html</a></li></ul><!--from: https://markmap.js.org/repl# 突破ChatGPT的知识限制## ChatGPT的局限性### 没有纳入训练数据的知识它是不知道## 扩充ChatGPT的知识的方法### 基础模型优化- 实践时会遇到很多问题, 可操作性不强### 长上下文支持#### 示例- Cloude: 100K- GPT-3.5: 4K -> 16K- GPT-4: 8K -> 32K- ChatGLM2-6B: 2K -> 32K#### 方法- ALiBi位置编码- 稀疏注意力机制- FlashAttention- 多查询注意力### 基于文档搜索的知识扩充- LlamaIndex- LangChain- SemanticKernel-->]]></content>
    
    
    <summary type="html">&lt;p&gt;作为一个一直对AI技术很感兴趣的软件开发工程师，早在深度学习开始火起来的15、16年，我也开始了相关技术的学习。当时还组织了公司内部同样有兴趣的同学一起研究，最终的成果汇集成几次社区中的分享以及几篇学习文章（见&lt;a href=&quot;https://brightliao.com/tags/ai/&quot;&gt;这里&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;从去年OpenAI发布ChatGPT以来，AI的能力再次惊艳了世人。在这样的一个时间节点，重新去学习相关技术显得很有必要。&lt;/p&gt;
&lt;p&gt;ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。&lt;/p&gt;
&lt;p&gt;ChatGPT本身就具备很丰富的知识，所以ChatGPT自身实际上就是一个很好的学习渠道，我也将借助ChatGPT来学习ChatGPT。&lt;/p&gt;
&lt;p&gt;这是此系列的第六篇，突破ChatGPT的知识限制。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/attaches/2023/2023-06-02-chatgpt-long-context/content.png&quot; alt=&quot;文章内容&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="machine-learning" scheme="http://brightliao.com/categories/machine-learning/"/>
    
    <category term="ChatGPT" scheme="http://brightliao.com/categories/machine-learning/chatgpt/"/>
    
    
    <category term="AI" scheme="http://brightliao.com/tags/ai/"/>
    
    <category term="机器学习" scheme="http://brightliao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="ML" scheme="http://brightliao.com/tags/ml/"/>
    
    <category term="ChatGPT" scheme="http://brightliao.com/tags/chatgpt/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT的自动优化</title>
    <link href="http://brightliao.com/2023/05/25/chatgpt-rlhf/"/>
    <id>http://brightliao.com/2023/05/25/chatgpt-rlhf/</id>
    <published>2023-05-25T12:00:00.000Z</published>
    <updated>2023-06-27T08:06:18.278Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><p>作为一个一直对AI技术很感兴趣的软件开发工程师，早在深度学习开始火起来的15、16年，我也开始了相关技术的学习。当时还组织了公司内部同样有兴趣的同学一起研究，最终的成果汇集成几次社区中的分享以及几篇学习文章（见<a href="https://brightliao.com/tags/ai/">这里</a>）。</p><p>从去年OpenAI发布ChatGPT以来，AI的能力再次惊艳了世人。在这样的一个时间节点，重新去学习相关技术显得很有必要。</p><p>ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。</p><p>ChatGPT本身就具备很丰富的知识，所以ChatGPT自身实际上就是一个很好的学习渠道，我也将借助ChatGPT来学习ChatGPT。</p><p>这是此系列的第五篇，ChatGPT的自动优化。</p><span id="more"></span><p><a href="https://brightliao.com/2023/05/20/2023-05-20-chatgpt-training/">上一篇</a>文章我们深入分析了ChatGPT是如何训练及优化的，了解了如何进行监督微调，及如何让模型可以支持更广泛领域的问答。但是，监督微调始终会限于训练集中的问题模板数量，无法支持更为一般的对话。这一步骤就需要引入强化学习的训练方式，让ChatGPT可以自动进行优化。</p><h2 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h2><p>强化学习模型是最为复杂的部分，参考ColossalAI的文档，模型的工作原理如下：</p><p><img data-src="/attaches/2023/2023-05-25-chatgpt-rlhf/rlhf.jpeg" alt="RLHF" /></p><p>为了理解上图，需要先了解一下强化学习相关的背景知识。</p><h3 id="强化学习"><a class="markdownIt-Anchor" href="#强化学习"></a> 强化学习</h3><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习方法，旨在使智能体（agent）通过与环境的交互来学习适应环境并制定实现特定目标的策略。在强化学习中，智能体通过观察环境的状态，执行动作，并接收环境的奖励或惩罚来不断调整自己的策略，以获得最大化累积奖励的能力。</p><p>强化学习的基本要素包括：</p><ul><li><strong>智能体（Agent）</strong>：智能体是进行学习和决策的主体，它通过观察环境的状态、选择合适的动作，并与环境进行交互。</li><li><strong>环境（Environment）</strong>：环境是智能体所处的外部世界，它可以是真实的物理环境，也可以是抽象的模拟环境。环境会根据智能体的动作进行状态转移，并根据智能体的表现给予奖励或惩罚。</li><li><strong>状态（State）</strong>：状态是描述环境的特征或信息，它可以是完全观察的，也可以是部分观察或隐含的。智能体的决策往往基于当前状态。</li><li><strong>动作（Action）</strong>：动作是智能体在某个状态下可以执行的操作或策略。智能体的目标是根据当前状态选择最优的动作。</li><li><strong>奖励（Reward）</strong>：奖励是环境根据智能体的动作和表现给予的反馈信号，用于指示动作的好坏。智能体的目标是通过最大化累积奖励来学习合适的策略。</li></ul><p>在ChatGPT这个场景中，ChatGPT模型即智能体，环境是一个对话系统，状态是当前对话的上下文及当前消息，动作是如何选择某一个回复，奖励是人类反馈的回复质量好或差。</p><p>强化学习的核心问题是通过智能体与环境的交互来学习一个最优的策略，以使智能体在长期累积奖励的过程中能够获得最大化的回报。强化学习算法通常基于价值函数或策略函数来进行决策和优化，其中价值函数用于评估状态或状态动作对的价值，策略函数用于指导智能体在特定状态下选择动作。</p><p>强化学习常常应用于机器人控制、游戏智能、自动驾驶等领域。</p><h3 id="强化学习算法"><a class="markdownIt-Anchor" href="#强化学习算法"></a> 强化学习算法</h3><p>如何学习最优策略呢？常见的学习算法包括Q-learning、SARSA、Deep Q-Network（DQN）、Policy Gradient、Proximal Policy Optimization（PPO）、Actor-Critic等。</p><p>以下简要介绍和RLHF相关的算法：</p><ul><li>Q-learning: 核心思想是学习一个状态-动作价值函数（Q函数），它衡量在给定状态下采取特定动作的长期累积回报。可根据贝尔曼公式 <code>Q(s,a) = Q(s,a) + α(r + γ * max(Q(s',a')) - Q(s,a))</code> 更新及优化Q函数，其中α是学习率，γ是折扣因子，r是奖励，s’是新的状态。</li><li>Policy Gradient（策略梯度）算法: 不需要建立值函数模型，而是直接优化策略（动作的选择）。其基本思想是通过采样经验轨迹（trajectory），通过最大化累积奖励来计算策略梯度，并利用梯度信息更新策略参数。</li><li>Proximal Policy Optimization（PPO）：一种基于策略梯度的强化学习算法，旨在通过有效地优化策略函数（通过引入一个重要性采样比率和一个剪切函数来限制策略更新的幅度，以保持策略的相对不变性）来提高强化学习的性能和稳定性。</li><li>Actor-Critic（演员-评论家）算法：结合了值函数和策略函数的强化学习算法。它通过同时学习一个策略函数（演员）和一个值函数（评论家），以提高强化学习的效率和性能。演员根据评论家的评估结果来更新策略，从而改进策略的质量。评论家则通过学习一个值函数来估计每个状态的值或动作值，以提供演员关于策略改进的反馈。</li></ul><p>RLHF算法结合了PPO和Actor-Critic算法的优势，所以可以高效而又稳定的优化ChatGPT的模型。</p><h2 id="代码分析"><a class="markdownIt-Anchor" href="#代码分析"></a> 代码分析</h2><p>有了前面的了解，下面咱们跟着代码一起来了解一下算法的细节。</p><h3 id="使用pytorch实现policy-gradient"><a class="markdownIt-Anchor" href="#使用pytorch实现policy-gradient"></a> 使用PyTorch实现Policy Gradient</h3><p>下面来看PyTorch的示例中提供的一个参考的策略梯度算法实现。</p><p>先介绍一下<code>gym</code>库，这个库提供了一个模拟环境，内置了很多小游戏，可以帮助我们开发强化学习算法。</p><p>比如下面这个平衡杆小游戏，我们要想办法控制平衡杆使其一直位于连接点的上方。有两个动作可以用来控制游戏中的连接点，即左和右。控制连接点向左时，可以避免平衡杆往左倾倒。控制连接点向右时，可以避免平衡杆往右倾倒。</p><p><img data-src="/attaches/2023/2023-05-25-chatgpt-rlhf/cartpole.png" alt="Cart Pole" /></p><p>下面来用策略梯度的方法训练一个强化学习算法让机器人自动玩游戏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Policy</span>(nn.Module): <span class="comment"># 定义策略函数网络，输出每个动作对应的概率</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        <span class="comment"># 定义网络结构，包含了两层线性连接层，第二层输出的动作数量为2</span></span><br><span class="line">        self.affine1 = nn.Linear(<span class="number">4</span>, <span class="number">128</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.6</span>)</span><br><span class="line">        self.affine2 = nn.Linear(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.saved_log_probs = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.dropout(self.affine1(x)))</span><br><span class="line">        action_scores = self.affine2(x)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(action_scores, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyTrainer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 初始化游戏环境</span></span><br><span class="line">        self.env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>, render_mode=<span class="string">&quot;rgb_array&quot;</span>)</span><br><span class="line">        <span class="comment"># 初始化策略函数网络及优化器</span></span><br><span class="line">        self.policy = Policy()</span><br><span class="line">        self.optimizer = optim.Adam(self.policy.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">        self.eps = np.finfo(np.float32).eps.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="comment"># 根据当前的状态，执行策略函数，并根据函数输出的概率选择一个动作</span></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        probs = self.policy(state)</span><br><span class="line">        m = Categorical(probs)</span><br><span class="line">        action = m.sample()</span><br><span class="line">        <span class="comment"># 将动作保存起来，在一局游戏结束的时候，用于训练</span></span><br><span class="line">        self.policy.saved_log_probs.append(m.log_prob(action))</span><br><span class="line">        <span class="keyword">return</span> action.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">finish_episode</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 在一局游戏结束的时候，执行策略函数的训练</span></span><br><span class="line">        <span class="comment"># 根据执行动作时保存的奖励，来迭代计算每一个动作的奖励</span></span><br><span class="line">        <span class="comment"># 每一个动作的奖励 = 当前奖励 + 折扣率 * 整局游戏中将来的奖励</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        returns = deque()</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> self.policy.rewards[::-<span class="number">1</span>]:</span><br><span class="line">            R = r + args.gamma * R</span><br><span class="line">            returns.appendleft(R)</span><br><span class="line">        returns = torch.tensor(returns)</span><br><span class="line">        <span class="comment"># 将奖励归一化</span></span><br><span class="line">        returns = (returns - returns.mean()) / (returns.std() + self.eps)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 采用梯度上升法最大化策略奖励，这里使用最小化策略奖励的负数来实现</span></span><br><span class="line">        policy_loss = []</span><br><span class="line">        <span class="keyword">for</span> log_prob, R <span class="keyword">in</span> <span class="built_in">zip</span>(self.policy.saved_log_probs, returns):</span><br><span class="line">            <span class="comment"># 计算每一个动作的策略损失，策略损失 = -动作概率 * 动作奖励</span></span><br><span class="line">            policy_loss.append(-log_prob * R)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据整局游戏的结果来计算梯度，并更新参数</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()</span><br><span class="line">        policy_loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> self.policy.rewards[:]</span><br><span class="line">        <span class="keyword">del</span> self.policy.saved_log_probs[:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        running_reward = <span class="number">10</span></span><br><span class="line">        <span class="keyword">for</span> i_episode <span class="keyword">in</span> count(<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 新的一局游戏开始，初始化环境</span></span><br><span class="line">            state, _ = self.env.reset()</span><br><span class="line">            ep_reward = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):  <span class="comment"># Don&#x27;t infinite loop while learning</span></span><br><span class="line">                <span class="comment"># 采用策略函数来生成下一步采用的动作，并执行</span></span><br><span class="line">                action = self.select_action(state)</span><br><span class="line">                state, reward, done, _, _ = self.env.step(action)</span><br><span class="line">                <span class="comment"># 记录奖励</span></span><br><span class="line">                self.policy.rewards.append(reward)</span><br><span class="line">                ep_reward += reward  <span class="comment"># 累计计算当前这一局游戏的奖励</span></span><br><span class="line">                <span class="keyword">if</span> done: <span class="comment"># 如果游戏结束，则退出循环</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在一局游戏结束是，计算奖励的移动平均值</span></span><br><span class="line">            <span class="comment"># 这里将当前这一局游戏的奖励以5%的百分比给平衡掉，让我们更容易看出游戏当前能得到的奖励</span></span><br><span class="line">            running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">            <span class="comment"># 触发策略网络更新</span></span><br><span class="line">            self.finish_episode()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 周期性打印日志</span></span><br><span class="line">            <span class="keyword">if</span> i_episode % args.log_interval == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(i_episode, ep_reward, running_reward))</span><br><span class="line">            <span class="keyword">if</span> running_reward &gt; self.env.spec.reward_threshold:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span> <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>完整代码见<a href="https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py">这里</a>。运行以上算法可以看到以下日志：</p><blockquote><p>Episode 10      Last reward: 21.00      Average reward: 16.30<br />Episode 20      Last reward: 41.00      Average reward: 24.02<br />Episode 30      Last reward: 33.00      Average reward: 33.05<br />Episode 40      Last reward: 64.00      Average reward: 50.71<br />Episode 50      Last reward: 73.00      Average reward: 59.70<br />Episode 60      Last reward: 41.00      Average reward: 63.28<br />Episode 70      Last reward: 59.00      Average reward: 63.88<br />Episode 80      Last reward: 86.00      Average reward: 80.87<br />Episode 90      Last reward: 125.00     Average reward: 91.54<br />Episode 100     Last reward: 224.00     Average reward: 136.09<br />Episode 110     Last reward: 95.00      Average reward: 182.31<br />Episode 120     Last reward: 200.00     Average reward: 170.03<br />Episode 130     Last reward: 80.00      Average reward: 149.48<br />Episode 140     Last reward: 102.00     Average reward: 148.63<br />Episode 150     Last reward: 644.00     Average reward: 349.26<br />Solved! Running reward is now 550.1608406568259 and the last episode runs to 2888 time steps!</p></blockquote><p>可以看到，在算法玩了150多次游戏的时候，已经可以玩得非常好了。但是我们也能注意到算法有一些波动，特别是在100局到110局时，曾经达到一个不错的水平，但是后来突然又有一些下降。</p><h3 id="使用pytorch实现actor-critic"><a class="markdownIt-Anchor" href="#使用pytorch实现actor-critic"></a> 使用PyTorch实现Actor-Critic</h3><p>Actor-Critic算法与Policy Gradient算法是类似的。下面看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">SavedAction = namedtuple(<span class="string">&quot;SavedAction&quot;</span>, [<span class="string">&quot;log_prob&quot;</span>, <span class="string">&quot;value&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Policy</span>(nn.Module): <span class="comment"># 定义演员函数和评论家函数网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.affine1 = nn.Linear(<span class="number">4</span>, <span class="number">128</span>)</span><br><span class="line">        self.action_head = nn.Linear(<span class="number">128</span>, <span class="number">2</span>) <span class="comment"># 演员函数输出每个动作对应的概率</span></span><br><span class="line">        self.value_head = nn.Linear(<span class="number">128</span>, <span class="number">1</span>) <span class="comment"># 评论家函数输出每个动作对应的奖励</span></span><br><span class="line">        <span class="comment"># action &amp; reward buffer</span></span><br><span class="line">        self.saved_actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.affine1(x))</span><br><span class="line">        action_prob = F.softmax(self.action_head(x), dim=-<span class="number">1</span>)</span><br><span class="line">        state_values = self.value_head(x)</span><br><span class="line">        <span class="keyword">return</span> action_prob, state_values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyTrainer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>)</span><br><span class="line">        self.model = Policy()</span><br><span class="line">        self.optimizer = optim.Adam(self.model.parameters(), lr=<span class="number">3e-2</span>)</span><br><span class="line">        self.eps = np.finfo(np.float32).eps.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>()</span><br><span class="line">        probs, state_value = self.model(state)</span><br><span class="line">        m = Categorical(probs)</span><br><span class="line">        action = m.sample()</span><br><span class="line">        self.model.saved_actions.append(SavedAction(m.log_prob(action), state_value))</span><br><span class="line">        <span class="keyword">return</span> action.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">finish_episode</span>(<span class="params">self</span>):</span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        saved_actions = self.model.saved_actions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每一个步骤的奖励</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> self.model.rewards[::-<span class="number">1</span>]:</span><br><span class="line">            R = r + args.gamma * R</span><br><span class="line">            returns.insert(<span class="number">0</span>, R)</span><br><span class="line"></span><br><span class="line">        returns = torch.tensor(returns)</span><br><span class="line">        returns = (returns - returns.mean()) / (returns.std() + self.eps)</span><br><span class="line"></span><br><span class="line">        policy_losses = []</span><br><span class="line">        value_losses = []</span><br><span class="line">        <span class="keyword">for</span> (log_prob, value), R <span class="keyword">in</span> <span class="built_in">zip</span>(saved_actions, returns):</span><br><span class="line">            <span class="comment"># 计算真实奖励与评论家函数估计的奖励之差，并将其用于计算演员函数的损失</span></span><br><span class="line">            advantage = R - value.item()</span><br><span class="line">            policy_losses.append(-log_prob * advantage)</span><br><span class="line">            <span class="comment"># 评论家函数的损失使用平滑L1损失函数（与均方差损失类似，但更稳定）</span></span><br><span class="line">            value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算梯度并更新网络</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss = torch.stack(policy_losses).<span class="built_in">sum</span>() + torch.stack(value_losses).<span class="built_in">sum</span>()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清理数据</span></span><br><span class="line">        <span class="keyword">del</span> self.model.rewards[:]</span><br><span class="line">        <span class="keyword">del</span> self.model.saved_actions[:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 训练过程与策略梯度方法类似</span></span><br><span class="line">        running_reward = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i_episode <span class="keyword">in</span> count(<span class="number">1</span>):</span><br><span class="line">            state, _ = self.env.reset()</span><br><span class="line">            ep_reward = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):</span><br><span class="line">                action = self.select_action(state)</span><br><span class="line">                state, reward, done, _, _ = self.env.step(action)</span><br><span class="line">                self.model.rewards.append(reward)</span><br><span class="line">                ep_reward += reward</span><br><span class="line">                <span class="keyword">if</span> done:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line"></span><br><span class="line">            self.finish_episode()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># log results</span></span><br><span class="line">            <span class="keyword">if</span> i_episode % args.log_interval == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(i_episode, ep_reward, running_reward))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if we have &quot;solved&quot; the cart pole problem</span></span><br><span class="line">            <span class="keyword">if</span> running_reward &gt; self.env.spec.reward_threshold:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span> <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>完整代码见<a href="https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py">这里</a>。运行以上算法可以看到以下日志：</p><blockquote><p>Episode 10      Last reward: 32.00      Average reward: 14.11<br />Episode 20      Last reward: 89.00      Average reward: 32.64<br />Episode 30      Last reward: 20.00      Average reward: 45.73<br />Episode 40      Last reward: 32.00      Average reward: 47.44<br />Episode 50      Last reward: 332.00     Average reward: 142.78<br />Episode 60      Last reward: 410.00     Average reward: 428.13<br />Solved! Running reward is now 476.8880228063767 and the last episode runs to 1334 time steps!</p></blockquote><p>可以看到算法在经历60多次的迭代之后就有一个很好的效果了。</p><h2 id="chatgpt的rlhf算法"><a class="markdownIt-Anchor" href="#chatgpt的rlhf算法"></a> ChatGPT的RLHF算法</h2><p>ColossalAI中使用的强化学习算法与上述算法基本一致，完整代码的入口在<a href="https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py">这里</a>， 代码比较长，以下是简化后的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    strategy = ColossalAIStrategy(stage=<span class="number">2</span>, placement_policy=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> strategy.model_init_context():</span><br><span class="line">        initial_model = LlamaActor(pretrained=args.pretrain)</span><br><span class="line">        reward_model = LlamaRM(pretrained=args.rm_pretrain)</span><br><span class="line">        actor = LlamaActor(pretrained=args.pretrain, lora_rank=args.lora_rank)</span><br><span class="line">        critic = LlamaCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank, use_action_mask=<span class="literal">True</span>)</span><br><span class="line">    actor_optim = HybridAdam(actor.parameters(), lr=<span class="number">1e-7</span>)</span><br><span class="line">    critic_optim = HybridAdam(critic.parameters(), lr=<span class="number">1e-7</span>)</span><br><span class="line">    tokenizer = LlamaTokenizer.from_pretrained(args.pretrain)</span><br><span class="line">    tokenizer = prepare_llama_tokenizer_and_embedding(tokenizer, actor)</span><br><span class="line"></span><br><span class="line">    prompt_dataset = PromptDataset(tokenizer=tokenizer, data_path=args.prompt_dataset, max_datasets_size=<span class="number">16384</span>)</span><br><span class="line">    prompt_dataloader = DataLoader(prompt_dataset, ...)</span><br><span class="line">    pretrain_dataset = SupervisedDataset(tokenizer=tokenizer, ...)</span><br><span class="line">    pretrain_dataloader = DataLoader(pretrain_dataset, ...)</span><br><span class="line"></span><br><span class="line">    (actor, actor_optim), (critic, critic_optim) = strategy.prepare((actor, actor_optim), (critic, critic_optim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># configure trainer</span></span><br><span class="line">    trainer = PPOTrainer(strategy, actor, critic, reward_model, initial_model, ...)</span><br><span class="line">    trainer.fit(prompt_dataloader, pretrain_dataloader, args.num_episodes, ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Critic</span>(<span class="title class_ inherited__">LoRAModule</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line">        self.convert_to_lora()  <span class="comment"># 将Critic模型转换为LoRA模型</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sequences: torch.LongTensor, action_mask, attention_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">        outputs = self.model(sequences, attention_mask=attention_mask)  <span class="comment"># 模型前向传播</span></span><br><span class="line">        <span class="comment"># 获取最后一层隐藏状态，并通过value_head线性层得到值函数估计值</span></span><br><span class="line">        last_hidden_states = outputs[<span class="string">&#x27;last_hidden_state&#x27;</span>]</span><br><span class="line">        values = self.value_head(last_hidden_states).squeeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> action_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> self.use_action_mask:</span><br><span class="line">            num_actions = action_mask.size(<span class="number">1</span>)</span><br><span class="line">            prompt_mask = attention_mask[:, :-num_actions]</span><br><span class="line">            values = values[:, :-num_actions]</span><br><span class="line">            value = masked_mean(values, prompt_mask, dim=<span class="number">1</span>) <span class="comment"># 根据动作掩码计算平均值</span></span><br><span class="line">            <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line">        values = values[:, :-<span class="number">1</span>]</span><br><span class="line">        value = values.mean(dim=<span class="number">1</span>)  <span class="comment"># 计算平均值作为最终的评论家函数估计值</span></span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaCritic</span>(<span class="title class_ inherited__">Critic</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        model = LlamaModel.from_pretrained(pretrained)  <span class="comment"># 使用预训练的LlamaModel初始化模型</span></span><br><span class="line">        value_head = nn.Linear(model.config.hidden_size, <span class="number">1</span>)  <span class="comment"># 使用线性层作为评论家函数头部</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Actor</span>(<span class="title class_ inherited__">LoRAModule</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model: nn.Module, ...</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.convert_to_lora()  <span class="comment"># 将Actor模型转换为LoRA模型</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, input_ids: torch.Tensor, return_action_mask: <span class="built_in">bool</span> = <span class="literal">True</span>, **kwargs</span>):</span><br><span class="line">        sequences = generate(self.model, input_ids, **kwargs)  <span class="comment"># 生成序列</span></span><br><span class="line">        attention_mask = <span class="literal">None</span></span><br><span class="line">        attention_mask = sequences.not_equal(pad_token_id)  <span class="comment"># 生成注意力掩码</span></span><br><span class="line">        <span class="comment"># left padding may be applied, only mask action</span></span><br><span class="line">        action_mask = (sequences[:, input_len:] == eos_token_id).cumsum(dim=-<span class="number">1</span>) == <span class="number">0</span>  <span class="comment"># 生成动作掩码</span></span><br><span class="line">        action_mask = F.pad(action_mask, (<span class="number">1</span> + input_len, -<span class="number">1</span>), value=<span class="literal">True</span>)    <span class="comment"># include eos token and input</span></span><br><span class="line">        action_mask[:, :input_len] = <span class="literal">False</span></span><br><span class="line">        action_mask = action_mask[:, <span class="number">1</span>:]</span><br><span class="line">        <span class="comment"># 返回生成的序列、注意力掩码和动作掩码</span></span><br><span class="line">        <span class="keyword">return</span> sequences, attention_mask, action_mask[:, -(sequences.size(<span class="number">1</span>) - input_len):]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sequences: torch.LongTensor, num_actions: <span class="built_in">int</span>, attention_mask</span>):</span><br><span class="line">        output = self.model(sequences, attention_mask=attention_mask)  <span class="comment"># 模型前向传播</span></span><br><span class="line">        <span class="comment"># 从logits计算动作的对数概率</span></span><br><span class="line">        log_probs = log_probs_from_logits(output[<span class="string">&#x27;logits&#x27;</span>][:, :-<span class="number">1</span>, :], sequences[:, <span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> log_probs[:, -num_actions:]  <span class="comment"># 返回动作的对数概率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaActor</span>(<span class="title class_ inherited__">Actor</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        model = LlamaForCausalLM.from_pretrained(pretrained)  <span class="comment"># 使用预训练的LlamaForCausalLM初始化模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PPOTrainer</span>(<span class="title class_ inherited__">Trainer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 初始化PPO训练器的各个组件和参数</span></span><br><span class="line">        self.experience_maker = NaiveExperienceMaker(actor, critic, reward_model, initial_model, kl_coef)</span><br><span class="line">        self.replay_buffer = NaiveReplayBuffer(train_batch_size, buffer_limit, buffer_cpu_offload)</span><br><span class="line"></span><br><span class="line">        self.actor = actor</span><br><span class="line">        self.critic = critic</span><br><span class="line"></span><br><span class="line">        self.actor_loss_fn = PolicyLoss(eps_clip)  <span class="comment"># 演员损失函数</span></span><br><span class="line">        self.critic_loss_fn = ValueLoss(value_clip)  <span class="comment"># 评论家损失函数</span></span><br><span class="line">        self.vf_coef = vf_coef</span><br><span class="line">        self.ptx_loss_fn = GPTLMLoss()  <span class="comment"># 预训练损失函数</span></span><br><span class="line">        self.ptx_coef = ptx_coef</span><br><span class="line">        self.actor_optim = actor_optim   <span class="comment"># 演员网络的优化器</span></span><br><span class="line">        self.critic_optim = critic_optim  <span class="comment"># 评论家网络的优化器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_learn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 根据是否使用重放缓冲区选择不同的训练方式</span></span><br><span class="line">        <span class="keyword">if</span> self.sample_replay_buffer:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.max_epochs):</span><br><span class="line">                experience = self.replay_buffer.sample()  <span class="comment"># 从重放缓冲区中采样经验</span></span><br><span class="line">                metrics = self.training_step(experience)  <span class="comment"># 执行训练步骤</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.max_epochs):</span><br><span class="line">                <span class="keyword">for</span> experience <span class="keyword">in</span> dataloader:  <span class="comment"># 从数据集中获取经验</span></span><br><span class="line">                    metrics = self.training_step(experience)  <span class="comment"># 执行训练步骤</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, prompt_dataloader, pretrain_dataloader</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        time = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">            <span class="keyword">for</span> timestep <span class="keyword">in</span> <span class="built_in">range</span>(max_timesteps):</span><br><span class="line">                time += <span class="number">1</span></span><br><span class="line">                prompts = <span class="built_in">next</span>(<span class="built_in">iter</span>(self.prompt_dataloader))  <span class="comment"># 获取输入提示数据</span></span><br><span class="line">                <span class="comment"># 生成经验，这里可以支持在线和人进行对话</span></span><br><span class="line">                experience = self.experience_maker.make_experience(prompts, **self.generate_kwargs)</span><br><span class="line">                self.replay_buffer.append(experience)  <span class="comment"># 将经验添加到重放缓冲区</span></span><br><span class="line">                <span class="keyword">if</span> time % update_timesteps == <span class="number">0</span>:</span><br><span class="line">                    self._learn()  <span class="comment"># 执行模型更新</span></span><br><span class="line">                    self.replay_buffer.clear()  <span class="comment"># 清空重放缓冲区</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, experience: Experience</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">float</span>]:</span><br><span class="line">        self.actor.train()  <span class="comment"># 设置演员网络为训练模式</span></span><br><span class="line">        self.critic.train()  <span class="comment"># 设置评论家网络为训练模式</span></span><br><span class="line">        <span class="comment"># 计算演员网络的动作对数概率</span></span><br><span class="line">        num_actions = experience.action_mask.size(<span class="number">1</span>)</span><br><span class="line">        action_log_probs = self.actor(experience.sequences, num_actions, attention_mask=experience.attention_mask)</span><br><span class="line">        <span class="comment"># 计算演员损失函数</span></span><br><span class="line">        actor_loss = self.actor_loss_fn(</span><br><span class="line">            action_log_probs, experience.action_log_probs, experience.advantages, experience.action_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算预训练损失函数</span></span><br><span class="line">        <span class="keyword">if</span> self.ptx_coef != <span class="number">0</span>:</span><br><span class="line">            batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(self.pretrain_dataloader))</span><br><span class="line">            ptx_log_probs = self.actor.get_base_model()(batch[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">                                                        attention_mask=batch[<span class="string">&#x27;attention_mask&#x27;</span>])[<span class="string">&#x27;logits&#x27;</span>]</span><br><span class="line">            ptx_loss = self.ptx_loss_fn(ptx_log_probs, batch[<span class="string">&#x27;labels&#x27;</span>])</span><br><span class="line">            actor_loss = ptx_loss * self.ptx_coef + actor_loss * (<span class="number">1</span> - self.ptx_coef)</span><br><span class="line"></span><br><span class="line">        self.strategy.backward(actor_loss, self.actor, self.actor_optim)  <span class="comment"># 演员网络的反向传播</span></span><br><span class="line">        self.strategy.optimizer_step(self.actor_optim)  <span class="comment"># 演员网络的优化器步骤</span></span><br><span class="line">        self.actor_optim.zero_grad()  <span class="comment"># 清空演员网络的梯度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算评论家损失函数</span></span><br><span class="line">        values = self.critic(experience.sequences, experience.action_mask, experience.attention_mask)</span><br><span class="line">        critic_loss = self.critic_loss_fn(values, experience.values, experience.reward, experience.action_mask)</span><br><span class="line">        critic_loss = critic_loss * self.vf_coef</span><br><span class="line">        self.strategy.backward(critic_loss, self.critic, self.critic_optim)  <span class="comment"># 评论家网络的反向传播</span></span><br><span class="line">        self.strategy.optimizer_step(self.critic_optim)  <span class="comment"># 评论家网络的优化器步骤</span></span><br><span class="line">        self.critic_optim.zero_grad()  <span class="comment"># 清空评论家网络的梯度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTLMLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logits: torch.Tensor, labels: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># 将logits向左移动一位，去掉最后一个时间步的预测</span></span><br><span class="line">        shift_logits = logits[..., :-<span class="number">1</span>, :].contiguous()</span><br><span class="line">        <span class="comment"># 将标签向右移动一位，去掉第一个时间步的标签</span></span><br><span class="line">        shift_labels = labels[..., <span class="number">1</span>:].contiguous()</span><br><span class="line">        <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">        <span class="keyword">return</span> self.loss(shift_logits.view(-<span class="number">1</span>, shift_logits.size(-<span class="number">1</span>)), shift_labels.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, clip_eps: <span class="built_in">float</span> = <span class="number">0.2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.clip_eps = clip_eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, log_probs, old_log_probs, advantages, action_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># 计算当前动作对数概率和旧动作对数概率的比例</span></span><br><span class="line">        ratio = (log_probs - old_log_probs).exp()</span><br><span class="line">        surr1 = ratio * advantages  <span class="comment"># 第一项损失计算</span></span><br><span class="line">        surr2 = ratio.clamp(<span class="number">1</span> - self.clip_eps, <span class="number">1</span> + self.clip_eps) * advantages <span class="comment"># 第二项损失计算</span></span><br><span class="line">        loss = -torch.<span class="built_in">min</span>(surr1, surr2)  <span class="comment"># 选取较小的损失</span></span><br><span class="line">        <span class="keyword">if</span> action_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = masked_mean(loss, action_mask)  <span class="comment"># 根据动作掩码计算平均损失</span></span><br><span class="line">        loss = loss.mean()  <span class="comment"># 计算平均损失</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ValueLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, clip_eps: <span class="built_in">float</span> = <span class="number">0.4</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.clip_eps = clip_eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, values, old_values, reward, action_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># 对奖励进行裁剪</span></span><br><span class="line">        values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)</span><br><span class="line">        surr1 = (values_clipped - reward)**<span class="number">2</span>  <span class="comment"># 第一项损失计算</span></span><br><span class="line">        surr2 = (values - reward)**<span class="number">2</span> <span class="comment"># 第二项损失计算</span></span><br><span class="line">        loss = torch.<span class="built_in">max</span>(surr1, surr2)  <span class="comment"># 选取较大的损失</span></span><br><span class="line">        loss = loss.mean()  <span class="comment"># 计算平均损失</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NaiveExperienceMaker</span>(<span class="title class_ inherited__">ExperienceMaker</span>):</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_experience</span>(<span class="params">self, input_ids: torch.Tensor, **generate_kwargs</span>) -&gt; Experience:</span><br><span class="line">        <span class="comment"># 基于演员函数生成回复及掩码</span></span><br><span class="line">        sequences, attention_mask, action_mask = self.actor.generate(input_ids, ...)</span><br><span class="line">        num_actions = action_mask.size(<span class="number">1</span>)  <span class="comment"># 获取动作的数量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算动作的对数概率</span></span><br><span class="line">        action_log_probs = self.actor(sequences, num_actions, attention_mask)</span><br><span class="line">        <span class="comment"># 使用初始模型计算动作的对数概率</span></span><br><span class="line">        base_action_log_probs = self.initial_model(sequences, num_actions, attention_mask)</span><br><span class="line">        <span class="comment"># 计算价值</span></span><br><span class="line">        value = self.critic(sequences, action_mask, attention_mask)</span><br><span class="line">        <span class="comment"># 计算基础奖励值</span></span><br><span class="line">        r = self.reward_model(sequences, attention_mask)</span><br><span class="line">        <span class="comment"># 基于动作概率调整奖励值</span></span><br><span class="line">        reward = compute_reward(r, self.kl_coef, action_log_probs, base_action_log_probs, action_mask=action_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算优势函数</span></span><br><span class="line">        advantage = reward - value</span><br><span class="line">        <span class="keyword">return</span> Experience(...)  <span class="comment"># 返回经验</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_reward</span>(<span class="params">r, kl_coef, log_probs, log_probs_base, action_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">    kl = compute_approx_kl(log_probs, log_probs_base, action_mask=action_mask)  <span class="comment"># 计算KL散度</span></span><br><span class="line">    reward = r - kl_coef * kl  <span class="comment"># 计算奖励</span></span><br><span class="line">    <span class="keyword">return</span> reward</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_approx_kl</span>(<span class="params">log_probs, log_probs_base, action_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">    log_ratio = log_probs - log_probs_base  <span class="comment"># 计算对数概率之间的差异</span></span><br><span class="line">    approx_kl = (log_ratio.exp() - <span class="number">1</span>) - log_ratio  <span class="comment"># 计算近似KL散度</span></span><br><span class="line">    <span class="keyword">if</span> action_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        approx_kl = masked_mean(approx_kl, action_mask, dim=<span class="number">1</span>)  <span class="comment"># 根据动作掩码计算平均KL散度</span></span><br><span class="line">        <span class="keyword">return</span> approx_kl</span><br><span class="line">    approx_kl = approx_kl.mean(dim=<span class="number">1</span>)  <span class="comment"># 计算平均KL散度</span></span><br><span class="line">    <span class="keyword">return</span> approx_kl</span><br></pre></td></tr></table></figure><p>上述代码中用到了KL散度。KL散度（Kullback-Leibler divergence）是一种用于衡量两个概率分布之间差异的指标。在信息论和统计学中广泛应用。</p><p>给定两个离散概率分布P和Q，它们的KL散度定义为：<code>KL(P || Q) = Σ P(i) * log(P(i) / Q(i))</code> 其中，P(i)和Q(i)分别表示P和Q在第i个事件上的概率。</p><p>KL散度不具备对称性，即KL(P || Q) ≠ KL(Q || P)。它度量的是从P到Q的信息损失或差异。KL散度的值为非负数，当且仅当P和Q相等时，KL散度等于0。当P和Q之间的差异增大时，KL散度的值也会增大。</p><p>在深度学习中，KL散度常用于衡量生成模型生成的样本分布与真实数据分布之间的差异。通过最小化KL散度，可以使生成模型逼近真实数据分布，从而提高生成样本的质量。在上述代码中，KL散度被用于计算奖励信号。通过比较动作对数概率与基准动作对数概率之间的差异，可以衡量动作选择与基准模型之间的差异程度，进而调整奖励的大小。</p><h3 id="rlhf算法总结"><a class="markdownIt-Anchor" href="#rlhf算法总结"></a> RLHF算法总结</h3><p>回顾RLHF算法的过程，可以看到，由于我们之前训练了一个奖励函数，RLHF算法在执行过程中，可以没有人类的参与而自动进行。奖励函数代替人给出了对于模型生成的回复的质量的反馈。</p><p>到这里，大家可以理解为什么ChatGPT可以如此智能的回复大家的任意的自然语言问题了吧？OpenAI开放ChatGPT模型给大家使用，随着大家使用越多，OpenAI就可以根据RLHF的算法让模型接触到更多的对话，从而基于这些对话自动的优化ChatGPT！</p><h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2><p>到这里，我们就分析完了所有的ChatGPT类模型的训练和微调、RLHF微调的代码。在分析代码时，我们有意忽略了很多细节及模型并行处理的部分代码，这些对于我们理解模型帮助不大。</p><p>到这里大家应该对ChatGPT类模型的训练有一个较为深入的认识了。</p><p>自ChatGPT发布以来，很多人认为这是一个人类走向通用人工智能的突破，也有一些人认为它其实没什么本质的改进。有很多人对自己的职业发展产生了很深的焦虑感，也有很多人感觉触碰到了科幻世界中的未来，还有很多人觉得又是一个可以好好捞一把的机会。</p><p>也许每个人都有必要去了解一下机器学习技术的原理，这样才能形成对它的理性的认知。</p><p>ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。</p><p>这是此系列的第五篇，ChatGPT的自动优化。</p><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><ul><li>alpaca博客介绍：<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></li><li>LLAMA Paper：<a href="https://arxiv.org/abs/2302.13971v1">https://arxiv.org/abs/2302.13971v1</a></li><li>Self-Instruct: Aligning Language Model with Self Generated Instructions：<a href="https://arxiv.org/abs/2212.10560">https://arxiv.org/abs/2212.10560</a></li><li>Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality: <a href="https://lmsys.org/blog/2023-03-30-vicuna/">https://lmsys.org/blog/2023-03-30-vicuna/</a></li><li>OpenAI开发的ChatGPT资料（Training language models to follow instructions<br />with human feedback）： <a href="https://arxiv.org/pdf/2203.02155.pdf">https://arxiv.org/pdf/2203.02155.pdf</a></li><li>OpenAI开放的GPT-3资料（Language Models are Few-Shot Learners）: <a href="https://arxiv.org/pdf/2005.14165.pdf">https://arxiv.org/pdf/2005.14165.pdf</a></li><li>OpenAI开放的GPT-2资料（Language Models are Unsupervised Multitask Learners）: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf</a></li><li>OpenAI开放的GPT资料（Improving Language Understanding by Generative Pre-Training): <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;作为一个一直对AI技术很感兴趣的软件开发工程师，早在深度学习开始火起来的15、16年，我也开始了相关技术的学习。当时还组织了公司内部同样有兴趣的同学一起研究，最终的成果汇集成几次社区中的分享以及几篇学习文章（见&lt;a href=&quot;https://brightliao.com/tags/ai/&quot;&gt;这里&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;从去年OpenAI发布ChatGPT以来，AI的能力再次惊艳了世人。在这样的一个时间节点，重新去学习相关技术显得很有必要。&lt;/p&gt;
&lt;p&gt;ChatGPT的内容很多，我计划采用一个系列，多篇文章来分享学习我自己学习过程中的一些理解。本系列文章，我将站在一个普通开发人员的角度展开，希望对想了解ChatGPT技术原理的普通开发者们有帮助。&lt;/p&gt;
&lt;p&gt;ChatGPT本身就具备很丰富的知识，所以ChatGPT自身实际上就是一个很好的学习渠道，我也将借助ChatGPT来学习ChatGPT。&lt;/p&gt;
&lt;p&gt;这是此系列的第五篇，ChatGPT的自动优化。&lt;/p&gt;</summary>
    
    
    
    <category term="machine-learning" scheme="http://brightliao.com/categories/machine-learning/"/>
    
    <category term="ChatGPT" scheme="http://brightliao.com/categories/machine-learning/chatgpt/"/>
    
    
    <category term="AI" scheme="http://brightliao.com/tags/ai/"/>
    
    <category term="机器学习" scheme="http://brightliao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="强化学习" scheme="http://brightliao.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="ML" scheme="http://brightliao.com/tags/ml/"/>
    
    <category term="ChatGPT" scheme="http://brightliao.com/tags/chatgpt/"/>
    
  </entry>
  
</feed>
