<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/attaches/assets/fonts/fonts.css">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/attaches/assets/next/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/attaches/assets/next/pace-theme-minimal.css">
  <script src="/attaches/assets/next/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"brightliao.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文试图帮大家理解深度学习中的两大重要而基础的模型RNN和LSTM，并结合google在udacity上面关于深度学习的课程习题进行实践。 近两年深度学习在自然语言处理领域取得了非常好的效果。深度学习模型可以直接进行端到端的训练，而无须进行传统的特征工程过程。在自然语言处理方面，主要的深度学习模型是RNN，以及在RNN之上扩展出来的LSTM。RNN和LSTM也可以广泛用于其他序列处理和预测的机器学">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN和LSTM从理论到实践一：词向量">
<meta property="og:url" content="http://brightliao.com/2016/12/02/dl-workshop-rnn-and-lstm/index.html">
<meta property="og:site_name" content="Bright LGM&#39;s Blog">
<meta property="og:description" content="本文试图帮大家理解深度学习中的两大重要而基础的模型RNN和LSTM，并结合google在udacity上面关于深度学习的课程习题进行实践。 近两年深度学习在自然语言处理领域取得了非常好的效果。深度学习模型可以直接进行端到端的训练，而无须进行传统的特征工程过程。在自然语言处理方面，主要的深度学习模型是RNN，以及在RNN之上扩展出来的LSTM。RNN和LSTM也可以广泛用于其他序列处理和预测的机器学">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/one-hot-encoding-for-words.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/word-analogies-question.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/way-to-find-relationship-between-words.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/unigram-model-e1.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/bigram-model-e1.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/trigram-ngram-model-e1.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/skip-gram-model.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/similar-words-in-graph.png">
<meta property="og:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/cbow-model.png">
<meta property="article:published_time" content="2016-12-02T15:22:40.000Z">
<meta property="article:modified_time" content="2023-06-16T09:52:20.502Z">
<meta property="article:author" content="Bright LGM">
<meta property="article:tag" content="tensorflow">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="RNN">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://brightliao.com/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/one-hot-encoding-for-words.png">

<link rel="canonical" href="http://brightliao.com/2016/12/02/dl-workshop-rnn-and-lstm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>RNN和LSTM从理论到实践一：词向量 | Bright LGM's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-88944761-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-88944761-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7438a320b8fb9e84348971d3c0cde17d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <link rel="stylesheet" href="/attaches/assets/next/share.min.css">

<link rel="alternate" href="/atom.xml" title="Bright LGM's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Bright LGM's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Code speaks.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-数据">

    <a href="/data/" rel="section"><i class="fa fa-table fa-fw"></i>数据<span class="badge">36</span></a>

  </li>
        <li class="menu-item menu-item-机器学习">

    <a href="/ml/" rel="section"><i class="fa fa-hand-sparkles fa-fw"></i>机器学习<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-敏捷">

    <a href="/agile/" rel="section"><i class="fa fa-skiing fa-fw"></i>敏捷<span class="badge">14</span></a>

  </li>
        <li class="menu-item menu-item-技术">

    <a href="/tech/" rel="section"><i class="fa fa-quidditch fa-fw"></i>技术<span class="badge">10</span></a>

  </li>
        <li class="menu-item menu-item-架构">

    <a href="/arch/" rel="section"><i class="fa fa-warehouse fa-fw"></i>架构<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-其他">

    <a href="/other/" rel="section"><i class="fa fa-wave-square fa-fw"></i>其他<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-rss">

    <a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a>

  </li>
      <li class="menu-item menu-item-search" style="border-bottom: solid 1px #eee; margin-bottom: 8px;">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">108</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">49</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">133</span></a>

  </li>

  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/gmlove" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://brightliao.com/2016/12/02/dl-workshop-rnn-and-lstm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.png">
      <meta itemprop="name" content="Bright LGM">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bright LGM's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RNN和LSTM从理论到实践一：词向量
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-02 23:22:40" itemprop="dateCreated datePublished" datetime="2016-12-02T23:22:40+08:00">2016-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-16 17:52:20" itemprop="dateModified" datetime="2023-06-16T17:52:20+08:00">2023-06-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
            </span>

          
            <span id="/2016/12/02/dl-workshop-rnn-and-lstm/" class="post-meta-item leancloud_visitors" data-flag-title="RNN和LSTM从理论到实践一：词向量" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>29 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text&#x2F;css" href="/attaches/assets/katex/katex.min.css"><p>本文试图帮大家理解深度学习中的两大重要而基础的模型RNN和LSTM，并结合google在udacity上面关于深度学习的课程习题进行实践。</p>
<p>近两年深度学习在自然语言处理领域取得了非常好的效果。深度学习模型可以直接进行端到端的训练，而无须进行传统的特征工程过程。在自然语言处理方面，主要的深度学习模型是RNN，以及在RNN之上扩展出来的LSTM。RNN和LSTM也可以广泛用于其他序列处理和预测的机器学习任务。</p>
<p>RNN，全称为Recurrent Neural Network，常译为循环神经网络，也可译为时序递归神经网络，很多人直接简称为递归神经网络。另一个模型Recursive Neural Network，缩写也同样是RNN，译为递归神经网络。递归神经网络是时序递归神经网络的超集，它还可以包括在结构上有递归的神经网络，但是结构递归神经网络使用远没有时序递归神经网络使用得广泛。</p>
<p>本文包括四个部分：</p>
<ul>
<li>NLP</li>
<li>单词的向量表示</li>
<li>RNN和LSTM理论介绍</li>
<li>训练一个LSTM模型</li>
</ul>
<span id="more"></span>
<h2 id="nlp"><a class="markdownIt-Anchor" href="#nlp"></a> NLP</h2>
<p>我们首先来看看自然语言处理。自然语言处理可以说是信息时代最重要的技术之一，实际上自然语言处理无处不在，因为人们几乎所有交流沟通都通过语言进行，如搜索，广告语，邮件，翻译等等。</p>
<p>我们可以列举自然语言处理中的部分任务如下：</p>
<ul>
<li>简单的任务：拼写检查、关键词搜索、同义词查找</li>
<li>比较复杂的任务：从网站或者文档中提取信息</li>
<li>很困难的任务：机器翻译、语义分析（在用户使用搜索引擎时，他输入的查询是什么意思？）、指代分析（如：文档里面的『他』或『她』具体指谁？）</li>
</ul>
<h2 id="单词的向量表示"><a class="markdownIt-Anchor" href="#单词的向量表示"></a> 单词的向量表示</h2>
<p>要解决自然语言处理中的问题，我们首先要解决的问题是，如何表示这些问题。回顾之前的课程，我们可以发现，通常我们都会将输入数据表示成向量，在向量上进行数学建模。对于自然语言处理，也是一样的，我们要想办法将输入数据转化为向量。这里我们只讨论英文语言处理。那么究竟应该怎样用向量表示英文中的单词呢？</p>
<p>我们最容易想到的方法就是，跟之前的课程中对类别的处理一样，直接做one-hot编码。将所有单词排序，排序之后每个单词就会有一个位置，然后用一个与单词数量等长的数组表示某单词，该单词所在的位置数组值就为1，而其他所有位置值都为0.</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/one-hot-encoding-for-words.png" alt="One-hot encoding for words" /></p>
<p>但是这样做有什么问题呢？第一个问题就是这样编码太稀疏了，会导致维度非常高，因为单词的数量级通常在10^6级别，维度高就导致计算困难。第二个问题是我们无法简单的从这样的编码中得知单词之间的关系。</p>
<p>为什么单词之间的关系重要呢？因为在我们使用语言时，单词之间并非完全相互独立的。比如短语&quot;at work&quot;，&quot;at&quot;和&quot;work&quot;之间存在一种可搭配使用的关系。而我们要进行语言分析时，单词之间的关系使用就更频繁了。我们来看看一个单词间关系的例子。</p>
<p>下面的习题答案是什么呢？</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/word-analogies-question.png" alt="Word analogies question" /></p>
<p>“puppy&quot;对&quot;dog&quot;增加了宠物的属性，那么&quot;cat&quot;加上宠物属性就变成了&quot;kitten”。</p>
<p>“taller&quot;对&quot;tall&quot;增加了比较级属性，那么&quot;short&quot;加上比较级属性就变成了&quot;shorter”。</p>
<p>那么问题来了，如何进行机器学习训练才能得到这样的关系属性呢？先看两个句子。</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/way-to-find-relationship-between-words.png" alt="Way to find relationship between words" /></p>
<p>如果说在我们的训练数据中出现了四个句子：</p>
<ul>
<li>The cat purrs.</li>
<li>This cat hunts mice.</li>
<li>The kitty purrs.</li>
<li>This kitty hunts mice.</li>
</ul>
<p>那么我们就有了一个很强的推断：<em>cat和kitty是相似的</em>。</p>
<p><strong>我们用于提取这种关系的方式就是：</strong></p>
<ul>
<li>使用低维向量来表示单词</li>
<li>用邻近的单词来进行相互预测</li>
</ul>
<h3 id="语言模型"><a class="markdownIt-Anchor" href="#语言模型"></a> 语言模型</h3>
<p>在进行实践之前，我们先来看看一个重要的背景知识：语言模型。</p>
<p>早期的自然语言处理采用硬编码的规则来实现。在上世纪80年代，机器学习被应用于自然语言处理中，统计语言模型被提出来，并广泛应用于机器学习模型中。我们这里的语言模型就是指统计语言模型。</p>
<p>我们认识一下什么是一个好的模型？对某个我们认为正确的句子，比如『狗啃骨头』，一个好的模型将能给出很高的概率。而对于不合理的句子，比如『骨头啃狗』它将给出很低的概率。这里面的一个重要的概念就是句子的概率。统计语言模型，简单而言，就是计算某一个句子的概率：P(w1, w2, w3, …)。其中w表示句子中的单词。</p>
<p>如何计算这样的概率呢？为了简便处理，我们可以根据前n个词来预测下一个词。这样我们就得到了Unigram Model，Bigram Model, Trigram Model或者N-gram Model。</p>
<p>Unigram Model是指，我们可以将每个单词视为独立无关的，于是可以得到下面的等式：</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/unigram-model-e1.png" alt="Unigram model" /></p>
<p>Bigram Model是指，如果当前单词只依赖其前面一个单词，在『狗啃骨头』中就表示可以用『狗』来预测『啃』。这样的话，我们的模型就可以用下式计算（P(w2|w1）表示在出现单词w1时，出现w2的概率)：</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/bigram-model-e1.png" alt="Bigram model" /></p>
<p>Trigram和N-gram Model可以得到的等式如下：</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/trigram-ngram-model-e1.png" alt="Trigram and N-gram Model" /></p>
<p>事实上直接使用N-gram模型来计算句子概率是有问题的。因为它太简单了，最多能表示单词和前n个单词的关系，前n+1个单词就无法表示。而且n不能太大，太大会导致计算问题，并且n太大通常性能不会有明显的提升。</p>
<h3 id="word2vec"><a class="markdownIt-Anchor" href="#word2vec"></a> Word2vec</h3>
<p>回到词向量这个主题，对于词向量模型，我们要介绍的是word2vec算法。Word2vec从这个名字简单易懂，但是它似乎概括了所有提取词向量的算法，从这个名字我们大家可以想象一下它在自然语言处理中的地位。该算法是google于2013年提出来的，一经提出便被广泛应用了起来。</p>
<p>word2vec算法，在不断发展沉淀之后，得到两个机器学习模型：Skip Gram Model和CBOW(Continuous Bag of Words)。Skip Gram Model在实现上相对简单，而且google在udacity上面的题目也是以Skip Gram Model作为引子。我们先看看Skip Gram Model，然后在后面的习题中再一起看看CBOW模型。</p>
<h3 id="skip-gram-model"><a class="markdownIt-Anchor" href="#skip-gram-model"></a> Skip Gram Model</h3>
<p>Skip Gram Model属于非监督学习领域，这跟之前的图片识别不同。图片识别时，对于每一张图片我们是有标签的，比如某一张内容为&quot;A&quot;的图片，那么它的标签就是&quot;a&quot;。对于文本而言，原始数据只有一堆文本，一长串的单词序列。我们是没有显示的给定任何标签的。但是机器学习算法又是需要标签的，要不然我们无法计算我们的损失函数。对于这个问题，我们的想法是通过文本内容构造标签。借鉴N-gram模型的想法，如果单词只跟周边的单词相关，那么我们是不是就可以说在使用单词进行预测时，周边的单词就是该单词的正确预测结果呢？Skip Gram Model就是基于这个想法。</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/skip-gram-model.png" alt="Skip gram model" /></p>
<p>这个算法的步骤如下：</p>
<ul>
<li>随机生成一个大小为(vocabulary_size, embedding_size)的embedding矩阵（即所有单词的词向量矩阵，每一个行对应一个单词的向量）</li>
<li>对于某一个单词，从embedding矩阵中提取单词向量</li>
<li>在该单词向量上使用logistic regression进行训练，softmax作为激活函数</li>
<li>期望logistic regression得到的概率向量可以与真实的概率向量（即周边词的one-hot编码向量）相匹配</li>
</ul>
<h3 id="skip-gram-model中的问题及negative-sampling"><a class="markdownIt-Anchor" href="#skip-gram-model中的问题及negative-sampling"></a> Skip Gram Model中的问题及Negative Sampling</h3>
<p>上面的算法最后一步在计算上是有问题的。根据公式 y = softmax(XW+b)，其中</p>
<ul>
<li>X的维度是：(batch_size, embedding_size)</li>
<li>W的维度是：(embedding_size, vocabulary_size)</li>
</ul>
<p>softmax将需要在(batch_size, vocabulary_size)矩阵上面进行计算，vocabulary_size通常是很大的，softmax在进行e^x运算时就会遇到计算问题。</p>
<p>我们应对这个问题的方法是Negative sampling。Negative sampling是指，我们在计算最终的softmax时，可以只选取部分错误的label和正确的label进行组合，而无须选择所有的错误label进行计算。当训练的迭代次数足够大时，这对于整个结果是没有影响的。在后面的作业中，num_sampled就是指选择的错误的label的数量。</p>
<h3 id="使用word2vec之后如何衡量单词间的相似性"><a class="markdownIt-Anchor" href="#使用word2vec之后如何衡量单词间的相似性"></a> 使用word2vec之后如何衡量单词间的相似性？</h3>
<p>在得到词向量之后，如何衡量单词间的相似性呢？</p>
<p>回顾一下我们的训练过程。如果两个单词相似，即出现了两个句子&quot;The cat purrs&quot;和&quot;The kitty purrs&quot;，那么cat的词向量经过计算之后可以得到purrs词向量，kitty词向量经过计算之后也可以得到purrs词向量。再结合softmax的比例计算过程，可以得出的结论是，最终的词向量里面，相似的单词，他们的词向量值在比例上也是相似的。</p>
<p>事实上我们通常会用余弦距离去衡量词向量的相似性，即词向量间的夹角。相似性就是：给定单词w1 w2 w3的词向量Vw1 Vw2 Vw3，如果Vw1 * Vw2.T / (|Vw1||Vw2|) &gt; Vw1 * Vw3.T / (|Vw1||Vw3|)，那么我们就认为w2比w3更接近w1。</p>
<h3 id="coding时间"><a class="markdownIt-Anchor" href="#coding时间"></a> Coding时间</h3>
<p><a href="git@github.com:gmlove/dl-workshop-rnn-lstm.git">这个工程</a>里面包含了所有用到的数据及代码。下载工程之后，按照之前的办法，将整个目录映射到docker镜像中。</p>
<p>下面的内容，请结合<a target="_blank" rel="noopener" href="https://github.com/gmlove/dl-workshop-rnn-lstm/blob/master/5_word2vec.ipynb">代码</a>来进行阅读。</p>
<p>首先是下载、验证和读取文本。在代码中对应<code>maybe_download</code>和<code>read_data</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;http://mattmahoney.net/dc/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maybe_download</span>(<span class="params">filename, expected_bytes</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Download a file if not present, and make sure it&#x27;s the right size.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">    filename, _ = urlretrieve(url + filename, filename)</span><br><span class="line">  statinfo = os.stat(filename)</span><br><span class="line">  <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Found and verified %s&#x27;</span> % filename)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(statinfo.st_size)</span><br><span class="line">    <span class="keyword">raise</span> Exception(</span><br><span class="line">      <span class="string">&#x27;Failed to verify &#x27;</span> + filename + <span class="string">&#x27;. Can you get to it with a browser?&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">filename = maybe_download(<span class="string">&#x27;text8.zip&#x27;</span>, <span class="number">31344016</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data</span>(<span class="params">filename</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Extract the first file enclosed in a zip file as a list of words&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">    data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">  <span class="keyword">return</span> data</span><br><span class="line">  </span><br><span class="line">words = read_data(filename)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Data size %d&#x27;</span> % <span class="built_in">len</span>(words))</span><br></pre></td></tr></table></figure>
<p>接下来是根据文本构造我们要用的数据结构。我们先定义我们的单词库大小，这里设置为50000。在调用<code>build_dataset</code>之后，得到的数据为：</p>
<ul>
<li><code>count</code>：一个单词和它出现的次数的list，按照单词出现次数排序。如果单词出现次数太低，排序在50000之后，那么我们就将它映射到&quot;UNK&quot;单词，即unknown。这里的单词索引将用于把文本映射为整数值。</li>
<li><code>data</code>：原始文本映射到索引之后的序列。如原始文本为&quot;anarchism originated as a term of abuse first&quot;，映射之后为<code>[5243, 3083, 12, 6, 195, 2, 3136, 46, 59, 156]</code></li>
<li><code>dictionary</code>：用于查询词对应的索引的字典</li>
<li><code>reverse_dictionary</code>：用于查询索引对应的单词的字典</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">words</span>):</span><br><span class="line">  count = [[<span class="string">&#x27;UNK&#x27;</span>, -<span class="number">1</span>]]</span><br><span class="line">  count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>))</span><br><span class="line">  dictionary = <span class="built_in">dict</span>()</span><br><span class="line">  <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">    dictionary[word] = <span class="built_in">len</span>(dictionary)</span><br><span class="line">  data = <span class="built_in">list</span>()</span><br><span class="line">  unk_count = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">      index = dictionary[word]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      index = <span class="number">0</span>  <span class="comment"># dictionary[&#x27;UNK&#x27;]</span></span><br><span class="line">      unk_count = unk_count + <span class="number">1</span></span><br><span class="line">    data.append(index)</span><br><span class="line">  count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">  reverse_dictionary = <span class="built_in">dict</span>(<span class="built_in">zip</span>(dictionary.values(), dictionary.keys())) </span><br><span class="line">  <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Most common words (+UNK)&#x27;</span>, count[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sample data&#x27;</span>, data[:<span class="number">10</span>])</span><br><span class="line"><span class="keyword">del</span> words  <span class="comment"># Hint to reduce memory.</span></span><br></pre></td></tr></table></figure>
<p>然后我们需要有一个过程来生成batch，每次都批量处理数据，我们才能发挥计算机的并行计算的实力。</p>
<p>在生成batch时，我们期望在输入参数为<code>batch_size</code> <code>num_skips</code> <code>skip_window</code>时，可以对一小段文本，即skip_window长的文本，使用中心词来预测周边的词，生成num_skips个类似(words[2] -&gt; words[0])这样的预测组。</p>
<p>当原始数据为<code>['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']</code>时，以num_skips=2和skip_window=1来调用generate_batch之后，得到的batch为<code>['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']</code>，对应的label为<code>['as', 'anarchism', 'a', 'originated', 'term', 'as', 'a', 'of']</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_batch</span>(<span class="params">batch_size, num_skips, skip_window</span>):</span><br><span class="line">  <span class="keyword">global</span> data_index</span><br><span class="line">  <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">  <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">  batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">  labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">  span = <span class="number">2</span> * skip_window + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">  buffer = collections.deque(maxlen=span)</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(span):</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % <span class="built_in">len</span>(data)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size // num_skips):</span><br><span class="line">    target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">    targets_to_avoid = [ skip_window ]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_skips):</span><br><span class="line">      <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">        target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">      targets_to_avoid.append(target)</span><br><span class="line">      batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">      labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % <span class="built_in">len</span>(data)</span><br><span class="line">  <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;data:&#x27;</span>, [reverse_dictionary[di] <span class="keyword">for</span> di <span class="keyword">in</span> data[:<span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num_skips, skip_window <span class="keyword">in</span> [(<span class="number">2</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">2</span>)]:</span><br><span class="line">    data_index = <span class="number">0</span></span><br><span class="line">    batch, labels = generate_batch(batch_size=<span class="number">8</span>, num_skips=num_skips, skip_window=skip_window)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nwith num_skips = %d and skip_window = %d:&#x27;</span> % (num_skips, skip_window))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;    batch:&#x27;</span>, [reverse_dictionary[bi] <span class="keyword">for</span> bi <span class="keyword">in</span> batch])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;    labels:&#x27;</span>, [reverse_dictionary[li] <span class="keyword">for</span> li <span class="keyword">in</span> labels.reshape(<span class="number">8</span>)])</span><br></pre></td></tr></table></figure>
<p>有了这些数据结构和工具函数之后，我们就可以构造我们的模型如代码中所示。</p>
<ul>
<li><code>embedding_size</code>：我们期望编码之后的词向量长度</li>
<li><code>train_dataset</code>：大小为batch_size的int数组，我们将单词索引在embedding矩阵中查找词向量</li>
<li><code>train_labels</code>：大小为[batch_size, 1]的矩阵，因为我们最终进行损失计算时，是使用one-hot编码的cross entropy</li>
<li><code>valid_examples</code>：从前100个词的单词集中随机选取16个单词用于计算相似度</li>
<li><code>valid_dataset</code>：对应于valid_examples，大小为16的int数组</li>
<li><code>embeddings</code>：大小为[vocabulary_size, embedding_size]的浮点型矩阵</li>
<li><code>softmax_weights</code>：大小为[vocabulary_size, embedding_size]的浮点型权值矩阵</li>
<li><code>softmax_biases</code>：大小为[vocabulary_size]的浮点型偏置向量</li>
</ul>
<p>我们的模型就是先调用<code>tf.nn.embedding_lookup</code>去查找词向量。然后调用<code>tf.reduce_mean</code>和<code>tf.nn.sampled_softmax_loss</code>计算损失值。之后使用<code>tf.train.AdagradOptimizer</code>进行梯度下降，期望最小化损失值。梯度下降会同时优化我们的embeddings矩阵、softmax_weights矩阵及softmax_biases向量。最后在这个模型上面迭代计算就可以得到优化后的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span> <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">skip_window = <span class="number">1</span> <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">num_skips = <span class="number">2</span> <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"><span class="comment"># We pick a random validation set to sample nearest neighbors. here we limit the</span></span><br><span class="line"><span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line"><span class="comment"># construction are also the most frequent. </span></span><br><span class="line">valid_size = <span class="number">16</span> <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">valid_window = <span class="number">100</span> <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">valid_examples = np.array(random.sample(<span class="built_in">range</span>(valid_window), valid_size))</span><br><span class="line">num_sampled = <span class="number">64</span> <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default(), tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input data.</span></span><br><span class="line">  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">  train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Variables.</span></span><br><span class="line">  embeddings = tf.Variable(</span><br><span class="line">    tf.random_uniform([vocabulary_size, embedding_size], -<span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">  softmax_weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                         stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Model.</span></span><br><span class="line">  <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">  embed = tf.nn.embedding_lookup(embeddings, train_dataset) <span class="comment"># embed.shape: (batch_size, embedding_size)</span></span><br><span class="line">  <span class="comment"># Compute the softmax loss, using a sample of the negative labels each time.</span></span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,</span><br><span class="line">                               train_labels, num_sampled, vocabulary_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optimizer.</span></span><br><span class="line">  <span class="comment"># Note: The optimizer will optimize the softmax_weights AND the embeddings.</span></span><br><span class="line">  <span class="comment"># This is because the embeddings are defined as a variable quantity and the</span></span><br><span class="line">  <span class="comment"># optimizer&#x27;s `minimize` method will by default modify all variable quantities </span></span><br><span class="line">  <span class="comment"># that contribute to the tensor it is passed.</span></span><br><span class="line">  <span class="comment"># See docs on `tf.train.Optimizer.minimize()` for more details.</span></span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br></pre></td></tr></table></figure>
<p>有了模型之后，我们该如何计算相似度呢？使用之前的相似度计算公式，我们可以计算先embedding的每一个词向量的1/|Vw1|，使用valid_dataset的词向量与所有其他单词的词向量相乘，然后从大到小排序就可以得到按相似度排序的其他相似词。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Compute the similarity between minibatch examples and all embeddings.</span><br><span class="line"># We use the cosine distance:</span><br><span class="line">norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))</span><br><span class="line">normalized_embeddings = embeddings / norm</span><br><span class="line">valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">  normalized_embeddings, valid_dataset)</span><br><span class="line">similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) # similarity.shape: (valid_size, vocabulary_size)</span><br></pre></td></tr></table></figure>
<p>有了这个模型之后，我们就可以在模型上面进行迭代计算了。迭代的同时，我们每2000步输出一下平均损失值，每10000步的时候，输出一下和验证集相似的词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Initialized&#x27;</span>)</span><br><span class="line">  average_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    batch_data, batch_labels = generate_batch(</span><br><span class="line">      batch_size, num_skips, skip_window)</span><br><span class="line">    feed_dict = &#123;train_dataset : batch_data, train_labels : batch_labels&#125;</span><br><span class="line">    _, l = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">    average_loss += l</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        average_loss = average_loss / <span class="number">2000</span></span><br><span class="line">      <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;Average loss at step %d: %f&#x27;</span> % (step, average_loss))</span><br><span class="line">      average_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># note that this is expensive (~20% slowdown if computed every 500 steps)</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      sim = similarity.<span class="built_in">eval</span>()</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(valid_size):</span><br><span class="line">        valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">        top_k = <span class="number">8</span> <span class="comment"># number of nearest neighbors</span></span><br><span class="line">        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k+<span class="number">1</span>] <span class="comment"># argsort will sort elements as ascended, so we need a minus symbol</span></span><br><span class="line">        log = <span class="string">&#x27;Nearest to %s:&#x27;</span> % valid_word</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(top_k):</span><br><span class="line">          close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">          log = <span class="string">&#x27;%s %s,&#x27;</span> % (log, close_word)</span><br><span class="line">        <span class="built_in">print</span>(log)</span><br><span class="line">  final_embeddings = normalized_embeddings.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>在模型计算好之后，我们可以使用TSNE降维方法，用二维表来表示我们的词向量，再绘制出来就可以得到最终的效果图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_points = <span class="number">400</span></span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">&#x27;pca&#x27;</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">two_d_embeddings = tsne.fit_transform(final_embeddings[<span class="number">1</span>:num_points+<span class="number">1</span>, :]) <span class="comment"># ignore UNK</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">embeddings, labels</span>):</span><br><span class="line">  <span class="keyword">assert</span> embeddings.shape[<span class="number">0</span>] &gt;= <span class="built_in">len</span>(labels), <span class="string">&#x27;More labels than embeddings&#x27;</span></span><br><span class="line">  pylab.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))  <span class="comment"># in inches</span></span><br><span class="line">  <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">    x, y = embeddings[i,:]</span><br><span class="line">    pylab.scatter(x, y)</span><br><span class="line">    pylab.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">&#x27;offset points&#x27;</span>,</span><br><span class="line">                   ha=<span class="string">&#x27;right&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line">  pylab.show()</span><br><span class="line"></span><br><span class="line">words = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_points+<span class="number">1</span>)]</span><br><span class="line">plot(two_d_embeddings, words)</span><br></pre></td></tr></table></figure>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/similar-words-in-graph.png" alt="Similar words in a graph" /></p>
<h3 id="cbow模型"><a class="markdownIt-Anchor" href="#cbow模型"></a> CBOW模型</h3>
<p>现在到我们的动手时间了。我们将在上面的代码的基础上实现一个CBOW模型。</p>
<p>CBOW模型跟Skip Gram模型正好相反，在这个模型中，我们使用单词周边的单词去预测该单词。其模型如下：</p>
<p><img data-src="/attaches/2016/2016-12-02-dl-workshop-rnn-and-lstm/cbow-model.png" alt="CBOW model" /></p>
<p>这个算法的步骤如下：</p>
<ul>
<li>随机生成一个大小为(vocabulary_size, embedding_size)的embedding矩阵（即所有单词的词向量矩阵，每一个行对应一个单词的向量）</li>
<li>对于某一个单词（中心词），从embedding矩阵中提取其周边单词的词向量</li>
<li>求周边单词的词向量的均值向量</li>
<li>在该均值向量上使用logistic regression进行训练，softmax作为激活函数</li>
<li>期望logistic regression得到的概率向量可以与真实的概率向量（即中心词的one-hot编码向量）相匹配</li>
</ul>
<p>对比CBOW的计算步骤和SkipGram的计算步骤，我们可以来一步步的修改代码。</p>
<p>下面的内容，请结合<a target="_blank" rel="noopener" href="https://github.com/gmlove/dl-workshop-rnn-lstm/blob/master/5_word2vec-problem.ipynb">代码</a>来进行阅读。</p>
<p>我们要修改的第一个地方是<code>generate_batch</code>函数。</p>
<ul>
<li>labels的定义，其大小不再是(batch_size, 1)而应该为(batch_size // num_skips, 1)</li>
<li>batch的赋值，batch现在应该为中心词周边的单词</li>
<li>labels的赋值，labels现在应该为中心词</li>
<li>print输出，label时，label的长度应该为batch_size // num_skips</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_batch</span>(<span class="params">batch_size, num_skips, skip_window</span>):</span><br><span class="line">  <span class="keyword">global</span> data_index</span><br><span class="line">  <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">  <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">  batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">  labels = np.ndarray(shape=(batch_size // num_skips, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">  span = <span class="number">2</span> * skip_window + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">  buffer = collections.deque(maxlen=span)</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(span):</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % <span class="built_in">len</span>(data)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size // num_skips):</span><br><span class="line">    target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">    targets_to_avoid = [ skip_window ]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_skips):</span><br><span class="line">      <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">        target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">      targets_to_avoid.append(target)</span><br><span class="line">      batch[i * num_skips + j] = buffer[target]</span><br><span class="line">    labels[i, <span class="number">0</span>] = buffer[skip_window]</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % <span class="built_in">len</span>(data)</span><br><span class="line">  <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;data:&#x27;</span>, [reverse_dictionary[di] <span class="keyword">for</span> di <span class="keyword">in</span> data[:<span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num_skips, skip_window <span class="keyword">in</span> [(<span class="number">2</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">2</span>)]:</span><br><span class="line">    data_index = <span class="number">0</span></span><br><span class="line">    batch, labels = generate_batch(batch_size=<span class="number">8</span>, num_skips=num_skips, skip_window=skip_window)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nwith num_skips = %d and skip_window = %d:&#x27;</span> % (num_skips, skip_window))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;    batch:&#x27;</span>, [reverse_dictionary[bi] <span class="keyword">for</span> bi <span class="keyword">in</span> batch])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;    labels:&#x27;</span>, [reverse_dictionary[li] <span class="keyword">for</span> li <span class="keyword">in</span> labels.reshape(<span class="number">8</span>//num_skips)])</span><br></pre></td></tr></table></figure>
<p>修改之后，可以通过这样的测试来验证程序逻辑。当原始数据为<code>['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']</code>时，以num_skips=2和skip_window=1来调用generate_batch之后，得到的batch为<code>['as', 'anarchism', 'a', 'originated', 'as', 'term', 'of', 'a']</code>，对应的label为<code>['originated', 'as', 'a', 'term']</code></p>
<p>第二个要修改的地方就是模型构建过程。</p>
<ul>
<li>train_labels的定义，train_labels的大小现在应该为(batch_size // num_skips, 1)</li>
<li>在进行loss计算之前，我们需要计算同一个label的train_data的均值向量。可以参考tensorflow的API <code>tf.segment_mean</code>来实现。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span> <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">skip_window = <span class="number">1</span> <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">num_skips = <span class="number">2</span> <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"><span class="comment"># We pick a random validation set to sample nearest neighbors. here we limit the</span></span><br><span class="line"><span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line"><span class="comment"># construction are also the most frequent. </span></span><br><span class="line">valid_size = <span class="number">16</span> <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">valid_window = <span class="number">100</span> <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">valid_examples = np.array(random.sample(<span class="built_in">range</span>(valid_window), valid_size))</span><br><span class="line">num_sampled = <span class="number">64</span> <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default(), tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input data.</span></span><br><span class="line">  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">  train_labels = tf.placeholder(tf.int32, shape=[batch_size//num_skips, <span class="number">1</span>])</span><br><span class="line">  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Variables.</span></span><br><span class="line">  embeddings = tf.Variable(</span><br><span class="line">    tf.random_uniform([vocabulary_size, embedding_size], -<span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">  softmax_weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                         stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Model.</span></span><br><span class="line">  <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">  embed = tf.nn.embedding_lookup(embeddings, train_dataset) <span class="comment"># embed.shape: (batch_size, embedding_size)</span></span><br><span class="line">  segment_ids = tf.constant([i//num_skips <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size)], dtype=tf.int32)</span><br><span class="line">  embed = tf.segment_mean(embed, segment_ids)</span><br><span class="line">  <span class="comment"># Compute the softmax loss, using a sample of the negative labels each time.</span></span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,</span><br><span class="line">                               train_labels, num_sampled, vocabulary_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optimizer.</span></span><br><span class="line">  <span class="comment"># Note: The optimizer will optimize the softmax_weights AND the embeddings.</span></span><br><span class="line">  <span class="comment"># This is because the embeddings are defined as a variable quantity and the</span></span><br><span class="line">  <span class="comment"># optimizer&#x27;s `minimize` method will by default modify all variable quantities </span></span><br><span class="line">  <span class="comment"># that contribute to the tensor it is passed.</span></span><br><span class="line">  <span class="comment"># See docs on `tf.train.Optimizer.minimize()` for more details.</span></span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Compute the similarity between minibatch examples and all embeddings.</span></span><br><span class="line">  <span class="comment"># We use the cosine distance:</span></span><br><span class="line">  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="literal">True</span>))</span><br><span class="line">  normalized_embeddings = embeddings / norm</span><br><span class="line">  valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">    normalized_embeddings, valid_dataset)</span><br><span class="line">  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) <span class="comment"># similarity.shape: (valid_size, vocabulary_size)</span></span><br></pre></td></tr></table></figure>
<p>在修改完成之后，我们就可以看到最后的结果。我们生成的词向量图与SkipGram模型生成的类似。</p>
<p>后续会继续RNN和LSTM的部分。敬请期待！</p>
<p>本文基于google在udacity上面关于深度学习的<a target="_blank" rel="noopener" href="https://classroom.udacity.com/courses/ud730">课程</a>而来。主要参考资料来自于斯坦福大学的自然语言处理课程<a target="_blank" rel="noopener" href="http://cs224d.stanford.edu/syllabus.html">cs224d</a>。</p>

    </div>

    
    
    
    <div class="share-component" style="text-align: right;" data-sites="wechat,weibo,douban,qq,linkedin,facebook,twitter" data-description="一键分享到微信，微博，QQ，豆瓣"></div>
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2016/12/11/dl-workshop-rnn-and-lstm-1/" rel="bookmark">RNN和LSTM从理论到实践二：RNN和LSTM模型</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2017/01/16/dl-workshop-massive-network-tips/" rel="bookmark">大规模Tensorflow网络的一些技巧</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2017/03/01/recognize-house-number/" rel="bookmark">识别门牌号的移动应用</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2017/04/15/understanding-gradients-of-conv2d-in-experiments/" rel="bookmark">理解Conv2d及其梯度的计算过程</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2017/06/21/dive-into-gan/" rel="bookmark">深入探索生成对抗网络（一）</a></div>
    </li>
  </ul>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat.png">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.infoq.cn/u/brightliao/publish">
            <span class="icon">
              <i class="fas fa-info"></i>
            </span>

            <span class="label">InfoQ</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/tensorflow/" rel="tag"><i class="fa fa-tag"></i> tensorflow</a>
              <a href="/tags/ai/" rel="tag"><i class="fa fa-tag"></i> AI</a>
              <a href="/tags/machine-learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
              <a href="/tags/rnn/" rel="tag"><i class="fa fa-tag"></i> RNN</a>
              <a href="/tags/lstm/" rel="tag"><i class="fa fa-tag"></i> LSTM</a>
              <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2016/11/27/a-pick-into-tensorflow/" rel="prev" title="Tensorflow一瞥">
      <i class="fa fa-chevron-left"></i> Tensorflow一瞥
    </a></div>
      <div class="post-nav-item">
    <a href="/2016/12/05/let-machine-play-games/" rel="next" title="让机器自己玩游戏">
      让机器自己玩游戏 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#nlp"><span class="nav-number">1.</span> <span class="nav-text"> NLP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">2.</span> <span class="nav-text"> 单词的向量表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text"> 语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec"><span class="nav-number">2.2.</span> <span class="nav-text"> Word2vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#skip-gram-model"><span class="nav-number">2.3.</span> <span class="nav-text"> Skip Gram Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#skip-gram-model%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8Anegative-sampling"><span class="nav-number">2.4.</span> <span class="nav-text"> Skip Gram Model中的问题及Negative Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8word2vec%E4%B9%8B%E5%90%8E%E5%A6%82%E4%BD%95%E8%A1%A1%E9%87%8F%E5%8D%95%E8%AF%8D%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7"><span class="nav-number">2.5.</span> <span class="nav-text"> 使用word2vec之后如何衡量单词间的相似性？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coding%E6%97%B6%E9%97%B4"><span class="nav-number">2.6.</span> <span class="nav-text"> Coding时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cbow%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.7.</span> <span class="nav-text"> CBOW模型</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bright LGM"
      src="/avatar.png">
  <p class="site-author-name" itemprop="name">Bright LGM</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">108</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">133</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/gmlove" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;gmlove" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/images/wechat.png" title="WeChat → &#x2F;images&#x2F;wechat.png"><i class="fab fa-wechat fa-fw"></i>WeChat</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:gmliao@thoughtworks.com" title="E-Mail → mailto:gmliao@thoughtworks.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://insights.thoughtworks.cn/" title="https:&#x2F;&#x2F;insights.thoughtworks.cn" rel="noopener" target="_blank">Thoughtworks洞见</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.thoughtworks.com/radar" title="https:&#x2F;&#x2F;www.thoughtworks.com&#x2F;radar" rel="noopener" target="_blank">Thoughtworks技术雷达</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://shaogefenhao.com/" title="https:&#x2F;&#x2F;shaogefenhao.com" rel="noopener" target="_blank">少个分号（DDD思考者）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.bmpi.dev/" title="https:&#x2F;&#x2F;www.bmpi.dev" rel="noopener" target="_blank">马大伟（被动收入实践者）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://maguangguang.xyz/" title="https:&#x2F;&#x2F;maguangguang.xyz&#x2F;" rel="noopener" target="_blank">麻广广（企业架构设计）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.icodebook.com/" title="http:&#x2F;&#x2F;www.icodebook.com&#x2F;" rel="noopener" target="_blank">爱码叔iCodeBook（软件架构）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://liuranthinking.com/" title="https:&#x2F;&#x2F;liuranthinking.com&#x2F;" rel="noopener" target="_blank">刘冉思辨悟（软件测试与质量沉思）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.bylinzi.com/" title="https:&#x2F;&#x2F;www.bylinzi.com" rel="noopener" target="_blank">BY林子（关注质量，不止测试）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://qualityfocus.club/yxn" title="https:&#x2F;&#x2F;qualityfocus.club&#x2F;yxn" rel="noopener" target="_blank">于晓南（QualityFocus）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.niezitalk.com/" title="http:&#x2F;&#x2F;www.niezitalk.com&#x2F;" rel="noopener" target="_blank">聂子云（数字化转型咨询）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.kaifengzhang.com/" title="http:&#x2F;&#x2F;www.kaifengzhang.com&#x2F;" rel="noopener" target="_blank">张凯峰（打造影响力）</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://taoshu.in/" title="https:&#x2F;&#x2F;taoshu.in&#x2F;" rel="noopener" target="_blank">涛叔（温故知新，笔耕不辍）</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">蜀ICP备2022013263号 </a>
  </div>

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bright LGM</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">580k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:06</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"4BFdbWUTO8tJBOkCpS2nj4df-gzGzoHsz","app_key":"9jxpPRQJh9dxgB6Ndb1HYuKO","security":false,"betterPerformance":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        // if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/attaches/assets/next/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/attaches/assets/next/jquery.min.js"></script>
  <script src="/attaches/assets/next/jquery.fancybox.min.js"></script>
  <script src="/attaches/assets/next/medium-zoom.min.js"></script>
  <script src="/attaches/assets/next/lozad.min.js"></script>
  <script src="/attaches/assets/next/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

  <script src="/attaches/assets/next/jquery.share.min.js"></script>


<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js?40.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('/attaches/assets/next/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

<link rel="stylesheet" href="/attaches/assets/next/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('/attaches/assets/next/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '75eb53812430d581cd14',
      clientSecret: '5f50853e9d5be69ddc4094d7ec896fc6e0f9f14b',
      repo        : 'gmlove.github.io',
      owner       : 'gmlove',
      admin       : ['gmlove'],
      id          : '6edf96f3a712819d192355cc7e6a45be',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
